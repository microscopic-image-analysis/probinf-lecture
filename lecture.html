<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>lecture</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="lecture_files/libs/clipboard/clipboard.min.js"></script>
<script src="lecture_files/libs/quarto-html/quarto.js"></script>
<script src="lecture_files/libs/quarto-html/popper.min.js"></script>
<script src="lecture_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="lecture_files/libs/quarto-html/anchor.min.js"></script>
<link href="lecture_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="lecture_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="lecture_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="lecture_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="lecture_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#inference-in-probabilistic-models---sampling-methods" id="toc-inference-in-probabilistic-models---sampling-methods" class="nav-link active" data-scroll-target="#inference-in-probabilistic-models---sampling-methods">Inference in Probabilistic Models - Sampling Methods</a></li>
  <li><a href="#literature" id="toc-literature" class="nav-link" data-scroll-target="#literature">Literature</a></li>
  <li><a href="#lecture-1-introduction-1" id="toc-lecture-1-introduction-1" class="nav-link" data-scroll-target="#lecture-1-introduction-1">Lecture 1: Introduction</a></li>
  <li><a href="#lecture-2-direct-sampling-methods-1" id="toc-lecture-2-direct-sampling-methods-1" class="nav-link" data-scroll-target="#lecture-2-direct-sampling-methods-1">Lecture 2: Direct Sampling Methods</a></li>
  <li><a href="#lecture-3-rejection-and-importance-sampling-1" id="toc-lecture-3-rejection-and-importance-sampling-1" class="nav-link" data-scroll-target="#lecture-3-rejection-and-importance-sampling-1">Lecture 3: Rejection and Importance Sampling</a></li>
  <li><a href="#lecture-4-markov-chain-monte-carlo" id="toc-lecture-4-markov-chain-monte-carlo" class="nav-link" data-scroll-target="#lecture-4-markov-chain-monte-carlo">Lecture 4: Markov chain Monte Carlo</a></li>
  <li><a href="#lecture-5-the-metropolis-hastings-algorithm-1" id="toc-lecture-5-the-metropolis-hastings-algorithm-1" class="nav-link" data-scroll-target="#lecture-5-the-metropolis-hastings-algorithm-1">Lecture 5: The Metropolis-Hastings algorithm</a></li>
  <li><a href="#lecture-6-gibbs-sampling-1" id="toc-lecture-6-gibbs-sampling-1" class="nav-link" data-scroll-target="#lecture-6-gibbs-sampling-1">Lecture 6: Gibbs sampling</a></li>
  <li><a href="#lecture-7-hamiltonian-monte-carlo-1" id="toc-lecture-7-hamiltonian-monte-carlo-1" class="nav-link" data-scroll-target="#lecture-7-hamiltonian-monte-carlo-1">Lecture 7: Hamiltonian Monte Carlo</a></li>
  <li><a href="#lecture-8-hamiltonian-monte-carlo-continued" id="toc-lecture-8-hamiltonian-monte-carlo-continued" class="nav-link" data-scroll-target="#lecture-8-hamiltonian-monte-carlo-continued">Lecture 8: Hamiltonian Monte Carlo continued</a></li>
  <li><a href="#lecture-9-practical-issues-and-diagnostics" id="toc-lecture-9-practical-issues-and-diagnostics" class="nav-link" data-scroll-target="#lecture-9-practical-issues-and-diagnostics">Lecture 9: Practical issues and diagnostics</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">



<section id="inference-in-probabilistic-models---sampling-methods" class="level1">
<h1>Inference in Probabilistic Models - Sampling Methods</h1>
<p>Michael Habeck - Jena University Hospital - michael.habeck@uni-jena.de</p>
<p>Wolfhart Feldmeier - Jena University Hospital - wolfhart.feldmeier@uni-jena.de</p>
<section id="dates-and-course-organization" class="level2">
<h2 class="anchored" data-anchor-id="dates-and-course-organization">Dates and course organization</h2>
<ul>
<li><p>Six weeks, two 2-hour lectures plus one 2-hour exercises session per week</p></li>
<li><p>Lectures on Monday and Friday, exercises on Wednesday</p></li>
<li><p>Timetable</p></li>
</ul>
<table class="table">
<colgroup>
<col style="width: 13%">
<col style="width: 16%">
<col style="width: 10%">
<col style="width: 17%">
<col style="width: 41%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Lecture</th>
<th style="text-align: center;">Date</th>
<th style="text-align: center;">Weekday</th>
<th>Time</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">Dec 16, 2022</td>
<td style="text-align: center;">Fri</td>
<td>10:15 - 11:45</td>
<td style="text-align: left;">Introduction</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">Jan 02, 2023</td>
<td style="text-align: center;">Mon</td>
<td>10:15 - 11:45</td>
<td style="text-align: left;">Direct Sampling Methods</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Ex 1</td>
<td style="text-align: center;">Jan 04, 2023</td>
<td style="text-align: center;">Wed</td>
<td>10:15 - 11:45</td>
<td style="text-align: left;">Exercises for lectures 1-2</td>
</tr>
<tr class="even">
<td style="text-align: center;">3</td>
<td style="text-align: center;">Jan 06, 2023</td>
<td style="text-align: center;">Fri</td>
<td>10:15 - 11:45</td>
<td style="text-align: left;">Rejection &amp; Importance Sampling</td>
</tr>
<tr class="odd">
<td style="text-align: center;">4</td>
<td style="text-align: center;">Jan 09, 2023</td>
<td style="text-align: center;">Mon</td>
<td>10:15 - 11:45</td>
<td style="text-align: left;">Markov chains, MCMC</td>
</tr>
<tr class="even">
<td style="text-align: center;">Ex 2</td>
<td style="text-align: center;">Jan 11, 2023</td>
<td style="text-align: center;">Wed</td>
<td>10:15 - 11:45</td>
<td style="text-align: left;">Exercises for lectures 3-4</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5</td>
<td style="text-align: center;">Jan 13, 2023</td>
<td style="text-align: center;">Fri</td>
<td>10:15 - 11:45</td>
<td style="text-align: left;">The Metropolis-Hastings algorithm</td>
</tr>
<tr class="even">
<td style="text-align: center;">6</td>
<td style="text-align: center;">Jan 16, 2023</td>
<td style="text-align: center;">Mon</td>
<td>10:15 - 11:45</td>
<td style="text-align: left;">Gibbs sampling</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Ex 3</td>
<td style="text-align: center;">Jan 18, 2023</td>
<td style="text-align: center;">Wed</td>
<td>10:15 - 11:45</td>
<td style="text-align: left;">Exercises for lectures 5-6</td>
</tr>
<tr class="even">
<td style="text-align: center;">7</td>
<td style="text-align: center;">Jan 20, 2023</td>
<td style="text-align: center;">Fri</td>
<td>10:15 - 11:45</td>
<td style="text-align: left;">Hamiltonian Monte Carlo</td>
</tr>
<tr class="odd">
<td style="text-align: center;">8</td>
<td style="text-align: center;">Jan 23, 2023</td>
<td style="text-align: center;">Mon</td>
<td>10:15 - 11:45</td>
<td style="text-align: left;">Practical aspects of HMC</td>
</tr>
<tr class="even">
<td style="text-align: center;">Ex 4</td>
<td style="text-align: center;">Jan 25, 2023</td>
<td style="text-align: center;">Wed</td>
<td>10:15 - 11:45</td>
<td style="text-align: left;">Exercises for lectures 7-8</td>
</tr>
<tr class="odd">
<td style="text-align: center;">9</td>
<td style="text-align: center;">Jan 27, 2023</td>
<td style="text-align: center;">Fri</td>
<td>10:15 - 11:45</td>
<td style="text-align: left;">Slice sampling</td>
</tr>
<tr class="even">
<td style="text-align: center;">10</td>
<td style="text-align: center;">Jan 30, 2023</td>
<td style="text-align: center;">Mon</td>
<td>10:15 - 11:45</td>
<td style="text-align: left;">Practical aspects, Diagnostics</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Ex 5</td>
<td style="text-align: center;">Feb 01, 2023</td>
<td style="text-align: center;">Wed</td>
<td>10:15 - 11:45</td>
<td style="text-align: left;">Exercises for lectures 9-10</td>
</tr>
<tr class="even">
<td style="text-align: center;">11</td>
<td style="text-align: center;">Feb 03, 2023</td>
<td style="text-align: center;">Fri</td>
<td>10:15 - 11:45</td>
<td style="text-align: left;">TBA</td>
</tr>
<tr class="odd">
<td style="text-align: center;">12</td>
<td style="text-align: center;">Feb 06, 2023</td>
<td style="text-align: center;">Mon</td>
<td>10:15 - 11:45</td>
<td style="text-align: left;">TBA</td>
</tr>
<tr class="even">
<td style="text-align: center;">Ex 6</td>
<td style="text-align: center;">Feb 08, 2023</td>
<td style="text-align: center;">Wed</td>
<td>10:15 - 11:45</td>
<td style="text-align: left;">Exercises for lectures 11-12</td>
</tr>
<tr class="odd">
<td style="text-align: center;">13</td>
<td style="text-align: center;">Feb 10, 2023</td>
<td style="text-align: center;">Fri</td>
<td>10:15 - 11:45</td>
<td style="text-align: left;">TBA</td>
</tr>
</tbody>
</table>
</section>
<section id="topics" class="level2">
<h2 class="anchored" data-anchor-id="topics">Topics</h2>
<section id="lecture-1-introduction" class="level3">
<h3 class="anchored" data-anchor-id="lecture-1-introduction">Lecture 1: Introduction</h3>
<ul>
<li>Motivation</li>
<li>Monte Carlo approximation</li>
<li>An inefficient way of computing <span class="math inline">\(\pi\)</span></li>
</ul>
</section>
<section id="lecture-2-direct-sampling-methods" class="level3">
<h3 class="anchored" data-anchor-id="lecture-2-direct-sampling-methods">Lecture 2: Direct Sampling Methods</h3>
<ul>
<li>Can we beat the curse of dimensionality?</li>
<li>Random number generation</li>
<li>Direct sampling by variable transformation methods</li>
</ul>
</section>
<section id="lecture-3-rejection-and-importance-sampling" class="level3">
<h3 class="anchored" data-anchor-id="lecture-3-rejection-and-importance-sampling">Lecture 3: Rejection and Importance Sampling</h3>
<ul>
<li>More direct sampling methods</li>
<li>Rejection sampling</li>
<li>Importance sampling</li>
</ul>
</section>
<section id="lecture-4-markov-chains" class="level3">
<h3 class="anchored" data-anchor-id="lecture-4-markov-chains">Lecture 4: Markov chains</h3>
<ul>
<li>Markov chains</li>
<li>Some mathematical facts about Markov chains</li>
</ul>
</section>
<section id="lecture-5-the-metropolis-hastings-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="lecture-5-the-metropolis-hastings-algorithm">Lecture 5: The Metropolis-Hastings Algorithm</h3>
<ul>
<li>Fundamental theorem of Markov chains</li>
<li>Metropolis-Hastings algorithm</li>
</ul>
</section>
<section id="lecture-6-gibbs-sampling" class="level3">
<h3 class="anchored" data-anchor-id="lecture-6-gibbs-sampling">Lecture 6: Gibbs sampling</h3>
<ul>
<li>Recap: Metropolis-Hastings algorithm</li>
<li>Combining Markov chains</li>
<li>Gibbs sampling</li>
<li>Auxiliary variable methods</li>
</ul>
</section>
<section id="lecture-7-hamiltonian-monte-carlo" class="level3">
<h3 class="anchored" data-anchor-id="lecture-7-hamiltonian-monte-carlo">Lecture 7: Hamiltonian Monte Carlo</h3>
<ul>
<li>Recap: MCMC + Gibbs Sampling</li>
<li>More on auxiliary variable methods</li>
<li>Hamiltonian Monte Carlo</li>
</ul>
</section>
<section id="lecture-8-hamiltonian-monte-carlo-practical-issues" class="level3">
<h3 class="anchored" data-anchor-id="lecture-8-hamiltonian-monte-carlo-practical-issues">Lecture 8: Hamiltonian Monte Carlo, Practical Issues</h3>
<ul>
<li>Hamiltonian Monte Carlo continued</li>
<li>Practical Issues (convergence, diagnostic checks)</li>
</ul>
</section>
<section id="lecture-9-slice-sampling" class="level3">
<h3 class="anchored" data-anchor-id="lecture-9-slice-sampling">Lecture 9: Slice sampling</h3>
<ul>
<li>General slice sampling</li>
<li>Neal’s bracketing method</li>
</ul>
<section id="lecture-10-practical-aspects-of-mcmc" class="level4">
<h4 class="anchored" data-anchor-id="lecture-10-practical-aspects-of-mcmc">Lecture 10: Practical Aspects of MCMC</h4>
<ul>
<li>Convergence, diagnostic checks</li>
</ul>
</section>
</section>
<section id="lectures-11-13-tba" class="level3">
<h3 class="anchored" data-anchor-id="lectures-11-13-tba">Lectures 11-13: TBA</h3>
<section id="possible-topics" class="level4">
<h4 class="anchored" data-anchor-id="possible-topics">Possible topics</h4>
<ul>
<li>Annealed Importance Sampling</li>
<li>Nested Sampling</li>
<li>Parallel Tempering / Replica-exchange Monte Carlo</li>
<li>Sequential Monte Carlo (SMC)</li>
<li>Graphical models</li>
<li>Ising model</li>
<li>Simulator models</li>
<li>Stochastic differential equation &amp; Langevin dynamics</li>
<li>Bridge sampling, thermodynamic integration</li>
<li>Partition function estimation</li>
<li>Intractable models</li>
<li>Exchange algorithm</li>
<li>Adaptive Monte Carlo methods</li>
<li>Wang-Landau</li>
<li>Exact sampling: coupling from the past</li>
</ul>
</section>
</section>
</section>
</section>
<section id="literature" class="level1">
<h1>Literature</h1>
<ul>
<li><p>Matti Vihola: <a href="http://users.jyu.fi/~mvihola/stochsim/notes-2020.pdf">Lectures on Stochastic Simulation</a></p></li>
<li><p>Radford Neal: <a href="https://www.cs.toronto.edu/~radford/ftp/review.pdf">Probabilistic Inference Using Markov Chain Monte Carlo Methods</a></p></li>
<li><p>Chris Bishop: <a href="https://www.springer.com/gp/book/9780387310732">Pattern Recognition and Machine Learning, Chap. 11</a></p></li>
<li><p>David MacKay: <a href="http://www.inference.org.uk/itprnn/book.pdf">Information Theory, Inference, and Learning Algorithms, Chap. 29 + 30</a></p></li>
<li><p>Iain Murray: <a href="http://homepages.inf.ed.ac.uk/imurray2/pub/07thesis/murray_thesis_2007.pdf">Advances in Markov chain Monte Carlo methods</a></p></li>
<li><p>Andrieu, de Freitas, Doucet, Jordan: <a href="https://link.springer.com/article/10.1023/A:1020281327116">An Introduction to MCMC for Machine Learning</a></p></li>
<li><p>Charles Geyer: <a href="http://si.biostat.washington.edu/sites/default/files/modules/Geyer-Introduction%20to%20markov%20chain%20Monte%20Carlo_0.pdf">Introduction to Markov Chain Monte Carlo</a></p></li>
<li><p>Jun S. Liu: <a href="https://www.springer.com/de/book/9780387763699">Monte Carlo Strategies in Scientific Computing</a></p></li>
<li><p>David A Levin, Yuval Peres: <a href="https://www.academia.edu/download/30694248/recent.pdf">Markov chains and mixing times</a></p></li>
</ul>
</section>
<section id="lecture-1-introduction-1" class="level1">
<h1>Lecture 1: Introduction</h1>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Motivation</li>
<li>Monte Carlo approximation</li>
<li>An inefficient way of computing <span class="math inline">\(\pi\)</span></li>
</ul>
</section>
<section id="why-do-we-need-sampling-methods" class="level2">
<h2 class="anchored" data-anchor-id="why-do-we-need-sampling-methods">Why do we need sampling methods?</h2>
<section id="solving-inference-problems" class="level3">
<h3 class="anchored" data-anchor-id="solving-inference-problems">Solving inference problems</h3>
<p>A major motivation is to use sampling methods for doing Bayesian inference and more generally for solving inference in probabilistic models. But sampling methods are also very important in many other domains such as Physics, Chemistry etc. Monte Carlo techniques are also essential in machine learning.</p>
<p>Being good Bayesians we are interested in the following computational tasks</p>
<ol type="1">
<li><p><strong>Marginalization</strong>: Integrating or summing out uninteresting parameters in a probabilistic model</p></li>
<li><p><strong>Conditioning</strong>: Fixing some variables and evaluating probabilities conditioned on the fixed variables. For example, conditioning on the observed data in order to compute the posterior probability</p></li>
<li><p><strong>Expectation</strong>: Computing the mean value of some function (e.g.&nbsp;computation of the <em>model evidence</em>)</p></li>
</ol>
</section>
<section id="bayesian-inference" class="level3">
<h3 class="anchored" data-anchor-id="bayesian-inference">Bayesian inference</h3>
<p>At its most fundamental level, <a href="https://en.wikipedia.org/wiki/Bayesian_inference">Bayesian inference</a> involves applying the sum and product rule of probability theory to learn a model from some information:</p>
<p><span id="eq-bayes"><span class="math display">\[
    \underbrace{\Pr(\theta\mid D, M)}_{Posterior}\,\, \underbrace{\Pr(D\mid M)}_{Evidence} = \underbrace{\Pr(D\mid \theta, M)}_{Likelihood}\,\, \underbrace{\Pr(\theta\mid M)}_{Prior}
\tag{1}\]</span></span></p>
<p>where <span class="math inline">\(D\)</span> are the data, <span class="math inline">\(M\)</span> is a model with parameters <span class="math inline">\(\theta\)</span>.</p>
<p>There are two main tasks in Bayesian inference:</p>
<ol type="1">
<li><p>estimation of the model parameters <span class="math inline">\(\theta\)</span>,</p></li>
<li><p>comparison of model <span class="math inline">\(M\)</span> to alternative model <span class="math inline">\(M'\)</span>.</p></li>
</ol>
<p>First task requires computations with the (unnormalized) posterior <span class="math inline">\(\Pr(\theta\mid{}D,M)\)</span> or <span class="math inline">\(\Pr(D\mid{}\theta,M)\,\Pr(\theta\mid{}M)\)</span>. The second task involves computation of the model evidence:</p>
<p><span id="eq-evidence"><span class="math display">\[
    \Pr(D\mid{}M) = \int \Pr(D\mid{}\theta, M)\, \Pr(\theta\mid{}M)\, d\theta
\tag{2}\]</span></span></p>
<p>Other important integrals that need to be computed in Bayesian analysis are:</p>
<ul>
<li><p>Marginal posterior: <span class="math display">\[
\Pr(\theta_1\mid{}D,M) = \int \Pr(\theta_1, \theta_2\mid{}D, M)\, d{\theta_2}
\]</span></p></li>
<li><p>Getting rid of <a href="https://en.wikipedia.org/wiki/Nuisance_parameter">nuisance parameters</a>: <span class="math display">\[
\Pr(D\mid{}\theta_1, M) = \int \Pr(D, \theta_2 \mid{} \theta_1, M) \, d{\theta_2}
\]</span></p></li>
<li><p>Getting rid of <a href="https://en.wikipedia.org/wiki/Hyperparameter">hyperparameters</a>: <span class="math display">\[
\Pr(\theta\mid{}D, M) \propto \int \Pr(D\mid{}\theta, M)\, \Pr(\theta\mid{}\alpha, M)\, \Pr(\alpha\mid{}M)\, d{\alpha}
\]</span></p></li>
<li><p>Evaluation of predictive distributions: <span class="math display">\[
\Pr(y\mid{}D,M) = \int \Pr(y\mid{}\theta, M)\, \Pr(\theta \mid{} D, M)\, d{\theta}
\]</span></p></li>
</ul>
</section>
<section id="use-of-monte-carlo-methods-in-machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="use-of-monte-carlo-methods-in-machine-learning">Use of Monte Carlo methods in machine learning</h3>
<p>Monte Carlo methods are also crucial in machine learning. A straightforward application is relevant to supervised learning with large datasets. Assume that the risk functional, which we minimize to train a model <span class="math inline">\(f(x)\)</span>, is</p>
<p><span class="math display">\[
R(f) = \sum_{n=1}^N \ell(y_n, f(x_n))
\]</span></p>
<p>where <span class="math inline">\(\ell(\cdot, \cdot)\)</span> is a loss function that assesses the discrepancy between the observation <span class="math inline">\(y_n\)</span> and the prediction <span class="math inline">\(f(x_n)\)</span> based on a (non-parametric) model <span class="math inline">\(f(x)\)</span>. For large <span class="math inline">\(N\)</span>, evaluation of <span class="math inline">\(R(f)\)</span> can be very costly. A simple strategy to overcome this problem is to train a surrogate of the risk obtained by <em>randomly</em> selecting a small subset from the data, a <em>mini-batch</em> <span class="math inline">\(B\)</span>, and use</p>
<p><span class="math display">\[
\hat R_B(f) = \frac{N}{|B|} \sum_{n\in B} \ell(y_n, f(x_n))
\]</span></p>
<p>as a cost function for training <span class="math inline">\(f(x)\)</span>. <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Stochastic gradient descent</a> follows the gradient of the surrogate rather than the full empirical risk, for example, when training deep neural nets.</p>
<p>Another application is learning of <em>intractable models</em> in representation learning (unsupervised learning). Many important models such as <a href="https://en.wikipedia.org/wiki/Boltzmann_machine">Boltzmann machines</a> or <a href="https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine">restrictive Boltzmann machines</a> are members of the exponential family involving a vector of features <span class="math inline">\(f(y)\)</span>:</p>
<p><span class="math display">\[
\Pr(y\mid{}\theta) = \frac{1}{Z(\theta)} \exp\bigl\{\theta^T\!\!f(y)\bigr\}\,\,\,\text{with}\,\,\, Z(\theta) = \int \exp\bigl\{\theta^T\!\!f(y)\bigr\} dy.
\]</span></p>
<p>The normalizing constant <span class="math inline">\(Z(\theta)\)</span> is called the <em>partition function</em> (by borrowing the terminology from <a href="https://en.wikipedia.org/wiki/Partition_function_(statistical_mechanics)">Statistical Physics</a>). <span class="math inline">\(Z(\theta)\)</span> is often <em>intractable</em>, meaning that we don’t have a closed form expression for evaluating <span class="math inline">\(Z(\theta)\)</span>. Computation of <span class="math inline">\(Z(\theta)\)</span> is formally analogous to computing the model evidence in Bayesian inference.</p>
<p>The log likelihood of observing <span class="math inline">\(y\)</span> is</p>
<p><span class="math display">\[
\log \Pr(y\mid{}\theta) = \theta^T\!\!f(y) - \log Z(\theta)
\]</span></p>
<p>and maximized by following its gradient</p>
<p><span class="math display">\[
\nabla_\theta \log \Pr(y\mid{}\theta) = f(y) - \mathbb E_{\Pr(y\mid{}\theta)}[f]\,\,\,\text{where}\,\,\,\mathbb E_{\Pr(y\mid{}\theta)}[f] = \int f(y) \Pr(y\mid{}\theta)\, dy = \sum_{y \in \mathcal X} f(y)\, \Pr(y \mid{} \theta)
\]</span></p>
<p>If computation of <span class="math inline">\(Z(\theta)\)</span> is challenging, then also the expectation of the features <span class="math inline">\(\mathbb E[f]\)</span> is oftentimes not available in analytically closed form. In this case, one often resorts to a Monte Carlo approximation of <span class="math inline">\(\mathbb E[f]\)</span>, for example, in the <a href="http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf"><em>contrastive divergence</em></a> approach by Geoffrey Hinton.</p>
</section>
</section>
<section id="generating-representative-states" class="level2">
<h2 class="anchored" data-anchor-id="generating-representative-states">Generating representative states</h2>
<p>In addition to the motivation coming from (Bayesian) learning, we also need sampling methods to answer queries over probabilistic models approximately. A sum over the entire sample space is approximated by a (weighted) sum over representative samples. Another motivation for developing sampling methods is to generate representative configurations for visualization.</p>
<section id="example-sampling-protein-structures" class="level3">
<h3 class="anchored" data-anchor-id="example-sampling-protein-structures">Example: Sampling protein structures</h3>
<p>As an example for a high-dimensional continuous sample space, we consider the three-dimensional structure of large biomolecules such as proteins. Here, the parameters <span class="math inline">\(x\)</span> are the angles parameterizing a protein configuration (these are the so-called <a href="https://en.wikipedia.org/wiki/Dihedral_angle#Proteins">dihedral angles</a>, rotational degrees of freedom about chemical bonds). Proteins are linear polymers, i.e.&nbsp;chain molecules with a backbone from which side-chain branch off. <a href="https://www.rcsb.org/3d-view/6YQ5">Here</a> is an example of a protein structure that was computed with Monte Carlo methods by sampling a posterior distribution over the dihedral angles.</p>
</section>
<section id="example-sampling-of-ising-models" class="level3">
<h3 class="anchored" data-anchor-id="example-sampling-of-ising-models">Example: Sampling of Ising models</h3>
<p>Ising models are very simple graphical models. Each node can have two colors only, <span class="math inline">\(\{-1, +1\}\)</span>. The graph is a regular square lattice with nearest neighbor edges but no connections otherwise (except for the periodic boundary conditions). The <a href="https://en.wikipedia.org/wiki/Ising_model">Ising model</a> originates in Statistical Physics and was introduced in 1925 by Ernst Ising as a model for spontaneous magnetization observed in ferromagnetic materials.</p>
<p>The probability of a state <span class="math inline">\(x\in\{-1, +1\}^{L\times L}\)</span> is</p>
<p><span class="math display">\[
p(x) = \frac{1}{Z(\beta)} \exp\biggl\{\beta \sum_{i\sim j} x_i x_j \biggr\}
= \frac{1}{Z(\beta)} \exp\bigl\{-\beta\, E(x) \bigr\}
\]</span></p>
<p>where the sum runs over all nearest neighbors <span class="math inline">\(i\sim j\)</span> on the <a href="https://en.wikipedia.org/wiki/Square_lattice_Ising_model">square lattice</a> of edge length <span class="math inline">\(L\)</span>, i.e.&nbsp;each spin has four neighbors with whom it interacts. The energy (negative log probability)</p>
<p><span class="math display">\[
E(x) = -\sum_{i\sim j} x_i x_j
\]</span></p>
<p>favors configurations in which neighboring spins are aligned. The partition function is</p>
<p><span class="math display">\[
Z(\beta) = \sum_{i\sim j} \exp\bigl\{-\beta\, E(x) \bigr\}
\]</span></p>
<p>The parameters <span class="math inline">\(\beta &gt; 0\)</span> is the inverse temperature (from a physical perspective). For <span class="math inline">\(\beta=0\)</span>, we have a uniform distribution over the hypercube. For increasing <span class="math inline">\(\beta\)</span>, the configurations become more and more fragmented forming patches of spins with the same orientation. For large <span class="math inline">\(\beta\)</span> (such as 1), practically only the configurations with all spins up <span class="math inline">\(x_i=+1\)</span> or down <span class="math inline">\(x_i=-1\)</span> have a non-vanishing probability.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext Cython</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>cython</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>cimport cython</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>cimport numpy <span class="im">as</span> np</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pylab <span class="im">as</span> plt</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> libc.math cimport exp</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> libc.stdlib cimport rand</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>cdef extern <span class="im">from</span> <span class="st">"limits.h"</span>:</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> RAND_MAX</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="at">@cython.boundscheck</span>(<span class="va">False</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="at">@cython.wraparound</span>(<span class="va">False</span>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ising_energy(np.int64_t[:, :] x):</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    cdef <span class="bu">int</span> N <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    cdef <span class="bu">int</span> M <span class="op">=</span> x.shape[<span class="dv">1</span>]</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    cdef <span class="bu">int</span> E <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    cdef <span class="bu">int</span> i, j</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(M):</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>            E <span class="op">+=</span> x[i,j] <span class="op">*</span> (x[i,(j<span class="op">+</span><span class="dv">1</span>)<span class="op">%</span>M] <span class="op">+</span> x[(i<span class="op">+</span><span class="dv">1</span>)<span class="op">%</span>N, j])</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>E</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="at">@cython.boundscheck</span>(<span class="va">False</span>)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="at">@cython.wraparound</span>(<span class="va">False</span>)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ising_sweep(np.int64_t[:, :] x, <span class="bu">float</span> beta<span class="op">=</span><span class="fl">0.4</span>):</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    cdef <span class="bu">int</span> N <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    cdef <span class="bu">int</span> M <span class="op">=</span> x.shape[<span class="dv">1</span>]</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    cdef <span class="bu">int</span> n_offset, m_offset, n, m</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n_offset <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> m_offset <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(n_offset, N, <span class="dv">2</span>):</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> m <span class="kw">in</span> <span class="bu">range</span>(m_offset, M, <span class="dv">2</span>):</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>                    ising_flip(x, n, m, beta)</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(x)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a><span class="at">@cython.boundscheck</span>(<span class="va">False</span>)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a><span class="at">@cython.wraparound</span>(<span class="va">False</span>)</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>cdef ising_flip(np.int64_t[:, :] x, <span class="bu">int</span> i, <span class="bu">int</span> j, <span class="bu">float</span> beta):</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>    cdef <span class="bu">int</span> total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>    cdef <span class="bu">int</span> N <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>    cdef <span class="bu">int</span> M <span class="op">=</span> x.shape[<span class="dv">1</span>]    </span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>    cdef <span class="bu">float</span> dE <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> x[i, j] <span class="op">*</span> (x[(i<span class="op">-</span><span class="dv">1</span>)<span class="op">%</span>N,j] <span class="op">+</span> x[(i<span class="op">+</span><span class="dv">1</span>)<span class="op">%</span>N,j] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>                                   x[i,(j<span class="op">-</span><span class="dv">1</span>)<span class="op">%</span>M] <span class="op">+</span> x[i,(j<span class="op">+</span><span class="dv">1</span>)<span class="op">%</span>M])</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> dE <span class="op">&lt;=</span> <span class="dv">0</span>:</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>        x[i, j] <span class="op">*=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> exp(<span class="op">-</span>dE <span class="op">*</span> beta) <span class="op">*</span> RAND_MAX <span class="op">&gt;</span> rand():</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>        x[i, j] <span class="op">*=</span> <span class="op">-</span><span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>In file included from /opt/hostedtoolcache/Python/3.10.9/x64/lib/python3.10/site-packages/numpy/core/include/numpy/ndarraytypes.h:1948,
                 from /opt/hostedtoolcache/Python/3.10.9/x64/lib/python3.10/site-packages/numpy/core/include/numpy/ndarrayobject.h:12,
                 from /opt/hostedtoolcache/Python/3.10.9/x64/lib/python3.10/site-packages/numpy/core/include/numpy/arrayobject.h:5,
                 from /home/runner/.cache/ipython/cython/_cython_magic_7bae0d7313cdd423600d65a294630b12.c:769:
/opt/hostedtoolcache/Python/3.10.9/x64/lib/python3.10/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning "Using deprecated NumPy API, disable it with " "#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
   17 | #warning "Using deprecated NumPy API, disable it with " \
      |  ^~~~~~~</code></pre>
</div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> <span class="dv">2</span><span class="op">**</span><span class="dv">8</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.choice([<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>],size<span class="op">=</span>(L,L))</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>betas <span class="op">=</span> [<span class="fl">0.</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>, <span class="fl">0.44</span>, <span class="fl">0.45</span>, <span class="fl">0.8</span>]</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> [x.copy()]</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> beta <span class="kw">in</span> betas[<span class="dv">1</span>:]:</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    X.append(x.copy())</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        ising_sweep(X[<span class="op">-</span><span class="dv">1</span>], beta)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="bu">len</span>(betas)<span class="op">//</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> <span class="bu">list</span>(ax.flat)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a, beta, x <span class="kw">in</span> <span class="bu">zip</span>(ax, betas, X):</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    a.set_title(<span class="vs">r'$\beta=</span><span class="sc">{0:.2f}</span><span class="vs">$'</span>.<span class="bu">format</span>(beta))</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    a.matshow(x, cmap<span class="op">=</span>plt.cm.gray_r)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    a.xaxis.set_visible(<span class="va">False</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    a.yaxis.set_visible(<span class="va">False</span>)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-4-output-1.png" width="984" height="565"></p>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Notation and conventions
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Most of the time I will consider a <em>probability density function</em> (pdf) or <em>probability mass function</em> (pmf) <span class="math inline">\(p(x)\)</span> where <span class="math inline">\(x\)</span> could be a parameter vector of a probabilistic model (e.g.&nbsp;in case we want to do computations with the posterior) or the observations from which we want to learn a model. For continuous sample spaces <span class="math inline">\(\mathcal X\)</span>, we are dealing with a <strong>pdf</strong>. In case of a discrete sample space <span class="math inline">\(\mathcal X\)</span> (finite or countably infinite), <span class="math inline">\(p(x)\)</span> is a <strong>pmf</strong>.</p></li>
<li><p>It is generally hard to compute normalized probabilities, but also not really necessary for many sampling algorithms. Therefore, often <span class="math inline">\(p(x)\)</span> is just a nonnegative function, and we assume that the integral <span class="math inline">\(\int_{\mathcal X} p(x)dx &lt; \infty\)</span> or sum <span class="math inline">\(\sum_{x\in\mathcal X} p(x) &lt; \infty\)</span> is finite.</p></li>
<li><p>Often, the computation that we need to carry out can be expressed as the <strong>expectation</strong> of some quantity under a probability. We will denote expectations by</p>
<p><span class="math display">\[
\mathbb{E}_{p}[f] = \int_{\mathcal X} f(x)\, p(x)\,  dx
\]</span> e.g.&nbsp;the model evidence used in Bayesian model comparison is <span class="math inline">\(\Pr(D\mid{}M) = \mathbb E_{\Pr(\theta\mid{}M)}[\Pr(D\mid{}\theta,M)]\)</span>.</p></li>
<li><p>The notation <span id="eq-sample_from"><span class="math display">\[
x \sim p(x)
\tag{3}\]</span></span></p>
<p>means that <span class="math inline">\(x\)</span> follows the distribution <span class="math inline">\(p\)</span></p></li>
</ul>
</div>
</div>
<p>At <span class="math inline">\(\beta = \frac{\log(1 + \sqrt 2)}{2} \approx 0.44\)</span>, something peculiar happens. The “attractive forces” that tend to align neighboring spins become so dominant that large regions form. Across these regions, all spins have a similar orientation. This is a <em>phase transition</em>. Similar phenomena occur also in learning large probabilistic models where the prior and the likelihood often favor distinct regions in parameter space.</p>
<p>In graphical models such as the Ising model all computations are finite but have an exponential complexity. For a square lattice of length <span class="math inline">\(L\)</span> there are <span class="math inline">\(2^{L^2}\)</span> possible states. Visiting all states is not an option even for small to moderate lattice sizes such as <span class="math inline">\(L=32\)</span>. In this case, we have <span class="math inline">\(2^{1024}\)</span> states. Compare this to the number of atoms in the universe which is estimated to be approx. <span class="math inline">\(2^{266}\)</span>. As an aside, according to <a href="https://en.wikipedia.org/wiki/Eddington_number">Eddington</a> the number of electrons in the universe is in fact</p>
<p><span class="math inline">\(15747724136275002577605653961181555468044717914527116709366231425076185631031296 \approx 2^{263}\)</span></p>
<p>Anyways, if each atom were a computing device and running since the Bing Bang, i.e.&nbsp;for <span class="math inline">\(13.5 \text{ billion years} \approx 2^{59} s\)</span>, at a rate of <span class="math inline">\(3.3 \text{Gz} =2^{32}\)</span> Hz, then we could have visited <span class="math inline">\(2^{357}\)</span> states. This is only a vanishingly small fraction of the entire state space of the <span class="math inline">\(32\times 32\)</span> Ising model. We would have to wait for <span class="math inline">\(2^{667} \simeq 10^{201}\)</span> universe life times until we had visited all possible states.</p>
<p>For a thorough discussion read the Chap. 29 in David MacKay’s book <a href="http://www.inference.org.uk/itprnn/book.pdf">Information Theory, Inference, and Learning Algorithms</a>.</p>
</section>
</section>
<section id="monte-carlo-methods" class="level2">
<h2 class="anchored" data-anchor-id="monte-carlo-methods">Monte Carlo methods</h2>
<p>The basic idea of <strong>Monte Carlo methods</strong> is to use a random process to compute a (deterministic) quantity. That is, we give up deterministic guarantees and satisfy ourselves with statistical guarantees: we resort to gambling (therefore the name “Monte Carlo”). For a historical background on the beginning of modern Monte Carlo methods have a look at the recollections of pioneers Nick Metropolis (<a href="https://fas.org/sgp/othergov/doe/lanl/pubs/00326866.pdf">The Beginning of the Monte Carlo Method</a>) and Roger Eckhard (<a href="https://fas.org/sgp/othergov/doe/lanl/pubs/00326867.pdf">Stan Ulam, John von Neumann, and the Monte Carlo Method</a>). Using a random experiment to compute a quantity is at least as old as <a href="https://en.wikipedia.org/wiki/Buffon%27s_needle_problem">Buffon’s needle problem</a>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="images/McCracken_TheMonteCarloMethod_Fig1.png" title="Buffon's needle" class="img-fluid figure-img">
</figure>
<p></p><figcaption class="figure-caption">Buffon’s needle</figcaption><p></p>
</figure>
</div>
<p>Figure from <a href="https://www.jstor.org/stable/24944647?seq=1#metadata_info_tab_contents">McCracken: The Monte Carlo Method</a>.</p>
<p>The idea of the Monte Carlo method for probabilistic inference is simple: Instead of computing the integral/sum by systematically visiting all possible states in <span class="math inline">\(\mathcal X\)</span>, we (randomly) pick those states that are likely to contribute strongly to the sum/integral:</p>
<p><span id="eq-sampling"><span class="math display">\[
\mathbb{E}_{p}[f] = \int_{\mathcal X} f(x)\, p(x)\,  dx \approx \hat f_S := \frac{1}{S} \sum_{s=1}^S f(x^{(s)})\,\,\,\text{with}\,\,\,  x^{(s)} \sim p(x)
\tag{4}\]</span></span> where <span class="math inline">\(x^{(s)}\)</span> are <span class="math inline">\(S\in\mathbb N\)</span> samples from <span class="math inline">\(p(x)\)</span> (the index <span class="math inline">\(s\)</span> enumerates all samples) and <span class="math inline">\(\hat f_S\)</span> is a <em>Monte Carlo estimate</em> or <em>Monte Carlo approximation</em> of <span class="math inline">\(\mathbb E_p[f]\)</span>. Our hope is that with <span class="math inline">\(S\to\infty\)</span>, the approximation becomes better and better. This is indeed the case, as we will see in a second.</p>
<section id="monte-carlo-as-density-estimation" class="level3">
<h3 class="anchored" data-anchor-id="monte-carlo-as-density-estimation">Monte Carlo as density estimation</h3>
<p>The Monte Carlo approximation can also be viewed as a <em>density estimation</em> approach since the estimate <span class="math inline">\(\hat f_S\)</span> can be interpreted as the expectation under the approximate probability <span id="eq-approximate_pdf"><span class="math display">\[
\hat p_S(x) = \frac{1}{S} \sum_{s=1}^S \delta(x - x^{(s)})
\tag{5}\]</span></span> where <span class="math inline">\(\delta(\cdot)\)</span> is the <a href="https://en.wikipedia.org/wiki/Dirac_delta_function">delta distribution</a>: <span id="eq-sampling2"><span class="math display">\[
\hat f_S := \frac{1}{S} \sum_{s=1}^S f(x^{(s)}) = \mathbb E_{\hat p_S}[f]
\tag{6}\]</span></span> We approximate the true probability <span class="math inline">\(p(x)\)</span> with a Monte Carlo estimate <span class="math inline">\(\hat p_S(x)\)</span> obtained at <span class="math inline">\(S\)</span> samples <span class="math inline">\(x^{(s)}\)</span> where</p>
<p><span class="math display">\[
|\hat p_S - p| \to 0\,\,\, \text{for}\,\,\, S\to \infty
\]</span></p>
<p>in some appropriate norm <span class="math inline">\(|\cdot|\)</span>.</p>
</section>
<section id="why-does-it-work" class="level3">
<h3 class="anchored" data-anchor-id="why-does-it-work">Why does it work?</h3>
<section id="unbiasedness" class="level4">
<h4 class="anchored" data-anchor-id="unbiasedness">Unbiasedness</h4>
<p>The joint distribution of all Monte Carlo samples is simply a product density, because all samples are generated <em>independently</em> of each other: <span id="eq-MC-sample-independece"><span class="math display">\[
p_S(x^{(1)}, \ldots, x^{(S)}) = \prod_{s=1}^S p(x^{(s)})
\tag{7}\]</span></span></p>
<p>The Monte Carlo estimate <span class="math inline">\(\hat f_S\)</span> is a random quantity, because with each realization of <span class="math inline">\(x^{(1)}, \ldots, x^{(S)}\)</span> we obtain a different result. We can compute the first two moments of <span class="math inline">\(\hat f_S\)</span>:</p>
<p><span id="eq-MCbias"><span class="math display">\[
\mathbb E_{p_S}[\hat f_S] = \frac{1}{S} \sum_{s=1}^S \mathbb{E}_p[f(x^{(s)})] = \mathbb{E}_p[f] =: \mu
\tag{8}\]</span></span> That is, the Monte Carlo estimate of <span class="math inline">\(\mathbb E_p[f]\)</span> is <strong>unbiased</strong>.</p>
<p>How accurate is the estimate on average (i.e.&nbsp;how close do we get to the true value if we run many replications of the sampling procedure)? To answer this question, we compute the variance</p>
<p><span id="eq-MCvariance"><span class="math display">\[
    \text{var}_{p_S}[\hat f_S] = \mathbb{E}_{p_S}\bigl[(\hat f_S - \mu)^2\bigr] =
    \frac{1}{S} \text{var}_{p}[f]\, .
\tag{9}\]</span></span></p>
<p>To see the validity of the last result, we first write</p>
<p><span class="math display">\[
\begin{aligned}
    \text{var}_{p_S}[\hat f_S]
    &amp;= \mathbb{E}_{p_S}\bigl[(\hat f_S - \mu)^2\bigr] \\
    &amp;= \mathbb{E}_{p_S}\bigl[\frac{1}{S^2} \sum_{s,s'} \underbrace{(f(x^{(s)}) - \mu)}_{\tilde f(x^{(s)})} (f(x^{(s')}) - \mu))\bigr] \\
    &amp;= \mathbb{E}_{p_S}\bigl[\frac{1}{S^2} \sum_{s,s'} \tilde f(x^{(s)}) \tilde f(x^{(s')}) \bigr] \\
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\tilde f(x^{(s)}) := f(x^{(s)}) - \mu\)</span> has a vanishing first moment:</p>
<p><span class="math display">\[
\mathbb{E}_{p_S}\bigl[ \tilde f(x^{(s)}) \bigr]
= \mathbb{E}_{p(x^{(s)})}\bigl[ \tilde f(x^{(s)}) \bigr]
= \mathbb{E}_{p}[f] - \mu = 0\, .
\]</span></p>
<p>The second moment is identical to the variance of f:</p>
<p><span class="math display">\[
\mathbb{E}_{p_S}\bigl[( \tilde f(x^{(s)}) )^2 \bigr]
= \mathbb{E}_{p(x^{(s)})} \bigl[( \tilde f(x^{(s)}) )^2 \bigr]
= \mathbb{E}_{p}[(f - \mu)^2]
= \text{var}_{p}[f]\, .
\]</span></p>
<p>It follows that <span class="math display">\[
\begin{aligned}
    \text{var}_{p_S}[\hat f_S]
    &amp;= \mathbb{E}_{p_S}\bigl[\frac{1}{S^2} \sum_{s,s'} \tilde f(x^{(s)}) \tilde f(x^{(s')}) \bigr] \\
    &amp;= \frac{1}{S^2} \sum_{s,s'} \mathbb{E}_{p_S}\bigl[\tilde f(x^{(s)}) \tilde f(x^{(s')}) \bigr] \\
    &amp;= \frac{1}{S^2} \sum_{s} \mathbb{E}_{p_S}\bigl[(\tilde f(x^{(s)}))^2 \bigr] + \frac{1}{S^2} \sum_{s\not= s'} \mathbb{E}_{p_S}\bigl[\tilde f(x^{(s)}) \tilde f(x^{(s')}) \bigr] \\
    &amp;= \frac{1}{S} \text{var}_{p}[f] + \frac{1}{S^2} \sum_{s\not= s'} \underbrace{\mathbb{E}_{p(x^{(s)})}\bigl[\tilde f(x^{(s)})\bigr]}_{0} \, \mathbb{E}_{p(x^{(s')})}\bigl[ \tilde f(x^{(s')}) \bigr] \\
    &amp;= \frac{1}{S} \text{var}_{p}[f] \\
\end{aligned}
\]</span></p>
<p>where we’ve made use of the linearity of expectations in the second equality, and of the independence of the samples (eq. <a href="#eq-MC-sample-independece">Equation&nbsp;7</a>) in the third.</p>
<p>The result shows that Monte Carlo error bars shrink like <span class="math inline">\(1/\sqrt{S}\)</span>:</p>
<p><span id="eq-MCerror"><span class="math display">\[
\sigma(\hat f_S) := \sqrt{\text{var}[\hat f_S]} = \sigma(f) / \sqrt{S}
\tag{10}\]</span></span></p>
<p>where the proportionality constant <span class="math inline">\(\sigma(f)=\sqrt{\text{var}[f]}\)</span> depends on the specific estimation problem. In practice, <span class="math inline">\(\sigma(f)\)</span> is not available (after all we are doing Monte Carlo because we cannot do the sum/integrals that are necessary to compute means and variances…). However, we can use Monte Carlo to estimate <span class="math inline">\(\sigma(f)\)</span>.</p>
</section>
<section id="asymptotic-guarantees" class="level4">
<h4 class="anchored" data-anchor-id="asymptotic-guarantees">Asymptotic guarantees</h4>
<p>So far, we studied the behavior of the Monte Carlo estimator for fixed number of samples <span class="math inline">\(S\)</span> and many repetitions. Let us now look at the limit <span class="math inline">\(S\to\infty\)</span>. We have (almost surely)</p>
<p><span id="eq-slln"><span class="math display">\[
\hat f_S \overset{S\to\infty}{\longrightarrow} \mu = \mathbb E_p[f]
\tag{11}\]</span></span></p>
<p>This result is known as the <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers"><em>strong law of large numbers</em></a>.</p>
<p>If <span class="math inline">\(\text{var}[f] &lt; \infty\)</span>, then the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem"><strong>Central limit theorem</strong> (CLT)</a>, a fundamental theorem in Statistics, guarantees that Monte Carlo works:</p>
<p><span id="eq-CLT"><span class="math display">\[
\hat f_S \overset{S\to\infty}{\longrightarrow} \mathcal N\left(\mathbb E_p[f], \sigma(f)^2/S\right)
\tag{12}\]</span></span></p>
<p>where <span class="math inline">\(\mathcal N(\mu, \sigma^2)\)</span> is the <a href="https://en.wikipedia.org/wiki/Normal_distribution">Normal distribution</a> with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma\)</span>.</p>
<p>But these are only statistical, asymptotic guarantees for the convergence of Monte Carlo methods.</p>
</section>
</section>
<section id="application-a-slightly-silly-way-to-estimate-pi" class="level3">
<h3 class="anchored" data-anchor-id="application-a-slightly-silly-way-to-estimate-pi">Application: A slightly silly way to estimate <span class="math inline">\(\pi\)</span></h3>
<p>Let us use a simple Monte Carlo approach to estimate <span class="math inline">\(\pi\)</span>. We have</p>
<p><span id="eq-pi"><span class="math display">\[
\pi = \int_{[-1,1]^2} \mathbb 1(x^2 + y^2 &lt; 1)\, \, dx dy = \int_{\mathcal X} f(x, y)\, p(x, y)\, dxdy
\tag{13}\]</span></span></p>
<p>where <span class="math inline">\(\mathbb 1(\cdot)\)</span> is the indicator function. In this example, the distribution <span class="math inline">\(p(x,y)=1/4\)</span> is the uniform distribution over the square <span class="math inline">\(\mathcal X = [-1, 1]^2\)</span> and <span class="math inline">\(f(x, y) = 4\cdot\mathbb 1(x^2 + y^2 &lt; 1)\)</span>. We have:</p>
<p><span class="math display">\[
\mathbb E[f] = \pi\,\,\,\text{and}\,\,\,\text{var}[f] = (4-\pi)\, \pi
\]</span></p>
<p>This integral can be approximated by:</p>
<p><span id="eq-pi_MC"><span class="math display">\[
\pi \approx \frac{1}{S} \sum_{s=1}^S f(x^{(s)}, y^{(s)})
\tag{14}\]</span></span></p>
<p>where <span class="math inline">\((x^{(s)}, y^{(s)})\)</span> are picked randomly from the unit square. The approximate value of <span class="math inline">\(\pi\)</span> is just four times the fraction of sampling points that land in the unit disk.</p>
<p>Let’s code it in Python:</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># graphical illustration</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.patches <span class="im">import</span> Circle</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">1</span>, <span class="op">+</span><span class="dv">1</span>, size<span class="op">=</span>(<span class="dv">2</span>, S))</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> np.<span class="bu">sum</span>(x<span class="op">**</span><span class="dv">2</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>circle <span class="op">=</span> Circle((<span class="fl">0.</span>,<span class="fl">0.</span>), radius<span class="op">=</span><span class="fl">1.</span>, facecolor<span class="op">=</span><span class="st">'b'</span>, alpha<span class="op">=</span><span class="fl">0.1</span>, edgecolor<span class="op">=</span><span class="st">'k'</span>, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>ax.scatter(<span class="op">*</span>x[:,r<span class="op">&lt;=</span><span class="dv">1</span>], color<span class="op">=</span><span class="st">'b'</span>, s<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>ax.scatter(<span class="op">*</span>x[:,r<span class="op">&gt;</span><span class="dv">1</span>], color<span class="op">=</span><span class="st">'r'</span>, s<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>ax.add_patch(circle)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-5-output-1.png" width="470" height="470"></p>
</div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> estimate_pi(S):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Monte Carlo estimate of pi.</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co">    S : number of samples</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># pick S points from unit square</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, size<span class="op">=</span>(<span class="dv">2</span>, <span class="bu">int</span>(S)))</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute squared distance from center</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> np.<span class="bu">sum</span>(x<span class="op">**</span><span class="dv">2</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># fraction of points in unit circle</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">4</span> <span class="op">*</span> np.mean(r <span class="op">&lt;</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>How well does this simple Monte Carlo procedure work, if we increase the number of sampling points?</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> np.logspace(<span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">10</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>estimates <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(estimate_pi, S))</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>kw <span class="op">=</span> <span class="bu">dict</span>(xlabel<span class="op">=</span><span class="vs">r'number of samples $S$'</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>), subplot_kw<span class="op">=</span>kw)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(S, estimates, color<span class="op">=</span><span class="st">'k'</span>, lw<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].axhline(np.pi, ls<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'r'</span>, lw<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="vs">r'estimated $\pi$'</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(S, np.fabs(np.array(estimates)<span class="op">-</span>np.pi), color<span class="op">=</span><span class="st">'k'</span>, lw<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="vs">r'error'</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>np.<span class="bu">round</span>(np.transpose([S, estimates]), <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>array([[1.00000000e+03, 3.12400000e+00],
       [2.15443500e+03, 3.12200000e+00],
       [4.64158900e+03, 3.20100000e+00],
       [1.00000000e+04, 3.12900000e+00],
       [2.15443470e+04, 3.15200000e+00],
       [4.64158880e+04, 3.13500000e+00],
       [1.00000000e+05, 3.13800000e+00],
       [2.15443469e+05, 3.14100000e+00],
       [4.64158883e+05, 3.14200000e+00],
       [1.00000000e+06, 3.14200000e+00]])</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-7-output-2.png" width="949" height="468"></p>
</div>
</div>
<p>To quantify the statistical error, we run multiple replications of the procedure:</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>n_rep <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>estimates <span class="op">=</span> np.array([<span class="bu">list</span>(<span class="bu">map</span>(estimate_pi, S)) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_rep)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>mean_pi <span class="op">=</span> estimates.mean(<span class="dv">0</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>std_pi <span class="op">=</span> estimates.std(<span class="dv">0</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>var <span class="op">=</span> np.pi <span class="op">*</span> (<span class="fl">4.0</span> <span class="op">-</span> np.pi)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>), subplot_kw<span class="op">=</span>kw)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>ax.fill_between(S, mean_pi <span class="op">-</span> np.pi <span class="op">+</span> std_pi, mean_pi <span class="op">-</span> np.pi <span class="op">-</span> std_pi,</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>                color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>ax.plot(S, mean_pi <span class="op">-</span> np.pi, lw<span class="op">=</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>ax.plot(S, <span class="op">-</span>(var<span class="op">/</span>S)<span class="op">**</span><span class="fl">0.5</span>, color<span class="op">=</span><span class="st">'r'</span>, ls<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>ax.plot(S, <span class="op">+</span>(var<span class="op">/</span>S)<span class="op">**</span><span class="fl">0.5</span>, color<span class="op">=</span><span class="st">'r'</span>, ls<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r'Monte Carlo error'</span>)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>ax.semilogx()</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-9-output-1.png" width="758" height="466"></p>
</div>
</div>
</section>
</section>
</section>
<section id="lecture-2-direct-sampling-methods-1" class="level1">
<h1>Lecture 2: Direct Sampling Methods</h1>
<section id="outline-1" class="level2">
<h2 class="anchored" data-anchor-id="outline-1">Outline</h2>
<ul>
<li>Can we beat the curse of dimensionality?</li>
<li>Random number generation</li>
<li>Direct sampling by variable transformation methods</li>
</ul>
<section id="a-warning" class="level3">
<h3 class="anchored" data-anchor-id="a-warning">A warning</h3>
<p>Alan Sokal (<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.49.4444&amp;rep=rep1&amp;type=pdf">Monte Carlo methods in statistical mechanics</a>):</p>
<blockquote class="blockquote">
<p>Monte Carlo is an extremely bad method; it should be used only when all alternative methods are worse.</p>
</blockquote>
<p>Why is this so? As we saw Monte Carlo methods have a statistical error that roughly scales with <span class="math inline">\(1/\sqrt{\text{computational budget}}\)</span>. Typically, for low-dimensional problems other numerical methods scale much better. For example, even simple quadrature methods such as <a href="https://en.wikipedia.org/wiki/Simpson%27s_rule">Simpon’s rule</a> have an error that scales with <span class="math inline">\(\mathcal O(S^{-4/D})\)</span> rather than <span class="math inline">\(\mathcal O(S^{-1/2})\)</span> where <span class="math inline">\(D\)</span> is the dimension of the integrand. This means that for <span class="math inline">\(D\le 8\)</span>, Simpson’s rule will be more efficient than Monte Carlo. However, in higher dimensions this is often no longer the case, and we have to resort to Monte Carlo methods.</p>
<p>Our specific application of Monte Carlo (estimation of <span class="math inline">\(\pi\)</span>) is a clear case where Sokal’s warning applies. If we use numerical quadrature, we can achieve a high accuracy with much less effort:</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pylab <span class="im">as</span> plt</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.integrate <span class="im">import</span> quad</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>val, err, info <span class="op">=</span> quad(<span class="kw">lambda</span> x: (<span class="dv">1</span><span class="op">-</span>x<span class="op">**</span><span class="dv">2</span>)<span class="op">**</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>), <span class="fl">0.</span>, <span class="fl">1.</span>, full_output<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'estimate: </span><span class="sc">{</span><span class="dv">4</span><span class="op">*</span>val<span class="sc">}</span><span class="ss"> based on </span><span class="sc">{</span>info[<span class="st">"neval"</span>]<span class="sc">}</span><span class="ss"> points'</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'accuracy: </span><span class="sc">{</span><span class="bu">abs</span>(<span class="dv">4</span><span class="op">*</span>val<span class="op">-</span>np.pi)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>estimate: 3.1415926535897922 based on 231 points
accuracy: 8.881784197001252e-16</code></pre>
</div>
</div>
</section>
</section>
<section id="do-we-beat-the-curse-of-dimensionality" class="level2">
<h2 class="anchored" data-anchor-id="do-we-beat-the-curse-of-dimensionality">Do we beat the curse of dimensionality?</h2>
<p>Although Monte Carlo doesn’t depend explicitly on the dimension of the sample space, it does so in practice. If we go back to our expression for the Monte Carlo error (Eq. <a href="#eq-MCerror">Equation&nbsp;10</a>)</p>
<p><span class="math display">\[
\sigma(\hat f_S) = \sigma(f) / \sqrt{S}
\]</span></p>
<p>for a generalized version of the <span class="math inline">\(\pi\)</span> estimation approach, then the dependence on the dimension of the sampling problem becomes apparent.</p>
<p>In the generalized version, we estimate the volume of a <a href="https://en.wikipedia.org/wiki/Ball_(mathematics)"><span class="math inline">\(D\)</span>-dimensional unit-ball</a> <span class="math inline">\(V(D)\)</span> by the following Monte Carlo procedure:</p>
<ul>
<li>Pick a point from a unit hypercube (assuming that this can be done easily)</li>
<li>Check if point lies inside ball</li>
</ul>
<p>Written out in equations where <span class="math inline">\(x\)</span> is now a <span class="math inline">\(D\)</span>-dimensional vector:</p>
<p><span class="math display">\[
V(D) = \int \mathbb 1(\|x\| \le 1) dx = \int p(x)\, f(x)\, dx
\]</span> with <span class="math display">\[
f(x) = 2^D\, \mathbb 1(\|x\| \le 1)\,\,\, \text{and} \,\,\, p(x) = \frac{1}{2^D} \mathbb 1(x \in [-1, 1]^D)
\]</span></p>
<p>We can compute how the Monte Carlo error scales by evaluating the mean and variance of <span class="math inline">\(f\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbb E[f] &amp;= V(D)\\
    \text{var}[f] &amp;= \mathbb E[f^2]  - V^2(D) = 2^D V(D) - V^2(D) = \bigl(2^D - V(D)\bigr)\, V(D)
\end{aligned}
\]</span></p>
<p>Therefore, the error of the above Monte Carlo procedure scales with <span class="math inline">\(D\)</span> as follows:</p>
<p><span class="math display">\[
\sigma(\hat f) = \sqrt{\frac{\bigl(2^D - V(D)\bigr)\, V(D)}{S}}
\]</span></p>
<p>We <a href="https://en.wikipedia.org/wiki/Volume_of_an_n-ball">have</a>:</p>
<p><span class="math display">\[
V(D) = \frac{\pi^{D/2}}{\Gamma(D/2+1)} \approx \frac{1}{\sqrt{D\pi}} \left(\frac{2\pi e}{D}\right)^{D/2}
\]</span></p>
<p>Therefore, overall:</p>
<p><span class="math display">\[
\sigma(\hat f) \approx (8e\pi/D)^{D/4} \bigl/\bigr. (\pi D)^{1/4} \sqrt{S}
\]</span></p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> gammaln, lambertw</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> estimate_volume(D, S):</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co">    D : dimension of embedding space</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co">    S : number of sampling points</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    volcube <span class="op">=</span> <span class="dv">2</span><span class="op">**</span>D</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    points <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>, size<span class="op">=</span>(S, D))</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    distance <span class="op">=</span> np.linalg.norm(points, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(distance <span class="op">&lt;=</span> <span class="fl">1.</span>) <span class="op">*</span> volcube</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> volball(D):</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="co">    Volume of a unit ball in D dimensions</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="co">    See: https://en.wikipedia.org/wiki/Volume_of_an_n-ball</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    logvol <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> D <span class="op">*</span> np.log(np.pi) <span class="op">-</span> gammaln(D<span class="op">/</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(logvol)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>n_trials <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>dims <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="dv">30</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>vols <span class="op">=</span> np.array([[estimate_volume(D, S) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_trials)] <span class="cf">for</span> D <span class="kw">in</span> dims])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># show results</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>dims <span class="op">=</span> np.arange(<span class="bu">len</span>(vols))<span class="op">+</span><span class="dv">1</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>kw <span class="op">=</span> <span class="bu">dict</span>(xlabel<span class="op">=</span><span class="vs">r'dimension $D$'</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>), subplot_kw<span class="op">=</span>kw)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].fill_between(dims, vols.mean(<span class="dv">1</span>) <span class="op">+</span> vols.std(<span class="dv">1</span>), </span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>                   vols.mean(<span class="dv">1</span>) <span class="op">-</span> vols.std(<span class="dv">1</span>), color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(dims, vols.mean(<span class="dv">1</span>), lw<span class="op">=</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'k'</span>, label<span class="op">=</span><span class="st">'estimated volume'</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(dims, volball(dims), lw<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'r'</span>, ls<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'true volume'</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">'volume of unit ball'</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].legend()</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylim(<span class="op">-</span><span class="fl">1.</span>, <span class="fl">6.</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Monte Carlo error</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>dims <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>err <span class="op">=</span> np.sqrt((<span class="fl">2.</span><span class="op">**</span>dims <span class="op">-</span> volball(dims)) <span class="op">*</span> volball(dims))</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>approx_err <span class="op">=</span> (np.pi<span class="op">*</span>dims)<span class="op">**</span>(<span class="op">-</span><span class="dv">1</span><span class="op">/</span><span class="dv">4</span>) <span class="op">*</span> (<span class="dv">8</span><span class="op">*</span>np.pi<span class="op">*</span>np.e<span class="op">/</span>dims)<span class="op">**</span>(dims<span class="op">/</span><span class="dv">4</span>)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>dim_crit <span class="op">=</span> <span class="op">-</span><span class="dv">1</span><span class="op">/</span>lambertw(<span class="op">-</span><span class="dv">1</span><span class="op">/</span>(<span class="dv">8</span><span class="op">*</span>np.pi)).real</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(dims, err, lw<span class="op">=</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(dims, approx_err, lw<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'r'</span>, ls<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'approx. error'</span>)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].axvline(dim_crit, lw<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'b'</span>, ls<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'largest MC error</span><span class="ch">\n</span><span class="st">'</span> <span class="op">+</span> <span class="vs">r'at $D=24$'</span>)</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="st">'MC error'</span>)</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].legend()</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(volball(np.arange(<span class="dv">1</span>, <span class="dv">10</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[2.         3.14159265 4.1887902  4.9348022  5.26378901 5.16771278
 4.72476597 4.05871213 3.2985089 ]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-13-output-2.png" width="757" height="373"></p>
</div>
</div>
<p>The Monte Carlo error depends on the dimension in a non-trivial fashion. The dimension with the largest scaling factor <span class="math inline">\(\sigma(f)\)</span> is approximately</p>
<p><span class="math display">\[
D_{\text{max. error}} \approx -1 / W(-1/(8\pi)) \approx 24
\]</span></p>
<p>where <span class="math inline">\(W\)</span> is the <a href="https://en.wikipedia.org/wiki/Lambert_W_function">Lambert W function</a>. Another problem is that the chance of hitting a point in the unit ball by sampling from the hypercube <span class="math inline">\([-1, 1]^D\)</span> dwindles dramatically as <span class="math inline">\(D\)</span> decreases, since</p>
<p><span class="math display">\[
\text{acceptance rate} = \frac{\text{ volume ball}}{\text{ volume cube}} = \frac{V(D)}{2^D} \to 0
\]</span></p>
<p>That is, although the Monte Carlo error decays beyond <span class="math inline">\(D &gt; 24\)</span>, the generation of a point inside the ball becomes extremely rare.</p>
</section>
<section id="pros-and-cons-of-monte-carlo" class="level2">
<h2 class="anchored" data-anchor-id="pros-and-cons-of-monte-carlo">Pros and Cons of Monte Carlo</h2>
<p><strong>Pros</strong> of the Monte Carlo method:</p>
<ul>
<li><p>Monte Carlo methods are widely applicable. For instance, <span class="math inline">\(f\)</span> and <span class="math inline">\(p\)</span> need not be continuous, differentiable etc.</p></li>
<li><p>Monte Carlo is often easy to implement.</p></li>
<li><p>Monte Carlo <em>can</em> work well in multiple dimensions, where grid-based methods can be inefficient/inapplicable. This is supported by the “<span class="math inline">\(\mathcal O(1/\sqrt{S})\)</span> rate of convergence” which is independent of the dimension.</p></li>
</ul>
<p><strong>Cons</strong>:</p>
<ul>
<li><p>Even though the Monte Carlo rate is usually <span class="math inline">\(\mathcal O(1/\sqrt{S})\)</span>, the constants involved may grow exponentially in dimension.</p></li>
<li><p>Deterministic methods may have better rate of convergence than the Monte Carlo rate <span class="math inline">\(1/\sqrt{S}\)</span> (but may also deteriorate faster when dimension increases).</p></li>
<li><p>Monte Carlo estimate is always random, so we never have a guaranteed tolerance, but only statistical evidence (consistent confidence intervals at best).</p></li>
</ul>
</section>
<section id="pseudo-random-number-generators" class="level2">
<h2 class="anchored" data-anchor-id="pseudo-random-number-generators">Pseudo-random Number Generators</h2>
<p>Monte Carlo Estimation depends on the availability of uniform random numbers (we needed these in order to generate points in the hypercube). One possibility to generate random numbers is to do random experiments such as rolling a die. Swiss Astronomer <a href="https://en.wikipedia.org/wiki/Rudolf_Wolf">Rudolf Wolf</a> rolled a pair of dice 20000 times (see e.g.&nbsp;<a href="https://www.lesswrong.com/posts/zd89utY4afA59p58k/wolf-s-dice">Wolf dice data</a>). He also performed <a href="https://en.wikipedia.org/wiki/Buffon%27s_needle_problem">Buffon’s needle</a> experiment to verify the value of <span class="math inline">\(\pi\)</span>. Francis Galton designed a device, the quincunx or <a href="https://en.wikipedia.org/wiki/Bean_machine">Galton board</a>, for generating randomly distributed balls or beans that follow a Gaussian distribution (in fact Binomial distribution). Here is a nice <a href="https://twitter.com/CentrlPotential/status/1332124614391173123">animation</a> and an interactive <a href="https://www.mathsisfun.com/data/quincunx.html">online tool</a>. The RAND cooperation used an electrical roulette wheel to generate 1 million random numbers that were published as a <a href="https://en.wikipedia.org/wiki/A_Million_Random_Digits_with_100,000_Normal_Deviates">book</a>. More hardware implementations for generating random numbers have been developed (see <a href="https://en.wikipedia.org/wiki/Hardware_random_number_generator">wikipedia</a> for more information). For example, <a href="https://en.wikipedia.org/wiki/RDRAND">RDRAND</a> extracts random numbers from an Intel on-chip hardware random number generator.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># simulation of Galton board</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GaltonBoard:</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># directions into which ball can jump</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    left, right <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span>, <span class="op">+</span><span class="fl">0.5</span>    </span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_beans<span class="op">=</span><span class="fl">1e4</span>, n_pegs<span class="op">=</span><span class="dv">20</span>):</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co">        n_beans : int or float</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co">          number of beans that will run through board</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co">        n_pegs : int &gt; 0</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="co">          number of pegs from top to bottom</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_beans <span class="op">=</span> <span class="bu">int</span>(n_beans)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_pegs <span class="op">=</span> <span class="bu">int</span>(n_pegs)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> simulate_jumps(<span class="va">self</span>):</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Simulate all jumps as a random walk. """</span></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.random.choice([GaltonBoard.left, GaltonBoard.right], </span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>                                size<span class="op">=</span>(<span class="va">self</span>.n_beans, <span class="va">self</span>.n_pegs))</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample_positions(<span class="va">self</span>):</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Simulate jumps and return final position of beans by adding up</span></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a><span class="co">        all steps for left/right. </span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.simulate_jumps().<span class="bu">sum</span>(<span class="dv">1</span>)</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>board <span class="op">=</span> GaltonBoard(n_beans<span class="op">=</span><span class="fl">1e3</span>, n_pegs<span class="op">=</span><span class="dv">99</span>)</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a><span class="co"># accumulate</span></span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>x, counts <span class="op">=</span> np.unique(board.sample_positions(), return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a><span class="co"># convert counts to probability</span></span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> counts.astype(<span class="st">'d'</span>) <span class="op">/</span> counts.<span class="bu">sum</span>()</span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a><span class="co"># fit Gaussian</span></span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> np.dot(p, x)</span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> np.dot(p, (x<span class="op">-</span>mu)<span class="op">**</span><span class="dv">2</span>)<span class="op">**</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>)</span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>, <span class="dv">1000</span>) <span class="op">*</span> <span class="dv">5</span> <span class="op">*</span> sigma <span class="op">+</span> mu</span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> (t<span class="op">-</span>mu)<span class="op">**</span><span class="dv">2</span><span class="op">/</span>sigma<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> np.log(<span class="dv">2</span><span class="op">*</span>np.pi<span class="op">*</span>sigma<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>g <span class="op">*=</span> np.diff(x).<span class="bu">min</span>()</span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a><span class="co"># compare results graphically</span></span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>ax.bar(x, p, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>ax.plot(t, g, lw<span class="op">=</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-14-output-1.png" width="662" height="470"></p>
</div>
</div>
<section id="linear-congruential-generator" class="level3">
<h3 class="anchored" data-anchor-id="linear-congruential-generator">Linear congruential generator</h3>
<p><a href="https://en.wikipedia.org/wiki/Pseudorandom_number_generator">Pseudo random number generators (PRNGs)</a> are typically used to produce uniformly distributed pseudo random numbers. One of the standard PRGNs is the <a href="https://en.wikipedia.org/wiki/Linear_congruential_generator"><strong>linear congruential generator</strong> (LCG)</a> introduced by <a href="https://en.wikipedia.org/wiki/D._H._Lehmer">D. H. Lehmer</a>. LCG uses a recurrence relation to generate a new random number <span class="math inline">\(x^{(s+1)}\)</span> from a current one <span class="math inline">\(x^{(s)}\)</span>:</p>
<p><span class="math display">\[
x^{(s+1)} = (a x^{(s)} + c)\, \text{mod}\, m
\]</span></p>
<p>with</p>
<ul>
<li><p><strong>modulus</strong> <span class="math inline">\(m &gt; 0\)</span></p></li>
<li><p><strong>multiplier</strong> <span class="math inline">\(a\)</span> where <span class="math inline">\(0 &lt; a &lt; m\)</span></p></li>
<li><p><strong>increment</strong> <span class="math inline">\(c\)</span> where <span class="math inline">\(0 \le c &lt; m\)</span></p></li>
<li><p><strong>seed</strong> <span class="math inline">\(x^{(0)}\)</span> where <span class="math inline">\(0 \le x^{(0)} &lt; m\)</span></p></li>
</ul>
<p>This is an iterative linear mapping combined with the modulo operation resulting in a discontinuity as soon as the next number escapes from the interval <span class="math inline">\([0,m-1]\)</span>. The initial value <span class="math inline">\(x^{(0)}\)</span> is called the <strong>random seed</strong> or just <strong>seed</strong>. LCGs produce <em>periodic</em> random numbers: as soon as a number is visited twice it will produce the exact same sequence of random numbers. This will happen after at most <span class="math inline">\(m\)</span> iterations. Therefore, the period of the sequence is <span class="math inline">\(m\)</span> or smaller, and we have to choose large <span class="math inline">\(m\)</span> in order to not exhaust our random numbers too quickly. Moreover, we should also choose the increment <span class="math inline">\(c\)</span> and multiplier <span class="math inline">\(a\)</span> such that they are smaller than <span class="math inline">\(m\)</span>. By construction, <span class="math inline">\(x^{(s)}\in[m]\)</span> and <span class="math inline">\(u^{(s)} = x^{(s)}/m\)</span> are uniformly distributed random numbers in <span class="math inline">\([0,1)\)</span>. Among the most widely used PRNGs is the <a href="https://en.wikipedia.org/wiki/Mersenne_Twister">Mersenne Twister</a>.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="co">Pseudo random number generator</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PRNG:</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""PRNG</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Pseudo-random number generator implemented as iterator. Using a linear </span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co">    congruential generator (LCG) to generate random numbers. Default settings</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co">    for modulus, multiplier and period are taken from Numerical Recipes. </span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Example</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="co">    -------</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; prng = PRNG(maximum=1e4)</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="co">    x = list(prng)</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; len(x)</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="co">    10000</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a><span class="co">    Details:</span></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a><span class="co">    * https://en.wikipedia.org/wiki/Linear_congruential_generator</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, m<span class="op">=</span><span class="dv">2</span><span class="op">**</span><span class="dv">32</span>, a<span class="op">=</span><span class="dv">1664525</span>, c<span class="op">=</span><span class="dv">1013904223</span>, seed<span class="op">=</span><span class="dv">10</span>, maximum<span class="op">=</span><span class="fl">1e6</span>):</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters</span></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a><span class="co">        ----------</span></span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a><span class="co">        m : int &gt; 0</span></span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a><span class="co">            modulus or period</span></span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a><span class="co">        a : int &gt; 0</span></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a><span class="co">            multiplier (should be smaller than modulus)</span></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a><span class="co">        c : int &gt;= 0</span></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a><span class="co">            increment (should be smaller than modulus)</span></span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a><span class="co">        seed : int &gt;= 0</span></span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a><span class="co">            initial state (should be smaller than modulus)</span></span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a><span class="co">        maximum : float or int</span></span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a><span class="co">            maximum number of random numbers to be generated by PRNG</span></span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> check_int(i, lower<span class="op">=</span><span class="dv">0</span>, upper<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>            valid <span class="op">=</span> <span class="bu">type</span>(i) <span class="kw">is</span> <span class="bu">int</span> <span class="kw">and</span> i <span class="op">&gt;=</span> lower</span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> upper <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>                valid <span class="op">&amp;=</span> i <span class="op">&lt;</span> upper</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> valid</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a>        msg <span class="op">=</span> <span class="st">'"</span><span class="sc">{0}</span><span class="st">" must be int &gt;= </span><span class="sc">{1}</span><span class="st">'</span></span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> check_int(m, <span class="dv">1</span>), msg.<span class="bu">format</span>(<span class="st">'m'</span>, <span class="dv">1</span>)</span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> check_int(a, <span class="dv">1</span>, m), msg.<span class="bu">format</span>(<span class="st">'a'</span>, <span class="dv">1</span>)</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> check_int(c, <span class="dv">0</span>, m), msg.<span class="bu">format</span>(<span class="st">'c'</span>, <span class="dv">0</span>)</span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> check_int(seed, <span class="dv">0</span>, m), msg.<span class="bu">format</span>(<span class="st">'seed'</span>, <span class="dv">0</span>)</span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.a, <span class="va">self</span>.c, <span class="va">self</span>.m, <span class="va">self</span>.seed <span class="op">=</span> a, c, m, seed</span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._max <span class="op">=</span> <span class="bu">int</span>(maximum)</span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._reset()</span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _reset(<span class="va">self</span>):        </span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.x <span class="op">=</span> <span class="va">self</span>.seed</span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._counter <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__next__</span>(<span class="va">self</span>):</span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a><span class="co">        Using recurrence relation </span></span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a><span class="co">            X_{n+1} = (a X_n + c) mod m</span></span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true" tabindex="-1"></a><span class="co">        to generate new random number</span></span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>._counter <span class="op">&gt;=</span> <span class="va">self</span>._max:</span>
<span id="cb18-70"><a href="#cb18-70" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">StopIteration</span></span>
<span id="cb18-71"><a href="#cb18-71" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-72"><a href="#cb18-72" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.x <span class="op">=</span> (<span class="va">self</span>.a <span class="op">*</span> <span class="va">self</span>.x <span class="op">+</span> <span class="va">self</span>.c) <span class="op">%</span> <span class="va">self</span>.m</span>
<span id="cb18-73"><a href="#cb18-73" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._counter <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb18-74"><a href="#cb18-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-75"><a href="#cb18-75" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.x</span>
<span id="cb18-76"><a href="#cb18-76" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-77"><a href="#cb18-77" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__iter__</span>(<span class="va">self</span>):</span>
<span id="cb18-78"><a href="#cb18-78" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._reset()</span>
<span id="cb18-79"><a href="#cb18-79" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb18-80"><a href="#cb18-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-81"><a href="#cb18-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-82"><a href="#cb18-82" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Uniform(PRNG):</span>
<span id="cb18-83"><a href="#cb18-83" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Pseudo-random numbers between 0 and 1. """</span></span>
<span id="cb18-84"><a href="#cb18-84" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__next__</span>(<span class="va">self</span>):</span>
<span id="cb18-85"><a href="#cb18-85" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">super</span>().<span class="fu">__next__</span>() <span class="op">/</span> <span class="bu">float</span>(<span class="va">self</span>.m)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The quality of a LCG depends on the choice of the four “magic numbers” <span class="math inline">\(a, c, m\)</span> and <span class="math inline">\(x^{(0)}\)</span>. Let us look at what happens for small periods <span class="math inline">\(m\)</span> (you can also do this with a nice <a href="https://demonstrations.wolfram.com/LinearCongruentialGenerators/">online app</a> and another <a href="https://demonstrations.wolfram.com/LinearCongruentialSequences/">app</a>). Let’s do it with our Python code:</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>prng <span class="op">=</span> PRNG(m<span class="op">=</span><span class="dv">181</span>, a<span class="op">=</span><span class="dv">40</span>, c<span class="op">=</span><span class="dv">0</span>, seed<span class="op">=</span><span class="dv">1</span>, maximum<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array(<span class="bu">list</span>(prng))</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.fft.fft(x)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>kw <span class="op">=</span> <span class="bu">dict</span>(s<span class="op">=</span><span class="dv">40</span>, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">3</span>))</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].scatter(np.arange(<span class="dv">100</span>), x[:<span class="dv">100</span>], <span class="op">**</span>kw)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(x[:<span class="dv">100</span>], color<span class="op">=</span><span class="st">'k'</span>, lw<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="vs">r'iteration $s$'</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="vs">r'pseudo random number $x^{(s)}$'</span>)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].scatter(x[:<span class="dv">100</span>], x[<span class="dv">1</span>:<span class="dv">101</span>], <span class="op">**</span>kw)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="vs">r'$x^{(s)}$'</span>)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="vs">r'$x^{(s+1)}$'</span>)</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].plot(np.<span class="bu">abs</span>(np.fft.fftshift(X))[<span class="dv">1</span>:<span class="bu">len</span>(x)<span class="op">//</span><span class="dv">2</span>], </span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>                  lw<span class="op">=</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_xlabel(<span class="st">'spatial frequency'</span>)</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_ylabel(<span class="vs">r'spectrum $|FT|$'</span>)</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-16-output-1.png" width="1140" height="278"></p>
</div>
</div>
<p>As shown in this simple example, LCGs can suffer from serious deficits and biases. A famous example is IBM’s <a href="https://en.wikipedia.org/wiki/RANDU">RANDU</a> algorithm, according to Donald Knuth a “truly horrible” algorithm. A whole array of <a href="https://en.wikipedia.org/wiki/Randomness_tests"><em>randomness tests</em></a> has been developed ever since. For example, in the <a href="https://en.wikipedia.org/wiki/Spectral_test">spectral test</a> successive random numbers are plotted against each other, thereby revealing nonrandom structures in the pseudo random number sequence.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># LCG from Numerical Recipes</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="fl">1e5</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>prng <span class="op">=</span> PRNG(maximum<span class="op">=</span>n_samples)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array(<span class="bu">list</span>(prng))   </span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>u <span class="op">=</span> x <span class="op">/</span> <span class="bu">float</span>(prng.m)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>kw <span class="op">=</span> <span class="bu">dict</span>(s<span class="op">=</span><span class="dv">5</span>, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].hist(u, bins<span class="op">=</span><span class="dv">100</span>, density<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].scatter(x[:<span class="dv">5000</span>], x[<span class="dv">1</span>:<span class="dv">5001</span>], <span class="op">**</span>kw)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="vs">r'$x^{(s)}$'</span>)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="vs">r'$x^{(s+1)}$'</span>)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-17-output-1.png" width="758" height="374"></p>
</div>
</div>
</section>
</section>
<section id="sampling-a-discrete-model" class="level2">
<h2 class="anchored" data-anchor-id="sampling-a-discrete-model">Sampling a discrete model</h2>
<p>Assuming that we have a good source for pseudo random numbers, let us first look at how to use these for sampling a <em>discrete model</em>. The sample space is finite or countably infinite: <span class="math inline">\(\mathcal X = \{x_1, \ldots, x_N\}\)</span>, where <span class="math inline">\(N=\infty\)</span> is also possible. Without loss of generality we assume <span class="math inline">\(p_i&gt;0\)</span> (zero-probability states are excluded from the sample space).</p>
<p>How can we use uniform random numbers from <span class="math inline">\([0, 1]\)</span> to generate samples from <span class="math inline">\(p\)</span>? For given <span class="math inline">\(u\sim \mathcal U(0,1)\)</span> pick state <span class="math inline">\(i\in[N]\)</span> such that</p>
<p><span id="eq-discrete_sampling"><span class="math display">\[
    i = \min\bigl\{j \in\mathbb N\, :\, \sum_{k=1}^j p_k \ge u  \bigr\}
\tag{15}\]</span></span></p>
<p>Here and in the following <span class="math inline">\(\mathcal U(0,1)\)</span> denotes the uniform distribution over the unit interval, i.e.&nbsp;<span class="math inline">\(x\sim U(0,1)\)</span> has density <span class="math inline">\(p(x) = \mathbb 1(0 &lt; x &lt; 1)\)</span>.</p>
<p>Why is this procedure correct? Let us call <span class="math inline">\(c_i = \sum_{k=1}^i p_k\)</span>, then <span class="math inline">\(c_1 = p_1 &gt; 0\)</span> and <span class="math inline">\(c_N = 1\)</span>. Moreover, define <span class="math inline">\(c_0:= 0\)</span>. All <span class="math inline">\(c_i\in(0,1]\)</span> (except <span class="math inline">\(c_0\)</span>) and sorted <span class="math inline">\(c_{i-1} &lt; c_i\)</span> (since all <span class="math inline">\(p_i&gt;0\)</span>). So the intervals <span class="math inline">\(I_i=[c_{i-1}, c_i)\)</span> form a partition of <span class="math inline">\([0,1)\)</span> and the uniform distribution <span class="math inline">\(\mathbb 1\bigl(x\in[0,1)\bigr)\)</span> can be written as a mixture of uniform distributions:</p>
<p><span class="math display">\[
\mathbb 1\bigl(x\in[0,1)\bigr) = \sum_{i=1}^{N} \mathbb 1\bigl(x\in[c_{i-1}, c_{i})\bigr)
\]</span></p>
<p>Criterion (Eq. <a href="#eq-discrete_sampling">Equation&nbsp;15</a>) picks the interval with <span class="math inline">\(u\in[c_{i-1}, c_{i})=:I_i\)</span>. The length of each interval <span class="math inline">\(I_i\)</span> is <span class="math inline">\(c_i - c_{i-1} = p_i\)</span>, and equal to the chance of landing in <span class="math inline">\(I_i\)</span>. Therefor the generated <span class="math inline">\(x_i\)</span> will follow <span class="math inline">\(p\)</span>.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># illustration discrete sampling</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> np.random.random(N)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>p <span class="op">/=</span> p.<span class="bu">sum</span>()</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> np.arange(N<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> np.append(<span class="fl">0.</span>, np.add.accumulate(p))</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>ax.barh(i, c, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r'$i$'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>ax.set_xticks(c)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels([<span class="vs">r'$c_</span><span class="sc">{{{}}}</span><span class="vs">$'</span>.<span class="bu">format</span>(ii) <span class="cf">for</span> ii <span class="kw">in</span> i])</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cc <span class="kw">in</span> c:</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>    ax.axvline(cc, ls<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'r'</span>, lw<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-18-output-1.png" width="951" height="566"></p>
</div>
</div>
<section id="sampling-from-the-poisson-distribution" class="level3">
<h3 class="anchored" data-anchor-id="sampling-from-the-poisson-distribution">Sampling from the Poisson distribution</h3>
<p>The Poisson distribution is a pmf over the sample space <span class="math inline">\(\mathbb N\)</span>, i.e.&nbsp;<span class="math inline">\(x=0, 1, 2, \ldots\)</span> and defined as</p>
<p><span id="eq-poisson"><span class="math display">\[
p(x) = \frac{\lambda^x}{x!} e^{-\lambda}, \,\,\, \lambda &gt; 0
\tag{16}\]</span></span></p>
<p>the parameter <span class="math inline">\(\lambda\)</span> is called <em>rate</em>. The mean and variance of <span class="math inline">\(x\)</span> are</p>
<p><span class="math display">\[
\mathbb E[x] = \lambda, \,\,\, \text{var}[x] = \lambda\,.
\]</span></p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sampling from the Poisson distribution</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="co"># - this is how it's *not* done in practice -</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> gammaln</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_poisson(S, rate<span class="op">=</span><span class="fl">1.</span>, return_cdf<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="co">    S : number of samples</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="co">    rate : rate of Poisson distribution</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># uniform random numbers</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    u <span class="op">=</span> np.random.random(<span class="bu">int</span>(S))</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    i <span class="op">=</span> np.argsort(u)</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># array storing samples</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="op">-</span>np.ones(u.shape, dtype<span class="op">=</span><span class="st">'i'</span>)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># building up cdf</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>    cdf <span class="op">=</span> [<span class="fl">0.</span>]</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># stepping through sorted list of uniform random numbers</span></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> v, j <span class="kw">in</span> <span class="bu">zip</span>(u[i], i):</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> v <span class="op">&gt;</span> cdf[k]:</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>            k <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>            pmf <span class="op">=</span> np.exp(k <span class="op">*</span> np.log(rate) <span class="op">-</span> rate <span class="op">-</span> gammaln(k<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>            cdf.append(cdf[<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> pmf)</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>        x[j] <span class="op">=</span> k</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> return_cdf:</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x, np.array(cdf)</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>rate <span class="op">=</span> <span class="fl">10.</span></span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>limits <span class="op">=</span> <span class="fl">0.</span>, <span class="dv">10</span> <span class="op">*</span> rate<span class="op">**</span><span class="fl">0.5</span></span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>x, cdf <span class="op">=</span> sample_poisson(<span class="fl">1e3</span>, rate)</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> np.arange(<span class="bu">len</span>(cdf))</span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a>pmf <span class="op">=</span> np.exp(k <span class="op">*</span> np.log(rate) <span class="op">-</span> rate <span class="op">-</span> gammaln(k<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a>bins, hist <span class="op">=</span> np.unique(x, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a>hist <span class="op">=</span> hist.astype(<span class="st">'d'</span>) <span class="op">/</span> hist.<span class="bu">sum</span>()</span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a>settings <span class="op">=</span> <span class="bu">dict</span>(xlim<span class="op">=</span>limits, xlabel<span class="op">=</span><span class="vs">r'bin $i$'</span>)</span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>), subplot_kw<span class="op">=</span>settings)</span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(k, pmf, color<span class="op">=</span><span class="st">'r'</span>, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb22-47"><a href="#cb22-47" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].bar(k, cdf, color<span class="op">=</span><span class="st">'lightgrey'</span>)</span>
<span id="cb22-48"><a href="#cb22-48" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="vs">r'cdf $\sum_{k=1}^i\, p_k$'</span>)</span>
<span id="cb22-49"><a href="#cb22-49" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].bar(bins, hist, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb22-50"><a href="#cb22-50" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(k, pmf, color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb22-51"><a href="#cb22-51" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-19-output-1.png" width="758" height="373"></p>
</div>
</div>
</section>
</section>
<section id="variable-transformation-methods" class="level2">
<h2 class="anchored" data-anchor-id="variable-transformation-methods">Variable transformation methods</h2>
<p>We will now move on to continuous sample spaces. Assuming that we can generate uniform random samples from <span class="math inline">\(\mathcal U(0, 1)\)</span>, how can we use these samples to generate samples from a non-uniform distribution <span class="math inline">\(p(x)\)</span>? In the last section we saw how to do this for pmfs (although it might not be practical for large finite models such as Ising models).</p>
<p>Let’s first look at the simplest version of sampling from a pdf where the sample space is one-dimensional. So we are looking for ways of how to transform a single uniformly distributed variable <span class="math inline">\(u \sim \mathcal U(0, 1)\)</span> to <span class="math inline">\(x \sim p(x)\)</span>. To design such as method, we first need to understand how probability distributions transform under parameter transformations.</p>
<section id="transformation-of-probability-distributions" class="level3">
<h3 class="anchored" data-anchor-id="transformation-of-probability-distributions">Transformation of probability distributions</h3>
<p>Let <span class="math inline">\(h\)</span> be a <em>one-to-one mapping</em> between two one-dimensional sample spaces <span class="math inline">\(h: \mathcal X \to \mathcal Y\)</span>, and <span class="math inline">\(h^{-1}\)</span> is the inverse function. If <span class="math inline">\(x\sim p_x(x)\)</span>, what is the distribution <span class="math inline">\(p_y(y)\)</span> of <span class="math inline">\(y=h(x)\)</span>? To answer this question let us compute the distribution <span class="math inline">\(p_y\)</span>:</p>
<p><span id="eq-transform1d"><span class="math display">\[
p_y(y) = \int_{\mathcal X} \delta(y - h(x))\, p_x(x)\, dx = \int_{\mathcal X} \frac{1}{|h'(x)|} \delta(x - h^{-1}(y))\, p_x(x)\, dx = \frac{p_x(h^{-1}(y))}{|h'(h^{-1}(y))|}
\tag{17}\]</span></span></p>
<p>where we used the transformation property of the <a href="https://en.wikipedia.org/wiki/Dirac_delta_function#Composition_with_a_function">delta distribution</a>. The transformation rule guarantees that normalized pdfs transform into normalized pdfs.</p>
<p>This result can be generalized to multiple dimensions. Let <span class="math inline">\(h\)</span> be an invertible one-to-one mapping between two <span class="math inline">\(D\)</span> dimensional sample spaces. Assume further that <span class="math inline">\(h\)</span> is continuously differentiable such that the Jacobian</p>
<p><span class="math display">\[
\nabla h(x) = \left(\frac{\partial h_i(x)}{\partial x_j} \right)_{i,j}
\]</span></p>
<p>is everywhere invertible in <span class="math inline">\(\mathcal X\)</span>, i.e.&nbsp;<span class="math inline">\(\text{det}(\nabla h(x)) \not= 0\)</span> for all <span class="math inline">\(x\in \mathcal X\)</span>. Then <span class="math inline">\(y=h(x)\)</span> has density</p>
<p><span class="math display">\[
p_y(y) = \left\{ \begin{array}{c c}
p_x(h^{-1}(y)) \, |\text{det}(\nabla h^{-1})(y)|, &amp; y \in h(\mathcal X) \\
0, &amp; y \notin h(\mathcal X)
\end{array}\right.
\]</span></p>
</section>
<section id="inversion-method" class="level3">
<h3 class="anchored" data-anchor-id="inversion-method">Inversion method</h3>
<p>The <a href="https://en.wikipedia.org/wiki/Inverse_transform_sampling"><em>inversion method</em></a> is a simple variable transformation method. Let <span class="math inline">\(p(x)\)</span> be a pdf over a sample space <span class="math inline">\(\mathcal X \subset \mathbb R\)</span>, then the <a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function"><em>cumulative distribution function</em> (cdf)</a> is:</p>
<p><span id="eq-cdf"><span class="math display">\[
P(y) = \Pr(x \le y) = \mathbb E_p[x \le y] = \int^y_{-\infty} p(x)\, dx
\tag{18}\]</span></span></p>
<p>this is the continuous analog of <span class="math inline">\(c_i\)</span> defined above in the section on sampling discrete models. By construction, <span class="math inline">\(P(x) \in [0, 1]\)</span> for <span class="math inline">\(x \in\mathcal X\)</span>. <span class="math inline">\(P(x)\)</span> is continuous and strictly increasing and therefore invertible. Inverse transform sampling uses the following mathematical fact:</p>
<p><span id="eq-inversion_method"><span class="math display">\[
x = P^{-1}(u) \sim p(x)\,\,\,\text{for}\,\,\, u\sim\mathcal U(0,1)
\tag{19}\]</span></span></p>
<p>That is, we can generate random samples from <span class="math inline">\(p(x)\)</span> by generating uniformly distributed random numbers in <span class="math inline">\([0, 1]\)</span> and map them to <span class="math inline">\(\mathcal X\)</span> with the inverse of the cdf <span class="math inline">\(P^{-1}\)</span>.</p>
<p>To see that this is a valid sampling procedure, let us compute the distribution of <span class="math inline">\(x=P^{-1}(u)\)</span> using the transformation rule (Eq. <a href="#eq-transform1d">Equation&nbsp;17</a>): We have</p>
<p><span class="math display">\[
\frac{d\, P^{-1}(u)}{d\, u} = \frac{1}{P'(P^{-1}(u))} = \frac{1}{p(P^{-1}(u))}
\]</span></p>
<p>Plugging this expression into (Eq. <a href="#eq-transform1d">Equation&nbsp;17</a>) yields</p>
<p><span class="math display">\[
p_x(x) = \frac{p_u(P(x))}{(P^{-1})'(P(x))} = \frac{1}{1 / p(x)} = p(x)
\]</span></p>
<p><em>Example</em>: Let us apply inverse transformation sampling to the exponential distribution</p>
<p><span class="math display">\[
p(x) = \lambda \, \exp\{-\lambda x\}, \,\,\, \lambda &gt; 0, \, \mathcal X = \mathbb R_+
\]</span></p>
<p>The cumulative distribution function is</p>
<p><span class="math display">\[
\text{cdf}(x) = \int^x_0 \lambda e^{-\lambda t} \, dt = 1 - e^{-\lambda x}
\]</span></p>
<p>and its inverse:</p>
<p><span class="math display">\[
\text{cdf}^{-1}(u) = - \frac{1}{\lambda} \log(1-u)
\]</span></p>
<p>Since <span class="math inline">\(1-u \sim \mathcal U(0,1)\)</span> if <span class="math inline">\(u \sim \mathcal U(0,1)\)</span>, we can generate exponentially distributed random variables as follows:</p>
<p><span class="math display">\[
x^{(s)} = - \frac{\log u^{(s)}}{\lambda}, \,\,\, u^{(s)} \sim \mathcal U(0,1)
\]</span></p>
<p>Some more examples:</p>
<table class="table">
<thead>
<tr class="header">
<th>name</th>
<th>pdf <span class="math inline">\(p(x)\)</span></th>
<th>cdf <span class="math inline">\(P(x)\)</span></th>
<th>inversion</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://en.wikipedia.org/wiki/Exponential_distribution">Exponential</a></td>
<td><span class="math inline">\(\lambda e^{-\lambda x}\)</span></td>
<td><span class="math inline">\(1 - e^{-\lambda x}\)</span></td>
<td><span class="math inline">\(-\log(u)/\lambda\)</span></td>
</tr>
<tr class="even">
<td><a href="https://en.wikipedia.org/wiki/Cauchy_distribution">Cauchy</a></td>
<td><span class="math inline">\(\frac{\sigma}{\pi(x^2 + \sigma^2)}\)</span></td>
<td><span class="math inline">\(\frac{1}{2} + \frac{1}{\pi} \arctan(x/\sigma)\)</span></td>
<td><span class="math inline">\(-\sigma\tan(\pi(u-0.5))\)</span></td>
</tr>
<tr class="odd">
<td><a href="https://en.wikipedia.org/wiki/Rayleigh_distribution">Rayleigh</a></td>
<td><span class="math inline">\(\frac{x}{\sigma^2} e^{-x^2/2\sigma^2}\)</span></td>
<td><span class="math inline">\(1- e^{-x^2/2\sigma^2}\)</span></td>
<td><span class="math inline">\(\sigma\,\sqrt{-2\log u}\)</span></td>
</tr>
<tr class="even">
<td>Triangular</td>
<td><span class="math inline">\(2 (1 - x/a)/a, \, x\in[0,a]\)</span></td>
<td><span class="math inline">\(2 (x - x^2/2a)\)</span></td>
<td><span class="math inline">\(a(1-\sqrt{u})\)</span></td>
</tr>
<tr class="odd">
<td><a href="https://en.wikipedia.org/wiki/Pareto_distribution">Pareto</a></td>
<td><span class="math inline">\(a\,b^a / x^{a+1}, \, x\ge b\)</span></td>
<td><span class="math inline">\(1-(b/x)^a\)</span></td>
<td><span class="math inline">\(b\, u^{-1/a}\)</span></td>
</tr>
</tbody>
</table>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># some examples</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PDF:</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">**</span>params):</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, value <span class="kw">in</span> params.items():</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>            <span class="bu">setattr</span>(<span class="va">self</span>, name, <span class="bu">float</span>(value))</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, n<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>        u <span class="op">=</span> np.random.random(<span class="bu">int</span>(n))</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._invert(u)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>):</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _invert(<span class="va">self</span>, u):</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Exponential(PDF):</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, _lambda<span class="op">=</span><span class="fl">1.</span>):</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(_lambda<span class="op">=</span>_lambda)</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._lambda <span class="op">*</span> np.exp(<span class="op">-</span><span class="va">self</span>._lambda<span class="op">*</span>x)</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _invert(<span class="va">self</span>, u):</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">-</span>np.log(u)<span class="op">/</span><span class="va">self</span>._lambda</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> support(<span class="va">self</span>):</span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">0.</span>, <span class="dv">5</span> <span class="op">/</span> <span class="va">self</span>._lambda</span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Cauchy(PDF):</span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, sigma<span class="op">=</span><span class="fl">1.</span>):</span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(sigma<span class="op">=</span>sigma)</span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.sigma <span class="op">/</span> np.pi <span class="op">/</span> (x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="va">self</span>.sigma<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _invert(<span class="va">self</span>, u):</span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.sigma <span class="op">*</span> np.tan(np.pi<span class="op">*</span>(u<span class="op">-</span><span class="fl">0.5</span>))</span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> support(<span class="va">self</span>):</span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">-</span><span class="fl">10.</span>, <span class="op">+</span><span class="fl">10.</span></span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-51"><a href="#cb23-51" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-52"><a href="#cb23-52" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Rayleigh(PDF):</span>
<span id="cb23-53"><a href="#cb23-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-54"><a href="#cb23-54" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, sigma<span class="op">=</span><span class="fl">1.</span>):</span>
<span id="cb23-55"><a href="#cb23-55" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(sigma<span class="op">=</span>sigma)</span>
<span id="cb23-56"><a href="#cb23-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-57"><a href="#cb23-57" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb23-58"><a href="#cb23-58" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> (x<span class="op">/</span><span class="va">self</span>.sigma)</span>
<span id="cb23-59"><a href="#cb23-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> t <span class="op">*</span> np.exp(<span class="op">-</span><span class="fl">0.5</span><span class="op">*</span>t<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> <span class="va">self</span>.sigma</span>
<span id="cb23-60"><a href="#cb23-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-61"><a href="#cb23-61" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _invert(<span class="va">self</span>, u):</span>
<span id="cb23-62"><a href="#cb23-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.sigma <span class="op">*</span> np.sqrt(<span class="op">-</span><span class="dv">2</span><span class="op">*</span>np.log(u))</span>
<span id="cb23-63"><a href="#cb23-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-64"><a href="#cb23-64" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb23-65"><a href="#cb23-65" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> support(<span class="va">self</span>):</span>
<span id="cb23-66"><a href="#cb23-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">0.</span>, <span class="dv">5</span> <span class="op">*</span> <span class="va">self</span>.sigma</span>
<span id="cb23-67"><a href="#cb23-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-68"><a href="#cb23-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-69"><a href="#cb23-69" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Triangular(PDF):</span>
<span id="cb23-70"><a href="#cb23-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-71"><a href="#cb23-71" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, a<span class="op">=</span><span class="fl">1.</span>):</span>
<span id="cb23-72"><a href="#cb23-72" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(a<span class="op">=</span>a)</span>
<span id="cb23-73"><a href="#cb23-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-74"><a href="#cb23-74" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb23-75"><a href="#cb23-75" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">2</span> <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> x<span class="op">/</span><span class="va">self</span>.a) <span class="op">/</span> <span class="va">self</span>.a</span>
<span id="cb23-76"><a href="#cb23-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-77"><a href="#cb23-77" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _invert(<span class="va">self</span>, u):</span>
<span id="cb23-78"><a href="#cb23-78" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.a <span class="op">*</span> (<span class="dv">1</span><span class="op">-</span>np.sqrt(u))</span>
<span id="cb23-79"><a href="#cb23-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-80"><a href="#cb23-80" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb23-81"><a href="#cb23-81" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> support(<span class="va">self</span>):</span>
<span id="cb23-82"><a href="#cb23-82" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">0.</span>, <span class="va">self</span>.a</span>
<span id="cb23-83"><a href="#cb23-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-84"><a href="#cb23-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-85"><a href="#cb23-85" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Pareto(PDF):</span>
<span id="cb23-86"><a href="#cb23-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-87"><a href="#cb23-87" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, a<span class="op">=</span><span class="fl">1.</span>, b<span class="op">=</span><span class="fl">1.</span>):</span>
<span id="cb23-88"><a href="#cb23-88" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(a<span class="op">=</span>a, b<span class="op">=</span>b)</span>
<span id="cb23-89"><a href="#cb23-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-90"><a href="#cb23-90" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb23-91"><a href="#cb23-91" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.a <span class="op">*</span> <span class="va">self</span>.b<span class="op">**</span><span class="va">self</span>.a <span class="op">/</span> (x<span class="op">**</span>(<span class="va">self</span>.a<span class="op">+</span><span class="dv">1</span>) <span class="op">+</span> <span class="fl">1e-100</span>) <span class="op">*</span> (x<span class="op">&gt;=</span><span class="va">self</span>.b)</span>
<span id="cb23-92"><a href="#cb23-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-93"><a href="#cb23-93" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _invert(<span class="va">self</span>, u):</span>
<span id="cb23-94"><a href="#cb23-94" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.b <span class="op">/</span> u<span class="op">**</span>(<span class="dv">1</span><span class="op">/</span><span class="va">self</span>.a)</span>
<span id="cb23-95"><a href="#cb23-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-96"><a href="#cb23-96" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb23-97"><a href="#cb23-97" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> support(<span class="va">self</span>):</span>
<span id="cb23-98"><a href="#cb23-98" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">0.</span>, <span class="dv">10</span> <span class="op">*</span> <span class="va">self</span>.b</span>
<span id="cb23-99"><a href="#cb23-99" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.b, <span class="dv">10</span> <span class="op">*</span> <span class="va">self</span>.b</span>
<span id="cb23-100"><a href="#cb23-100" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-101"><a href="#cb23-101" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-102"><a href="#cb23-102" aria-hidden="true" tabindex="-1"></a>pdfs <span class="op">=</span> [Exponential(_lambda<span class="op">=</span><span class="fl">0.5</span>), Cauchy(), Rayleigh(sigma<span class="op">=</span><span class="fl">2.</span>),</span>
<span id="cb23-103"><a href="#cb23-103" aria-hidden="true" tabindex="-1"></a>        Triangular(a<span class="op">=</span><span class="fl">2.</span>), Pareto()]</span>
<span id="cb23-104"><a href="#cb23-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-105"><a href="#cb23-105" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> <span class="bu">int</span>(<span class="fl">1e4</span>)</span>
<span id="cb23-106"><a href="#cb23-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-107"><a href="#cb23-107" aria-hidden="true" tabindex="-1"></a>kw <span class="op">=</span> <span class="bu">dict</span>(bins<span class="op">=</span><span class="dv">20</span>, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.1</span>, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-108"><a href="#cb23-108" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="bu">len</span>(pdfs), figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">3</span>))</span>
<span id="cb23-109"><a href="#cb23-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-110"><a href="#cb23-110" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a, pdf <span class="kw">in</span> <span class="bu">zip</span>(ax, pdfs):</span>
<span id="cb23-111"><a href="#cb23-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-112"><a href="#cb23-112" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.linspace(<span class="op">*</span>(pdf.support <span class="op">+</span> (<span class="dv">1000</span>,)))</span>
<span id="cb23-113"><a href="#cb23-113" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> pdf.sample(S)</span>
<span id="cb23-114"><a href="#cb23-114" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> y[(y <span class="op">&gt;</span> x.<span class="bu">min</span>()) <span class="op">&amp;</span> (y <span class="op">&lt;</span> x.<span class="bu">max</span>())]</span>
<span id="cb23-115"><a href="#cb23-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-116"><a href="#cb23-116" aria-hidden="true" tabindex="-1"></a>    a.set_title(pdf.__class__.<span class="va">__name__</span>)</span>
<span id="cb23-117"><a href="#cb23-117" aria-hidden="true" tabindex="-1"></a>    a.plot(x, pdf(x), lw<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb23-118"><a href="#cb23-118" aria-hidden="true" tabindex="-1"></a>    a.hist(y, <span class="op">**</span>kw)</span>
<span id="cb23-119"><a href="#cb23-119" aria-hidden="true" tabindex="-1"></a>    a.set_xlim(<span class="op">*</span>pdf.support)</span>
<span id="cb23-120"><a href="#cb23-120" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-121"><a href="#cb23-121" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-20-output-1.png" width="1148" height="278"></p>
</div>
</div>
<p>In principle, the inverse transformation approach (Eq. <a href="#eq-inversion_method">Equation&nbsp;19</a>) generalizes to multiple dimensions (see, for example, Murray Rosenblatt: <a href="https://www.jstor.org/stable/2236692?seq=1#metadata_info_tab_contents">Remarks on a Multivariate Transformation</a>):</p>
<p><span id="eq-multivariate_transform"><span class="math display">\[
%
\begin{aligned}
    P_1(x_1') &amp;= \Pr(x_1 &lt; x_1') = \int \mathbb 1(x_1 \le x_1')\, p(x_1, \ldots, x_D)\, dx_1 \cdots dx_D \\
    %
    P_2(x_2'\mid{}x_1) &amp;= \Pr(x_2 &lt; x_2' \mid{}x_1) = \int \mathbb 1(x_2 \le x_2')\, p(x_2, \ldots, x_D\mid{}x_1)\, dx_2 \cdots dx_D \\
    &amp;\vdots  \\
    P_D(x_D'\mid{}x_{D-1}, \ldots, x_{1}) &amp;= \Pr(x_D &lt; x_D'\mid{}x_{D-1}, \ldots, x_{1}) = \int \mathbb 1(x_D \le x_D')\, p(x_D\mid{}x_{D-1}, \ldots, x_{1})\, dx_D
\end{aligned}
\tag{20}\]</span></span></p>
<p>However, only in very rare cases is it possible to compute the multivariate cumulative distribution function in higher dimensional spaces, let alone invert it in closed form.</p>
<p>So this expression is mostly of theoretical interest to us. Nevertheless, it is curious to see that multivariate pdfs can in principle be mapped to a uniform distributions over the hypercube:</p>
<p><span class="math display">\[
u_i = P_i(x_i\mid{}x_{i-1}, \ldots x_{1}), \,\,\, i=1, \ldots, D
\]</span></p>
<p>By construction <span class="math inline">\(u=(u_1, \ldots, u_D) \in [0, 1]^D\)</span>, and the pdf of <span class="math inline">\(u\)</span> is</p>
<p><span class="math display">\[
p_u(u) = p_x(x(u)) \prod_i \biggl|\frac{d u_i}{d x_i}\biggr|^{-1} = 1
\]</span></p>
<p>since the Jacobian matrix is triagonal (so its determinant is just a product over the diagonal elements) and</p>
<p><span class="math display">\[
p(x_1, \ldots, x_D) = p(x_D \mid{} x_{D-1}, \ldots, x_{1}) \cdots p(x_2\mid{}x_1) \, p(x_1)
\]</span></p>
</section>
<section id="variable-transformation-method" class="level3">
<h3 class="anchored" data-anchor-id="variable-transformation-method">Variable transformation method</h3>
<p>Sometimes one can find a transformation of the sample space such that the new distribution is easier to sample. A specific example is the <a href="https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform">Box-Muller method</a> for generating samples from a standard Gaussian distribution:</p>
<p><span class="math display">\[
p(x) = \frac{1}{\sqrt{2\pi}} \exp\bigl\{-x^2/2\bigr\}
\]</span></p>
<p>where <span class="math inline">\(x \in \mathbb R\)</span>. The first trick is to make the problem seemingly more complicated by transforming it to a two-dimensional distribution by introducing <span class="math inline">\(y\)</span> which also follows a standard Gaussian distribution. That is,</p>
<p><span class="math display">\[
p(x,y) = \frac{1}{2\pi} \exp\bigl\{-(x^2+y^2)/2 \bigr\}\, .
\]</span></p>
<p>Because <span class="math inline">\(p(x, y)\)</span> depends on <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> only through their distance from the origin <span class="math inline">\(r = \sqrt{x^2 + y^2}\)</span>, it makes sense to transform <span class="math inline">\((x, y)\)</span> to <a href="https://en.wikipedia.org/wiki/Polar_coordinate_system">polar coordinates</a>:</p>
<p><span class="math display">\[
\begin{pmatrix} x \\ y \end{pmatrix} =
\begin{pmatrix} r\cos\varphi \\ r\sin\varphi \end{pmatrix}
\]</span></p>
<p>with the new sample space <span class="math inline">\([0, \infty) \times [0, 2\pi]\)</span>. The Jacobian of the parameter transformation is</p>
<p><span class="math display">\[
\frac{\partial (x, y)}{\partial (r, \varphi)} =
\begin{pmatrix}
    \cos\varphi &amp; -r\sin\varphi \\
    \sin\varphi &amp; r\cos\varphi \\
\end{pmatrix}
\]</span></p>
<p>with determinant <span class="math display">\[
\left|\frac{\partial (x, y)}{\partial (r, \varphi)}\right| = r
\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[
p(r, \varphi) = \frac{r}{2\pi} e^{-r^2/2} = p(r)\, p(\varphi)
\]</span></p>
<p>with <span class="math inline">\(p(\varphi) = \frac{1}{2\pi} \mathbb 1\bigl(\varphi\in[0, 2\pi]\bigr)\)</span>.</p>
<p>The cdf of <span class="math inline">\(p(r)\)</span> is <span class="math display">\[
\int_0^r t e^{-t^2/2}\, dt = e^{-t^2/2} \biggl|_r^0 \biggr. = 1 - e^{-r^2/2}
\]</span></p>
<p>We can obtain a random sample from <span class="math inline">\(p(x, y)\)</span> by first generating two uniform random numbers <span class="math inline">\(u, v \in \mathcal U(0,1)\)</span> and then letting <span class="math inline">\(r = \sqrt{-2\log(1 - u)}\)</span> and <span class="math inline">\(\varphi=2\pi v\)</span> from which we obtain:</p>
<p><span class="math display">\[
x = \sqrt{-2\log(u)}\cos(2\pi v),\,\,\,
y = \sqrt{-2\log(u)}\sin(2\pi v)
\]</span></p>
<p>where we used the fact that <span class="math inline">\(1-u \in\mathcal U(0,1)\)</span> if <span class="math inline">\(u\in \mathcal U(0,1)\)</span>.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Box Muller method</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_gaussian(S):</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Sample standard Gaussian distribution using Box-Muller </span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    u, v <span class="op">=</span> np.random.random(size<span class="op">=</span>(<span class="dv">2</span>, <span class="bu">int</span>(S)))</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> np.sqrt(<span class="op">-</span><span class="dv">2</span><span class="op">*</span>np.log(u))</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    phi <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> v</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> r <span class="op">*</span> np.cos(phi), r <span class="op">*</span> np.sin(phi)</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> <span class="fl">1e4</span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> sample_gaussian(S)</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>, <span class="dv">1000</span>) <span class="op">*</span> <span class="dv">5</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span><span class="op">*</span>t<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi)</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>kw <span class="op">=</span> <span class="bu">dict</span>(bins<span class="op">=</span><span class="dv">100</span>, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>), </span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>                       subplot_kw<span class="op">=</span><span class="bu">dict</span>(yticks<span class="op">=</span>[<span class="fl">0.</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>]))</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].hist(x, <span class="op">**</span>kw)</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="vs">r'$x$'</span>)</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].hist(y, <span class="op">**</span>kw)</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="vs">r'$y$'</span>)</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a <span class="kw">in</span> ax:</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>    a.plot(t, p, lw<span class="op">=</span><span class="dv">3</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-21-output-1.png" width="758" height="373"></p>
</div>
</div>
</section>
<section id="multivariate-gaussians" class="level3">
<h3 class="anchored" data-anchor-id="multivariate-gaussians">Multivariate Gaussians</h3>
<p>By using a variable transformation, we can use samples from univariate standard Gaussians to generate samples from general multivariate Gaussians:</p>
<p><span class="math display">\[
x \sim \mathcal N(\mu, \Sigma) = \frac{1}{|2\pi\Sigma|^{1/2}} \exp\left\{-\frac{1}{2} (x-\mu)^T\Sigma^{-1} (x-\mu) \right\}
\]</span></p>
<p>where the covariance matrix <span class="math inline">\(\Sigma\)</span> is positive definite and therefore has a <a href="https://en.wikipedia.org/wiki/Cholesky_decomposition">Cholesky decomposition</a> <span class="math inline">\(\Sigma=LL^T\)</span> with a lower triangular matrix <span class="math inline">\(L\)</span>.</p>
<p>Now consider <span class="math inline">\(y\sim \mathcal N(0, I)\)</span> (these can be generated with the Box-Muller method) and the linear transformation</p>
<p><span class="math display">\[
x = \mu + L y\,\,\, \Rightarrow \,\,\, y = L^{-1} (x-\mu).
\]</span></p>
<p>The Jacobian of this transform is <span class="math inline">\(L\)</span> with determinant <span class="math inline">\(|L| = \sqrt{|\Sigma|}\)</span>. Thus the distribution of <span class="math inline">\(x\)</span> is</p>
<p><span class="math display">\[
p_x(x) = (2\pi)^{-D/2} \exp\left\{-\frac{1}{2} (x-\mu)^T (L^{-1})^T L^{-1} (x-\mu) \right\} / \sqrt{|\Sigma|} = \mathcal N(\mu, \Sigma)
\]</span></p>
<p>since <span class="math inline">\((L^{-1})^T L^{-1} = (L^T)^{-1} L^{-1} = (LL^T)^{-1} = \Sigma^{-1}\)</span>.</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 2d example</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co"># parameters of a 2d Gaussian</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>sigma1, sigma2, rho <span class="op">=</span> <span class="fl">1.</span>, <span class="fl">3.</span>, <span class="fl">0.7</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>Sigma <span class="op">=</span> np.array([[sigma1<span class="op">**</span><span class="dv">2</span>, rho <span class="op">*</span>sigma1 <span class="op">*</span> sigma2],</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>                  [rho <span class="op">*</span> sigma1 <span class="op">*</span> sigma2, sigma2<span class="op">**</span><span class="dv">2</span>]])</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> np.linalg.cholesky(Sigma)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> np.array([<span class="fl">1.</span>, <span class="op">-</span><span class="fl">1.</span>])</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="co"># transformation of the grid</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>grid1 <span class="op">=</span> np.reshape(np.mgrid[<span class="op">-</span><span class="fl">1.</span>:<span class="fl">1.</span>:<span class="ot">10j</span>,<span class="op">-</span><span class="dv">1</span>:<span class="op">+</span><span class="fl">1.</span>:<span class="ot">10j</span>], (<span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)).T</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>limits <span class="op">=</span> (<span class="op">-</span><span class="fl">5.</span>, <span class="fl">5.</span>)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> plt.cm.viridis(np.linspace(<span class="fl">0.</span>, <span class="fl">1.</span>, <span class="bu">len</span>(grid1)))</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="co"># sampling using a linear transformation</span></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> <span class="bu">int</span>(<span class="fl">1e3</span>)</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.standard_normal((S, <span class="dv">2</span>))</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> x.dot(L.T) <span class="op">+</span> mu</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate 2d Gaussian on a grid</span></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> [np.linspace(yy.<span class="bu">min</span>(), yy.<span class="bu">max</span>(), <span class="dv">100</span>) <span class="cf">for</span> yy <span class="kw">in</span> y.T]</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> np.reshape(np.meshgrid(<span class="op">*</span>axes), (<span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)).T</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>prob <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> np.<span class="bu">sum</span>(np.square((grid<span class="op">-</span>mu).dot(np.linalg.inv(L).T)), <span class="dv">1</span>))</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>prob <span class="op">=</span> prob.reshape(<span class="bu">len</span>(axes[<span class="dv">0</span>]), <span class="bu">len</span>(axes[<span class="dv">1</span>]))</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a><span class="co"># show initial distributions and its transformed version</span></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>kw <span class="op">=</span> <span class="bu">dict</span>(s<span class="op">=</span><span class="dv">20</span>, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>), sharex<span class="op">=</span><span class="st">'all'</span>, sharey<span class="op">=</span><span class="st">'all'</span>)</span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].scatter(<span class="op">*</span>grid1.T, c<span class="op">=</span>c)</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].scatter(<span class="op">*</span>(grid1.dot(L.T) <span class="op">+</span> mu).T, c<span class="op">=</span>c)</span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">0</span>].scatter(<span class="op">*</span>x.T, <span class="op">**</span>kw)</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">1</span>].scatter(<span class="op">*</span>y.T, <span class="op">**</span>kw)</span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">1</span>].scatter(<span class="op">*</span>mu, s<span class="op">=</span><span class="dv">100</span>, color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">1</span>].contour(axes[<span class="dv">0</span>], axes[<span class="dv">1</span>], prob)</span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>xmax <span class="op">=</span> <span class="dv">7</span></span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a <span class="kw">in</span> ax.flat:</span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>    a.set_xlim(<span class="op">-</span>xmax, xmax)</span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>    a.set_ylim(<span class="op">-</span>xmax, xmax)</span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-22-output-1.png" width="758" height="758"></p>
</div>
</div>
</section>
</section>
</section>
<section id="lecture-3-rejection-and-importance-sampling-1" class="level1">
<h1>Lecture 3: Rejection and Importance Sampling</h1>
<section id="outline-2" class="level2">
<h2 class="anchored" data-anchor-id="outline-2">Outline</h2>
<ul>
<li>More direct sampling methods</li>
<li>Rejection sampling</li>
<li>Importance sampling</li>
</ul>
<section id="recap" class="level3">
<h3 class="anchored" data-anchor-id="recap">Recap</h3>
<ul>
<li><p>Our goal is to compute <span class="math inline">\(\mathbb E_p[f]\)</span> for some probabilistic model <span class="math inline">\(p\)</span>. Most inference tasks can be reduced to such sums or integrals</p></li>
<li><p>Monte Carlo approximation: <span class="math inline">\(\mathbb E_p[f] \approx \hat f_S = \frac{1}{S} \sum_{s=1}^S f(x^{(s)})\)</span> where <span class="math inline">\(x^{(s)} \sim p(x)\)</span>.</p></li>
<li><p>Properties: unbiased (<span class="math inline">\(\mathbb E[\hat f_S] = \mathbb E_p[f]\)</span>) and <span class="math inline">\(\text{var}[\hat f_S] = \frac{\text{var}[f]}{S}\)</span></p></li>
<li><p>Monte Carlo errors shrink with <span class="math inline">\(1/\sqrt{S}\)</span>, no dependence on dimension <span class="math inline">\(D\)</span>, but factor <span class="math inline">\(\text{var}[f]\)</span> can depend on <span class="math inline">\(D\)</span> in an unfavorable fashion (hypersphere example)</p></li>
<li><p>Correct sampling means that approximate probability <span class="math inline">\(\hat p_S(x) = \frac{1}{S} \sum_s \delta(x - x^{(s)}) \to p(x)\)</span> for <span class="math inline">\(S\to\infty\)</span> (“histogram over samples approximates true model”). But how to sample?</p></li>
<li><p>Uniformly distributed samples can be generated with pseudo-random number generators such as the linear congruential generator. These have their own subtleties…</p></li>
<li><p>Direct sampling is possible via variable transformation methods that utilize the transformation rule: <span class="math inline">\(p_y(y) = p_x(h^{-1}(y)) / |\nabla h(h^{-1}(y))|\)</span></p></li>
<li><p>Inversion method: <span class="math inline">\(h(x) = P(x) = \int_{-\infty}^x p(x')\, dx'\)</span> (cumulative distribution function)</p></li>
<li><p>Some examples of transformation methods: Box-Muller <span class="math inline">\((x, y) = r (\cos\varphi, \sin\varphi)\)</span>, multivariate Gaussians</p></li>
</ul>
<section id="sampling-uniformly-from-the-hypersphere" class="level4">
<h4 class="anchored" data-anchor-id="sampling-uniformly-from-the-hypersphere">Sampling uniformly from the hypersphere</h4>
<p>The <span class="math inline">\(D\)</span>-dimensional standard Gaussian distribution <span class="math inline">\(\mathcal N(0, I)\)</span> can be used to sample from the <a href="https://en.wikipedia.org/wiki/N-sphere">hypersphere</a> in <span class="math inline">\(D\)</span>-dimensional space <span class="math inline">\(\mathbb S^{D-1} = \{ x \in \mathbb R^D : \|x\| = 1\}\)</span>. To see this, consider</p>
<p><span class="math display">\[
\mathcal N(0, I) = (2\pi)^{-D/2} \exp\bigl\{-\|x\|^2/2 \bigr\} \to (2\pi)^{-D/2}\, r^{D-1} e^{-r^2/2} \mathbb 1(\|x\| = 1)
\]</span></p>
<p>That is, the <span class="math inline">\(D\)</span>-dimensional standard Gaussian distribution is spherically symmetric and therefore</p>
<p><span class="math display">\[
x/\|x\| \sim \mathcal U(\mathbb S^{D-1})\,\,\, \text{where}\,\,\, x \sim \mathcal N(0, I)
\]</span></p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sampling from the hypersphere</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pylab <span class="im">as</span> plt</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_sphere(S, D<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="co">    S : number of samples</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="co">    D : dimension of embedding space</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.random.standard_normal((D, S))</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (x<span class="op">/</span>np.linalg.norm(x, axis<span class="op">=</span><span class="dv">0</span>)).T</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> sample_sphere(<span class="dv">10000</span>)</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>angle <span class="op">=</span> np.arctan2(x[:,<span class="dv">1</span>], x[:,<span class="dv">0</span>])</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>kw <span class="op">=</span> <span class="bu">dict</span>(color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, s<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].scatter(<span class="op">*</span>x[:<span class="dv">200</span>].T, <span class="op">**</span>kw)</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].hist(angle, bins<span class="op">=</span><span class="dv">30</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">'angle'</span>)</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-23-output-1.png" width="758" height="374"></p>
</div>
</div>
</section>
<section id="sampling-from-the-unit-ball" class="level4">
<h4 class="anchored" data-anchor-id="sampling-from-the-unit-ball">Sampling from the unit ball</h4>
<p>Using samples from the <span class="math inline">\((D-1)\)</span>-sphere we can also easily sample from the <a href="https://en.wikipedia.org/wiki/Ball_(mathematics)"><span class="math inline">\(D\)</span>-dimensional unit ball</a> <span class="math inline">\(\mathbb B^D = \{x\in\mathbb R^D : \|x\| \le 1\}\)</span>. Every element in <span class="math inline">\(\mathbb B^D\)</span> can be decomposed into</p>
<p><span class="math display">\[
x = r\, u \in \mathbb B^D\,\,\,\text{where}\,\,\, u\in\mathbb S^{D-1},\, r\in[0, 1]
\]</span></p>
<p>The distribution of <span class="math inline">\(r\)</span> follows from the surface area of <span class="math inline">\(\mathbb S^{D-1}\)</span> which scales with <span class="math inline">\(r^{D-1}\)</span>, therefore:</p>
<p><span class="math display">\[
p(r, u) = D\, r^{D-1}\, \mathbb 1(\|u\| = 1)
\]</span></p>
<p>The cdf of the radial component is <span class="math inline">\(r^D\)</span> and we obtain the following sampling rule:</p>
<ol type="1">
<li><p><span class="math inline">\(u^{(s)}=x^{(s)}/\|x^{(s)}\|\)</span> where <span class="math inline">\(x^{(s)} \sim \mathcal N(0, I_D)\)</span></p></li>
<li><p><span class="math inline">\(r^{(s)} = (v^{(s)})^{1/D}\)</span> where <span class="math inline">\(v^{(s)} \sim \mathcal U(0, 1)\)</span></p></li>
<li><p>then <span class="math inline">\(r^{(s)} u^{(s)} \sim \mathcal U(\mathbb B^D)\)</span></p></li>
</ol>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sampling from the unit ball</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_ball(S, D<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Sampling from the D-ball</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="co">    S : number of samples</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co">    D : dimension of embedding space</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># sample from hypersphere</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.random.standard_normal((D, <span class="bu">int</span>(S)))</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    u <span class="op">=</span> x <span class="op">/</span> np.linalg.norm(x, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># sample radius</span></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> np.random.random(<span class="bu">int</span>(S))<span class="op">**</span>(<span class="dv">1</span><span class="op">/</span>D)</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (u<span class="op">*</span>r).T</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>                                                                            </span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> sample_ball(<span class="dv">10000</span>)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>angle <span class="op">=</span> np.arctan2(x[:,<span class="dv">1</span>], x[:,<span class="dv">0</span>])</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>kw <span class="op">=</span> <span class="bu">dict</span>(color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, s<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].scatter(<span class="op">*</span>x[:<span class="dv">1000</span>].T, <span class="op">**</span>kw)</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].hist(angle, bins<span class="op">=</span><span class="dv">30</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">'angle'</span>)</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-24-output-1.png" width="758" height="374"></p>
</div>
</div>
</section>
<section id="sampling-from-a-radially-symmetric-distribution" class="level4">
<h4 class="anchored" data-anchor-id="sampling-from-a-radially-symmetric-distribution">Sampling from a radially symmetric distribution</h4>
<p>It is straightforward to generalize sampling from a unit ball to any spherically symmetric distribution. A spherically symmetric distribution over <span class="math inline">\(D\)</span> continuous variables has the form:</p>
<p><span class="math display">\[
    p(x) \propto f(\|x\|)
\]</span></p>
<p>where <span class="math inline">\(f(r)\)</span> is defined for <span class="math inline">\(r\in\mathbb R_+\)</span>. A convenient parameterization are spherical coordinates, resulting in the distribution</p>
<p><span class="math display">\[
p(x) \to r^{D-1}f(r)\, \mathcal{U}(\mathbb S^{D-1})
\]</span></p>
<p>We know how to generate uniform samples from <span class="math inline">\(\mathcal U(\mathbb S^{D-1})\)</span>. The remaining problem is to sample from</p>
<p><span class="math display">\[
p(r) \propto r^{D-1} f(r),\,\,\, r\ge 0
\]</span></p>
<p>So we have reduced a <span class="math inline">\(D\)</span>-dimensional sampling problem to a one-dimensional problem.</p>
</section>
<section id="sampling-from-an-elliptically-symmetric-distribution" class="level4">
<h4 class="anchored" data-anchor-id="sampling-from-an-elliptically-symmetric-distribution">Sampling from an elliptically symmetric distribution</h4>
<p>A similar technique can be used to generalize sampling from spherically symmetric distributions to elliptically symmetric distributions of the form</p>
<p><span class="math display">\[
p(x) \propto f\bigl((x-b)^T\!\!A(x-b)\bigr)
\]</span></p>
<p>where <span class="math inline">\(x, b \in \mathbb R^D\)</span> and <span class="math inline">\(A\in\mathbb R^{D\times D}\)</span> is positive definite; <span class="math inline">\(f(r)\ge 0\)</span> is an (unnormalized) radial pdf defined for <span class="math inline">\(r\in\mathbb R_+\)</span>.</p>
</section>
</section>
<section id="using-known-relations-between-probability-distributions-for-sampling" class="level3">
<h3 class="anchored" data-anchor-id="using-known-relations-between-probability-distributions-for-sampling">Using known relations between probability distributions for sampling</h3>
<p>The <a href="https://en.wikipedia.org/wiki/Erlang_distribution">Erlang distribution</a>, a special version of the <a href="https://en.wikipedia.org/wiki/Gamma_distribution">Gamma distribution</a>, has the functional form</p>
<p><span id="eq-erlang"><span class="math display">\[
    p(x\mid{}k, \lambda) = \frac{\lambda^k}{\Gamma(k)} x^{k - 1} e^{-\lambda x}, \, k \in \mathbb N, \, x &gt; 0,\, \lambda &gt; 0
\tag{21}\]</span></span></p>
<p>We can use exponentially distributed random variables <span class="math inline">\(z_i \sim \lambda e^{-\lambda z_i}, i=1, \ldots, k\)</span> to produce an Erlang variate. Define <span class="math inline">\(z_i = x y_i\)</span> with <span class="math inline">\(y_i\in[0, 1]\)</span> for <span class="math inline">\(1 \le i \le k-1\)</span> and <span class="math inline">\(z_k = x \bigl(1-\sum_{i=1}^{k-1} y_i\bigr)\)</span>, then <span class="math inline">\(x = \sum_i z_i\)</span>. The Jacobian of the parameter transform is</p>
<p><span class="math display">\[
\frac{\partial (z_1, \ldots, z_k)}{\partial (x, y_1, \ldots, y_{k-1})} =
\begin{pmatrix}
y_1 &amp; x &amp; 0 &amp; \cdots &amp; 0 \\
y_2 &amp; 0 &amp; x &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
y_{k-1} &amp; 0 &amp; 0 &amp; \cdots &amp; x \\
1- \sum_i y_{i} &amp; -x &amp; -x &amp; \cdots &amp; -x \\
\end{pmatrix}
\]</span></p>
<p>with determinant</p>
<p><span class="math display">\[
\left|\frac{\partial (z_1, \ldots, z_k)}{\partial (x, y_1, \ldots, y_{k-1})}\right| \propto x^{k-1}
\]</span></p>
<p>Therefore:</p>
<p><span class="math display">\[
x \sim x^{k-1} e^{-\lambda x}
\]</span></p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sampling from Erlang</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> gammaln</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_erlang(S, k<span class="op">=</span><span class="dv">1</span>, beta<span class="op">=</span><span class="fl">1.</span>):</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    u <span class="op">=</span> np.random.random((<span class="bu">int</span>(S), k))</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> <span class="op">-</span> np.log(u) <span class="op">/</span> beta</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> z.<span class="bu">sum</span>(<span class="dv">1</span>)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> <span class="fl">2.</span></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> <span class="fl">1e4</span></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> sample_erlang(S, k<span class="op">=</span>k, beta<span class="op">=</span>beta)</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> np.linspace(<span class="fl">0.</span>, x.<span class="bu">max</span>(), <span class="dv">1000</span>)</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> k <span class="op">*</span> np.log(beta) <span class="op">+</span> (k<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> np.log(t<span class="op">+</span><span class="fl">1e-100</span>) <span class="op">-</span> beta <span class="op">*</span> t <span class="op">-</span> gammaln(k)</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> np.exp(p)</span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>ax.plot(t, p, color<span class="op">=</span><span class="st">'r'</span>, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>ax.hist(x, bins<span class="op">=</span><span class="dv">50</span>, color<span class="op">=</span><span class="st">'lightgrey'</span>, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>fig.tight_layout() </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-25-output-1.png" width="470" height="470"></p>
</div>
</div>
<p>There are many more relationships between standard univariate probability distributions that can be exploited for sampling (an interactive version of the following Figure can be found at <a href="http://www.math.wm.edu/~leemis/chart/UDR/UDR.html">Leemis &amp; Mc Queston: Univariate Distribution RelationShips</a>):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="images/UnivariateDistributionRelationships.png" title="Relationshipts between univariate pdfs" class="img-fluid figure-img">
</figure>
<p></p><figcaption class="figure-caption">Relationships between pdfs</figcaption><p></p>
</figure>
</div>
</section>
<section id="further-reading" class="level3">
<h3 class="anchored" data-anchor-id="further-reading">Further reading</h3>
<p>Donald Knuth: <a href="https://en.wikipedia.org/wiki/The_Art_of_Computer_Programming">The Art of Computer Programming, Vol. 2, Chap. 1</a></p>
<p>Luc Devroye: <a href="https://link.springer.com/book/10.1007/978-1-4613-8643-8">Non-Uniform Random Variate Generation</a></p>
</section>
</section>
<section id="rejection-sampling" class="level2">
<h2 class="anchored" data-anchor-id="rejection-sampling">Rejection sampling</h2>
<p>Direct sampling methods are specifically designed for particular target distributions. For complex probabilistic models such as the Ising model, it will not be possible to use these methods. We will now discuss methods that can be applied to more general probabilistic models.</p>
<p><a href="https://en.wikipedia.org/wiki/Rejection_sampling"><em>Rejection sampling</em></a> is an early sampling approach that has been developed by von Neumann. The idea is to use a helper distribution <span class="math inline">\(q\)</span> from which we can sample easily in order to sample from a more complicated model <span class="math inline">\(p\)</span>. To be a valid proposal distribution, <span class="math inline">\(q\)</span> must satisfy</p>
<p><span id="eq-rejection_proposal"><span class="math display">\[
    p(x) \le M q(x)
\tag{22}\]</span></span></p>
<p>for a constant <span class="math inline">\(M\)</span> which implies <span class="math inline">\(M\ge 1\)</span>, since <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> are normalized pdfs. Moreover, the support of <span class="math inline">\(p\)</span> should be contained in the support of the proposal distribution <span class="math inline">\(q\)</span>. Let’s define the ratio</p>
<p><span id="eq-rejection_accprob"><span class="math display">\[
    r(x) := \frac{p(x)}{M q(x)}
\tag{23}\]</span></span></p>
<p>which is smaller than or equal to 1 for all <span class="math inline">\(x\)</span> with <span class="math inline">\(q(x)&gt;0\)</span>, otherwise we set <span class="math inline">\(r(x) = 1\)</span>.</p>
<section id="algorithm-rejection-sampling" class="level3">
<h3 class="anchored" data-anchor-id="algorithm-rejection-sampling">Algorithm: Rejection sampling</h3>
<p>The algorithm produces random samples <span class="math inline">\(x^{(s)}\)</span> by iterating over the following steps until the desired number of samples <span class="math inline">\(S\)</span> has been generated:</p>
<ol type="1">
<li><p>Draw <span class="math inline">\(x \sim q(x)\)</span></p></li>
<li><p>Draw <span class="math inline">\(u \sim \mathcal U(0, 1)\)</span></p></li>
<li><p><span class="math inline">\(r \gets \frac{p(x)}{Mq(x)}\)</span></p></li>
<li><p>If <span class="math inline">\(u &lt; r\)</span>, then</p>
<ul>
<li><span class="math inline">\(x^{(s)} \gets x\)</span></li>
<li><span class="math inline">\(s \gets s+1\)</span>.</li>
</ul></li>
<li><p>If <span class="math inline">\(s &lt; S\)</span>, go to 1.</p></li>
</ol>
<section id="example-sampling-a-gaussian-with-a-cauchy-proposal" class="level4">
<h4 class="anchored" data-anchor-id="example-sampling-a-gaussian-with-a-cauchy-proposal">Example: Sampling a Gaussian with a Cauchy proposal</h4>
<p>The standard Gaussian distribution is</p>
<p><span class="math display">\[
p(x) = \frac{1}{\sqrt{2\pi}}\, e^{-x^2/2}
\]</span></p>
<p>and the Cauchy distribution</p>
<p><span class="math display">\[
q(x) = \frac{1}{\pi} \frac{1}{1 + x^2}\, .
\]</span></p>
<p>The Cauchy distribution can be sampled with the inversion method: <span class="math inline">\(x^{(s)} = -\tan(\pi u^{(s)}), \, u^{(s)} \sim \mathcal U(0, 1)\)</span> (see table above).</p>
<p>To show that the Cauchy distribution is a valid proposal distribution, we first have to bound the ratio</p>
<p><span class="math display">\[
f(x) = \frac{p(x)}{q(x)} = \sqrt{\frac{\pi}{2}}\, (1 + x^2)\, e^{-x^2/2}\, .
\]</span></p>
<p>The first derivative is <span class="math display">\[
f'(x)  = \sqrt{\frac{\pi}{2}}\, x\, (1 - x^2)\, e^{-x^2/2}
\]</span> with zeros <span class="math inline">\(x=-1, 0, 1\)</span>. The second derivative <span class="math inline">\(f''(x) = \sqrt{\frac{\pi}{2}}\, (x^4 - 4x^2 +1)\, e^{-x^2/2}\)</span> is positive at <span class="math inline">\(x=0\)</span> and negative at <span class="math inline">\(x=\pm 1\)</span>. Therefore, <span class="math inline">\(x=\pm 1\)</span> are the locations of the maxima of <span class="math inline">\(f\)</span>, and the upper bound is <span class="math inline">\(M := f(1) = \sqrt{2\pi/e} \approx 1.52\)</span>.</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_cauchy(S):</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    u <span class="op">=</span> np.random.random(<span class="bu">int</span>(S))</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.tan(np.pi <span class="op">*</span> u)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_ratio(t):</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> (<span class="dv">1</span> <span class="op">+</span> t<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> t<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> np.sqrt(np.e)</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> <span class="bu">int</span>(<span class="fl">1e3</span>)</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>M <span class="op">=</span> np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi<span class="op">/</span>np.e)</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> sample_cauchy(M<span class="op">*</span>S)</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> calc_ratio(y)</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>u <span class="op">=</span> np.random.random(y.shape)</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>Mq <span class="op">=</span> M <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> y<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> np.pi</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>, <span class="dv">1000</span>) <span class="op">*</span> <span class="dv">5</span></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> t<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> np.pi</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> t<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi)</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>M</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>1.520346901066281</code></pre>
</div>
</div>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>), sharex<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>                       sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].fill_between(t, t<span class="op">*</span><span class="fl">0.</span>, M<span class="op">*</span>q, color<span class="op">=</span><span class="st">'b'</span>, alpha<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(t, M<span class="op">*</span>q, color<span class="op">=</span><span class="st">'b'</span>, lw<span class="op">=</span><span class="dv">2</span>, ls<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="vs">r'scaled Cauchy $Mq$'</span>)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(t, p, color<span class="op">=</span><span class="st">'r'</span>, lw<span class="op">=</span><span class="dv">2</span>, ls<span class="op">=</span><span class="st">'-'</span>, label<span class="op">=</span><span class="vs">r'Gaussian $p$'</span>)</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].legend(loc<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].fill_between(t, t<span class="op">*</span><span class="fl">0.</span>, M<span class="op">*</span>q, color<span class="op">=</span><span class="st">'b'</span>, alpha<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].scatter(y[u<span class="op">&lt;</span>r], (Mq<span class="op">*</span>u)[u<span class="op">&lt;</span>r], color<span class="op">=</span><span class="st">'r'</span>, s<span class="op">=</span><span class="dv">10</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].scatter(y[u<span class="op">&gt;=</span>r], (Mq<span class="op">*</span>u)[u<span class="op">&gt;=</span>r], color<span class="op">=</span><span class="st">'b'</span>, s<span class="op">=</span><span class="dv">10</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(t, M<span class="op">*</span>q, color<span class="op">=</span><span class="st">'b'</span>, lw<span class="op">=</span><span class="dv">2</span>, ls<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="vs">r'scaled Cauchy $Mq$'</span>)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(t, p, color<span class="op">=</span><span class="st">'r'</span>, lw<span class="op">=</span><span class="dv">2</span>, ls<span class="op">=</span><span class="st">'-'</span>, label<span class="op">=</span><span class="vs">r'Gaussian $p$'</span>)</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].hist(y[u<span class="op">&lt;</span>r], bins<span class="op">=</span><span class="dv">50</span>, color<span class="op">=</span><span class="st">'lightgrey'</span>, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].plot(t, p, color<span class="op">=</span><span class="st">'r'</span>, lw<span class="op">=</span><span class="dv">2</span>, ls<span class="op">=</span><span class="st">'-'</span>)</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_xlim(<span class="op">-</span><span class="fl">5.</span>, <span class="dv">5</span>)</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_ylim(<span class="fl">0.</span>, <span class="fl">0.5</span>)</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-27-output-1.png" width="1142" height="374"></p>
</div>
</div>
</section>
</section>
<section id="why-does-rejection-sampling-work" class="level3">
<h3 class="anchored" data-anchor-id="why-does-rejection-sampling-work">Why does rejection sampling work?</h3>
<p>Let’s formalize the rejection sampling algorithm. To this end, we introduce a binary random variable <span class="math inline">\(a\in\{0, 1\}\)</span> indicating if a proposal <span class="math inline">\(x \sim q(x)\)</span> has been accepted or not. The probability of being accepted, <span class="math inline">\(a=1\)</span>, or rejected, <span class="math inline">\(a=0\)</span>, is</p>
<p><span class="math display">\[
p(a=1\mid{}x) = r(x), \,\,\ p(a=0\mid{}x) = 1 - r(x)\, .
\]</span></p>
<p>This means that <span class="math inline">\(a\)</span> is a <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli variable</a>:</p>
<p><span class="math display">\[
p(a\mid{}x) = \bigl[r(x)\bigr]^a \bigl[1-r(x)\bigr]^{1-a}, \,\,\, a\in\{0, 1\}\, .
\]</span></p>
<p>The joint distribution of <span class="math inline">\(x\)</span> and <span class="math inline">\(a\)</span> is</p>
<p><span class="math display">\[
p(a, x) = p(a\mid{}x)\, q(x)
\]</span></p>
<p>since <span class="math inline">\(x\sim q\)</span>. The accepted samples are those for which <span class="math inline">\(a=1\)</span>. These samples follow the <em>conditional</em> distribution <span class="math inline">\(p(x\mid{}a=1)\)</span>. That is</p>
<p><span class="math display">\[
x^{(s)} \sim p(x\mid{}a=1)\, .
\]</span></p>
<p>It is straight forward to compute this distribution. We have <span class="math inline">\(p(x\mid{}a=1) = p(x, a=1)\, /\, p(a=1)\)</span>. We need to compute the marginal probability <span class="math inline">\(p(a=1)\)</span>:</p>
<p><span id="eq-rs-acceptance"><span class="math display">\[
p(a=1) = \int p(x, a=1)\, dx = \int q(x)\, r(x)\, dx = \int q(x)\, \frac{p(x)}{Mq(x)}\, dx = \frac{1}{M}
\tag{24}\]</span></span></p>
<p>since <span class="math inline">\(p\)</span> is normalized. Equation (<a href="#eq-rs-acceptance">Equation&nbsp;24</a>) tells us that the average probability to propose an acceptable sample is <span class="math inline">\(M^{-1}\)</span>. We can now compute the desired conditional distribution <span class="math inline">\(p(x\mid{}a=1)\)</span>:</p>
<p><span class="math display">\[
p(x\mid{}a=1) = \frac{p(x, a=1)}{p(a=1)} = M q(x)\, r(x) = M q(x)\, \frac{p(x)}{M q(x)} = p(x)\, .
\]</span></p>
<section id="waiting-time-before-acceptance" class="level4">
<h4 class="anchored" data-anchor-id="waiting-time-before-acceptance">Waiting time before acceptance</h4>
<p>We can also compute the number of attempts it needs to generate an acceptable proposal. Let’s call this number <span class="math inline">\(T\)</span>. The probability that a proposal is accepted after <span class="math inline">\(T-1\)</span> unsuccessful trials is</p>
<p><span class="math display">\[
\Pr(T) = [p(a=0)]^{T-1} p(a=1) = [1-M^{-1}]^{T-1} M^{-1}, \,\,\, T\in \{1, 2, 3, \ldots \}
\]</span></p>
<p>which is the probability that the first <span class="math inline">\(T-1\)</span> samples are rejected (<span class="math inline">\(a^{(t)}=0, \, t&lt;T\)</span>) and the last sample is accepted <span class="math inline">\(a^{(T)}=1\)</span>. This distribution is a <a href="https://en.wikipedia.org/wiki/Geometric_distribution">geometric distribution</a> and normalized since</p>
<p><span class="math display">\[
\sum_{T\ge 1} \Pr(T) = \sum_{T\ge 1} [1-M^{-1}]^{T-1}\, M^{-1} = M^{-1} \sum_{T\ge 0} [1-M^{-1}]^{T} = \frac{M^{-1}}{1 - (1 - M^{-1})} = 1
\]</span></p>
<p>using the summation rules for <a href="https://en.wikipedia.org/wiki/Geometric_progression">geometric progressions</a> and <a href="https://en.wikipedia.org/wiki/Geometric_series">geometric series</a>.</p>
<p>The expected time one has to wait until an acceptable sample is proposed is</p>
<p><span class="math display">\[
\mathbb E[T] = \sum_{T\ge 1} T\, \Pr(T) = M^{-1} \sum_{T\ge 1} T\, [1-M^{-1}]^{T-1} = 1 + M^{-1} \sum_{T \ge 0} T \, [1-M^{-1}]^T\, .
\]</span></p>
<p>To compute the last series, let us rewrite it as <span class="math inline">\(\sum_{T\ge 0} T e^{-\lambda T}\)</span> where <span class="math inline">\(\lambda = - \log(1-M^{-1}) &gt; 0\)</span>:</p>
<p><span class="math display">\[
\sum_{T\ge 0} T e^{-\lambda T} = - \sum_{T\ge 0} \frac{d}{d\lambda} e^{-\lambda T} = - \frac{d}{d\lambda} \sum_{T\ge 0} e^{-\lambda T} = - \frac{d}{d\lambda} \frac{1}{1 - e^{-\lambda}} = \frac{e^{-\lambda}}{(1 - e^{-\lambda})^2}\, .
\]</span></p>
<p>By substituting <span class="math inline">\(e^{-\lambda} = 1-M^{-1}\)</span> we obtain</p>
<p><span class="math display">\[
\mathbb E[T] = 1 + M^{-1} \frac{1-M^{-1}}{(1 -(1-M^{-1}))^2} = 1 + M^{-1} \frac{1-M^{-1}}{M^{-2}} = M\, .
\]</span></p>
<p>The larger <span class="math inline">\(M\)</span>, the longer is the average time that we have to wait until a sample is accepted. Therefore, we should try to design an envelope <span class="math inline">\(Mq(x)&gt;p(x)\)</span> that is as tight as possible.</p>
<p>Let us modify the above code for sampling a Gaussian using Cauchy proposals by allowing <span class="math inline">\(M\)</span> to be greater than the tightest upper bound <span class="math inline">\(\sqrt{2\pi/e}\)</span> and investigate the effect on the waiting time:</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tightest bound</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>M_opt <span class="op">=</span> (<span class="dv">2</span><span class="op">*</span>np.pi<span class="op">/</span>np.e)<span class="op">**</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_ratio(t, M<span class="op">=</span>M_opt):</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> (<span class="dv">1</span> <span class="op">+</span> t<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> t<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> np.sqrt(np.e) <span class="op">*</span> (M_opt<span class="op">/</span>M)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate Gaussian and Cauchy distribution</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>, <span class="dv">1000</span>) <span class="op">*</span> <span class="dv">5</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> t<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> np.pi</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> t<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi)</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a><span class="co"># rejection sampling</span></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> <span class="bu">int</span>(<span class="fl">2e4</span>)</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>), sharex<span class="op">=</span><span class="st">'col'</span>)</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a><span class="co"># M = factor * M_opt</span></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, factor <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="dv">2</span>, <span class="dv">4</span>]):</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span> factor <span class="op">*</span> M_opt</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> sample_cauchy(S)</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> calc_ratio(y, M)</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>    u <span class="op">=</span> np.random.random(y.shape)</span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>    Mq <span class="op">=</span> M <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> y<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> np.pi</span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># waiting times</span></span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>    T <span class="op">=</span> np.diff(np.nonzero(u<span class="op">&lt;</span>r)[<span class="dv">0</span>])</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a>    ax[i,<span class="dv">0</span>].set_title(<span class="vs">r'$M=</span><span class="sc">{0}</span><span class="vs">\, \sqrt</span><span class="sc">{{</span><span class="vs">2\pi/e</span><span class="sc">}}</span><span class="vs">$'</span>.<span class="bu">format</span>(factor))</span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a>    ax[i,<span class="dv">0</span>].fill_between(t, t<span class="op">*</span><span class="fl">0.</span>, M<span class="op">*</span>q, color<span class="op">=</span><span class="st">'b'</span>, alpha<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a>    ax[i,<span class="dv">0</span>].plot(t, M<span class="op">*</span>q, color<span class="op">=</span><span class="st">'b'</span>, lw<span class="op">=</span><span class="dv">2</span>, ls<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="vs">r'scaled Cauchy $Mq$'</span>)</span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a>    ax[i,<span class="dv">0</span>].plot(t, p, color<span class="op">=</span><span class="st">'r'</span>, lw<span class="op">=</span><span class="dv">2</span>, ls<span class="op">=</span><span class="st">'-'</span>, label<span class="op">=</span><span class="vs">r'Gaussian $p$'</span>)</span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a>    ax[i,<span class="dv">0</span>].legend()</span>
<span id="cb32-34"><a href="#cb32-34" aria-hidden="true" tabindex="-1"></a>    <span class="co">#</span></span>
<span id="cb32-35"><a href="#cb32-35" aria-hidden="true" tabindex="-1"></a>    ax[i,<span class="dv">1</span>].set_title(<span class="vs">r'$M=</span><span class="sc">{0}</span><span class="vs">\, \sqrt</span><span class="sc">{{</span><span class="vs">2\pi/e</span><span class="sc">}}</span><span class="vs">$'</span>.<span class="bu">format</span>(factor))</span>
<span id="cb32-36"><a href="#cb32-36" aria-hidden="true" tabindex="-1"></a>    ax[i,<span class="dv">1</span>].fill_between(t, t<span class="op">*</span><span class="fl">0.</span>, M<span class="op">*</span>q, color<span class="op">=</span><span class="st">'b'</span>, alpha<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb32-37"><a href="#cb32-37" aria-hidden="true" tabindex="-1"></a>    ax[i,<span class="dv">1</span>].scatter(y[u<span class="op">&lt;</span>r], (Mq<span class="op">*</span>u)[u<span class="op">&lt;</span>r], color<span class="op">=</span><span class="st">'r'</span>, s<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb32-38"><a href="#cb32-38" aria-hidden="true" tabindex="-1"></a>    ax[i,<span class="dv">1</span>].scatter(y[u<span class="op">&gt;=</span>r], (Mq<span class="op">*</span>u)[u<span class="op">&gt;=</span>r], color<span class="op">=</span><span class="st">'b'</span>, s<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb32-39"><a href="#cb32-39" aria-hidden="true" tabindex="-1"></a>    ax[i,<span class="dv">1</span>].plot(t, M<span class="op">*</span>q, color<span class="op">=</span><span class="st">'b'</span>, lw<span class="op">=</span><span class="dv">2</span>, ls<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="vs">r'scaled Cauchy $Mq$'</span>)</span>
<span id="cb32-40"><a href="#cb32-40" aria-hidden="true" tabindex="-1"></a>    ax[i,<span class="dv">1</span>].plot(t, p, color<span class="op">=</span><span class="st">'r'</span>, lw<span class="op">=</span><span class="dv">2</span>, ls<span class="op">=</span><span class="st">'-'</span>, label<span class="op">=</span><span class="vs">r'Gaussian $p$'</span>)</span>
<span id="cb32-41"><a href="#cb32-41" aria-hidden="true" tabindex="-1"></a>    <span class="co">#</span></span>
<span id="cb32-42"><a href="#cb32-42" aria-hidden="true" tabindex="-1"></a>    times, counts <span class="op">=</span> np.unique(T, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-43"><a href="#cb32-43" aria-hidden="true" tabindex="-1"></a>    ax[i,<span class="dv">2</span>].bar(times, counts, color<span class="op">=</span><span class="st">'lightgrey'</span>)</span>
<span id="cb32-44"><a href="#cb32-44" aria-hidden="true" tabindex="-1"></a>    ax[i,<span class="dv">2</span>].set_xlim(<span class="dv">0</span>, times.<span class="bu">max</span>()<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb32-45"><a href="#cb32-45" aria-hidden="true" tabindex="-1"></a>    ax[i,<span class="dv">2</span>].set_ylim(<span class="dv">10</span>, counts.<span class="bu">max</span>()<span class="op">*</span><span class="fl">1.1</span>)</span>
<span id="cb32-46"><a href="#cb32-46" aria-hidden="true" tabindex="-1"></a>    ax[i,<span class="dv">2</span>].set_xlabel(<span class="vs">r'waiting time $T$'</span>)</span>
<span id="cb32-47"><a href="#cb32-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-48"><a href="#cb32-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> a <span class="kw">in</span> ax[i,:<span class="dv">2</span>]:</span>
<span id="cb32-49"><a href="#cb32-49" aria-hidden="true" tabindex="-1"></a>        a.set_xlim(<span class="op">-</span><span class="dv">5</span>, <span class="fl">5.</span>)</span>
<span id="cb32-50"><a href="#cb32-50" aria-hidden="true" tabindex="-1"></a>        a.set_ylim(<span class="fl">0.</span>, factor<span class="op">/</span><span class="dv">2</span>)</span>
<span id="cb32-51"><a href="#cb32-51" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-52"><a href="#cb32-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> a <span class="kw">in</span> ax[:,<span class="dv">2</span>]:</span>
<span id="cb32-53"><a href="#cb32-53" aria-hidden="true" tabindex="-1"></a>        a.set_xlim(<span class="dv">0</span>, times.<span class="bu">max</span>()<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb32-54"><a href="#cb32-54" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-28-output-1.png" width="1142" height="754"></p>
</div>
</div>
</section>
</section>
<section id="geometric-interpretation" class="level3">
<h3 class="anchored" data-anchor-id="geometric-interpretation">Geometric interpretation</h3>
<p>A geometric interpretation of rejection sampling is that we generate points <span class="math inline">\((x,y)\)</span> under the graph of <span class="math inline">\(Mq(x)\)</span> in the following fashion: The <span class="math inline">\(x\)</span> coordinate is drawn from <span class="math inline">\(q\)</span>, and the <span class="math inline">\(y\)</span> coordinate from a uniform distribution <span class="math inline">\(y=M q(x) u\)</span> where <span class="math inline">\(u\sim \mathcal U(0, 1)\)</span>:</p>
<p><span class="math display">\[
(x, y) \sim q(x)\, \mathbb 1\bigl(y &lt; Mq(x)\bigr)
\]</span></p>
<p><span class="math inline">\((x,y)\)</span> is accepted if <span class="math inline">\(y &lt; p(x)\)</span> otherwise it is rejected. Therefore, the area under <span class="math inline">\(Mq\)</span> can be separated into an <em>acceptance region</em> and a <em>rejection region</em>. The value of <span class="math inline">\(M\)</span> determines the relative size of the rejection and acceptance regions.</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.ndimage <span class="im">import</span> gaussian_filter</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>, <span class="dv">1000</span>) <span class="op">*</span> <span class="dv">10</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> np.array([<span class="op">-</span><span class="dv">2</span>,  <span class="fl">5.</span> ])</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> np.array([<span class="fl">90.</span>, <span class="fl">120.</span> ])</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.array([<span class="fl">0.7</span>, <span class="fl">0.3</span>])</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(mu)):</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> t <span class="op">*</span> <span class="fl">0.</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>    y[np.argmin(np.fabs(mu[k]<span class="op">-</span>t))] <span class="op">=</span> <span class="fl">1.</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>    p <span class="op">+=</span> gaussian_filter(y, sigma<span class="op">=</span>sigma[k]) <span class="op">*</span> w[k]</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>p <span class="op">/=</span> p.<span class="bu">sum</span>()</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> t<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> <span class="dv">16</span>) <span class="op">+</span> <span class="fl">0.1</span></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>q <span class="op">/=</span> q.<span class="bu">sum</span>()</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>M <span class="op">=</span> np.<span class="bu">max</span>(p<span class="op">/</span>q) <span class="op">*</span> <span class="fl">1.1</span></span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>fs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(subplot_kw<span class="op">=</span><span class="bu">dict</span>(xticks<span class="op">=</span>[], yticks<span class="op">=</span>[]))</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>ax.fill_between(t, p, q<span class="op">*</span>M, color<span class="op">=</span><span class="st">'r'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>ax.plot(t, q<span class="op">*</span>M, color<span class="op">=</span><span class="st">'r'</span>, lw<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="vs">r'$Mq$'</span>)</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>ax.fill_between(t, p<span class="op">*</span><span class="dv">0</span>, p, color<span class="op">=</span><span class="st">'b'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>ax.plot(t, p, color<span class="op">=</span><span class="st">'b'</span>, lw<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="vs">r'$p$'</span>)</span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>ax.annotate(<span class="st">'accept'</span>, (<span class="fl">0.32</span>, <span class="fl">0.2</span>), xycoords<span class="op">=</span><span class="st">'axes fraction'</span>, color<span class="op">=</span><span class="st">'b'</span>, fontsize<span class="op">=</span>fs)</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>ax.annotate(<span class="st">'reject'</span>, (<span class="fl">0.55</span>, <span class="fl">0.4</span>), xycoords<span class="op">=</span><span class="st">'axes fraction'</span>, color<span class="op">=</span><span class="st">'r'</span>, fontsize<span class="op">=</span>fs)</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span>fs)</span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>ax.set_frame_on(<span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-29-output-1.png" width="540" height="389"></p>
</div>
</div>
<p>There are multiple factors that influence <span class="math inline">\(M\)</span>. The size of the support of <span class="math inline">\(q\)</span> has a direct impact on <span class="math inline">\(M\)</span>: The larger the support of <span class="math inline">\(q\)</span>, the larger we have to choose <span class="math inline">\(M\)</span> to still satisfy the condition <span class="math inline">\(p(x) \le Mq(x)\)</span>, because the probability mass of the proposal distribution is distributed over a larger region and the density <span class="math inline">\(q\)</span> is scaled down. The tightest <span class="math inline">\(M\)</span> can be found by maximizing the ratio <span class="math inline">\(p(x)/q(x)\)</span> over the sample space. In general, this will be a hard optimization problem. But even if we find the tightest value of <span class="math inline">\(M\)</span>, the size of the rejection region will grow exponentially with dimension <span class="math inline">\(D\)</span>.</p>
</section>
<section id="rejection-sampling-scales-badly-with-dimension" class="level3">
<h3 class="anchored" data-anchor-id="rejection-sampling-scales-badly-with-dimension">Rejection sampling scales badly with dimension</h3>
<p>Finding tight bounds in high dimensions is very difficult. Moreover, the volume of the rejection region will scale exponentially with dimension. For example, if we sample a <span class="math inline">\(D\)</span> dimensional Gaussian by using <span class="math inline">\(D\)</span> Cauchy variates, than each dimension contributes a factor of <span class="math inline">\(\sqrt{2\pi/e} \approx 1.5\)</span> such that the overall <span class="math inline">\(M\)</span> scales as <span class="math inline">\(1.5^D\)</span>. As a consequence, the waiting time will scale exponentially with dimension and using rejection sampling will become increasingly inefficient as the size of our probabilistic model grows.</p>
</section>
<section id="unnormalized-target-and-proposal-distribution" class="level3">
<h3 class="anchored" data-anchor-id="unnormalized-target-and-proposal-distribution">Unnormalized target and proposal distribution</h3>
<p>Often, it is not possible to normalize a probabilistic model. So we only know <span class="math inline">\(p^*(x)\)</span> and <span class="math inline">\(q^*(x)\)</span> with</p>
<p><span class="math display">\[
\begin{aligned}
    p(x) &amp;= \frac{p^*(x)}{Z_p}\,\,\,\text{with}\,\,\, Z_p = \int_{\mathcal X} p^*(x)\, dx &lt; \infty\\
    q(x) &amp;= \frac{q^*(x)}{Z_q}\,\,\,\text{with}\,\,\, Z_q = \int_{\mathcal X} q^*(x)\, dx &lt; \infty\, .
\end{aligned}
\]</span></p>
<p>The condition that has to be satisfied is now</p>
<p><span class="math display">\[
p^*(x) \le M q^*(x)\,\,\, \Rightarrow M \ge Z_p/Z_q\, .
\]</span></p>
<p>In analogy to Eq. (<a href="#eq-rejection_accprob">Equation&nbsp;23</a>), the acceptance probability changes to</p>
<p><span class="math display">\[
r(x) = \frac{p^*(x)}{M q^*(x)} \le 1
\]</span></p>
<p>Otherwise the sampling procedure and the proof of its validity work in the same fashion:</p>
<p><span class="math display">\[
p(x, a=1) = q(x)\, r(x) = \frac{q^*(x)}{Z_q} \frac{p^*(x)}{M q^*(x)} = \frac{p^*(x)}{MZ_q}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
p(a=1) = \int_{\mathcal X} p(x, a=1)\, dx = \frac{Z_p}{MZ_q}\, .
\]</span></p>
<p>So <span class="math display">\[
x^{(s)} \sim \frac{p(x, a=1)}{p(a=1)} = \frac{p^*(x)}{MZ_q}\, \frac{MZ_q}{Z_p} = \frac{p^*(x)}{Z_p} = p(x)\, .
\]</span></p>
</section>
</section>
<section id="importance-sampling" class="level2">
<h2 class="anchored" data-anchor-id="importance-sampling">Importance sampling</h2>
<p>Rejection sampling becomes very inefficient as soon as no tight bound can be found and most samples are rejected. We saw a drastic version of this problem in our attempt to estimate the volume of the <span class="math inline">\(D\)</span>-ball by accepting or rejecting samples from a hypercube: The acceptance probability is the ratio of the volumes of the <span class="math inline">\(D\)</span>-ball and the <span class="math inline">\(D\)</span>-cube and drops to zero with a rate that is exponential in <span class="math inline">\(D\)</span>.</p>
<p><a href="https://en.wikipedia.org/wiki/Importance_sampling"><em>Importance sampling</em></a> tries to overcome some of these problems by using a strategy that does not reject samples, but weighs them thereby avoiding to “waste” any samples. Otherwise the idea of importance sampling is very similar to rejection sampling: A helper distribution <span class="math inline">\(q\)</span> (from which we can sample easily) is used to generate samples that are now reweighted rather than accepted or rejected. The weights are chosen such that samples from <span class="math inline">\(q\)</span> can be used to compute expectations under the target model <span class="math inline">\(p\)</span>.</p>
<p>To derive importance sampling, let us look at the expectation</p>
<p><span class="math display">\[
\mathbb E_p[f] = \int_{\mathcal X} f(x)\, p(x)\, dx = \int_{\mathcal X} f(x)\, \frac{p(x)}{q(x)}\, q(x)\, dx = \int_{\mathcal X} f(x)\, w(x)\, q(x)\, dx = \mathbb E_q[wf]\, .
\]</span></p>
<p>So expectations with respect to <span class="math inline">\(p\)</span> can be expressed as expectations with respect to <span class="math inline">\(q\)</span>, if samples <span class="math inline">\(x\)</span> are weighted with <span class="math inline">\(w(x) = \frac{p(x)}{q(x)}\)</span>. The only requirement is</p>
<p><span class="math display">\[
q(x) = 0\,\,\, \Rightarrow\,\,\, f(x)p(x) = 0
\]</span></p>
<p>which is easier to satisfy than the requirements for rejection sampling.</p>
<section id="algorithm-importance-sampling" class="level3">
<h3 class="anchored" data-anchor-id="algorithm-importance-sampling">Algorithm: Importance sampling</h3>
<p>The algorithm produces random samples <span class="math inline">\(x^{(s)}\)</span> and (importance) weights <span class="math inline">\(w^{(s)}\)</span>:</p>
<ol type="1">
<li><p>Sample <span class="math inline">\(x^{(s)} \sim q(x)\)</span> for <span class="math inline">\(s=1,\ldots, S\)</span>.</p></li>
<li><p>Compute weights <span class="math inline">\(w^{(s)} = \frac{p(x^{(s)})}{q(x^{(s)})}\)</span></p></li>
</ol>
<p>Expectation values are then approximated by</p>
<p><span id="eq-IS"><span class="math display">\[
    \mathbb E_p[f] \approx \hat f_{\text{IS}} := \frac{1}{S} \sum_{s=1}^S w^{(s)} \, f(x^{(s)})
\tag{25}\]</span></span></p>
<p>The right hand side can be interpreted as the expectation of <span class="math inline">\(f\)</span> under the approximate density</p>
<p><span id="eq-is-approximation"><span class="math display">\[
    \hat p_S(x) = \frac{1}{S} \sum_{s=1}^S w^{(s)} \delta\bigl(x - x^{(s)}\bigr)\, .
\tag{26}\]</span></span></p>
<p>This is a generalization of the approximate density introduced earlier in Eq. (<a href="#eq-approximate_pdf">Equation&nbsp;5</a>).</p>
<p>Let us again use a Gaussian target and a Cauchy proposal to illustrate the sampling algorithm:</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_cauchy(S, lower<span class="op">=-</span><span class="dv">10</span>, upper<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    u <span class="op">=</span> np.random.random(<span class="dv">2</span><span class="op">*</span><span class="bu">int</span>(S))</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="op">-</span>np.tan(np.pi <span class="op">*</span> u)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> (x <span class="op">&gt;=</span> lower) <span class="op">&amp;</span> (x <span class="op">&lt;=</span> upper)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x[m][:<span class="bu">int</span>(S)]</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pdf_gaussian(x):</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(<span class="op">-</span><span class="fl">0.5</span><span class="op">*</span>x<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi)</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pdf_cauchy(x):</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.pi <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> x<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> <span class="bu">int</span>(<span class="fl">5e3</span>)</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>, <span class="dv">1000</span>) <span class="op">*</span> <span class="dv">5</span></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> pdf_gaussian(t)</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> pdf_cauchy(t)</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> sample_cauchy(S, <span class="op">-</span><span class="dv">20</span>, <span class="dv">20</span>)</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> pdf_gaussian(x) <span class="op">/</span> pdf_cauchy(x)</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>kw <span class="op">=</span> <span class="bu">dict</span>(xlim <span class="op">=</span> (<span class="op">-</span><span class="dv">8</span>, <span class="dv">8</span>), xlabel <span class="op">=</span> <span class="vs">r'$x$'</span>)</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>kw_hist <span class="op">=</span> <span class="bu">dict</span>(bins<span class="op">=</span><span class="dv">100</span>, density<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>), sharex<span class="op">=</span><span class="va">True</span>, subplot_kw<span class="op">=</span>kw)</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="vs">r'weight $w(x) = p(x) / q(x)$'</span>)</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(t, p <span class="op">/</span> q, color<span class="op">=</span><span class="st">'k'</span>, lw<span class="op">=</span><span class="dv">3</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="vs">r'$w(x)$'</span>)</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'without importance weights'</span>)</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].hist(x, <span class="op">**</span>kw_hist)</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_title(<span class="st">'with importance weights'</span>)</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].hist(x, weights<span class="op">=</span>w<span class="op">/</span>S, <span class="op">**</span>kw_hist)</span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a <span class="kw">in</span> ax[<span class="dv">1</span>:]:</span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a>    a.plot(t, p, color<span class="op">=</span><span class="st">'r'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="vs">r'target $p$'</span>)</span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a>    a.legend()</span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-30-output-1.png" width="1142" height="373"></p>
</div>
</div>
</section>
<section id="properties-of-importance-sampling" class="level3">
<h3 class="anchored" data-anchor-id="properties-of-importance-sampling">Properties of importance sampling</h3>
<p>The importance sampling estimator (Eq. <a href="#eq-IS">Equation&nbsp;25</a>) is unbiased: The <span class="math inline">\(S\)</span> samples follow the joint distribution <span class="math inline">\(q_S(x^{(1)}, \ldots, x^{(S)}) = \prod_s q(x^{(s)})\)</span>, and the expectation of the importance sampling estimator is</p>
<p><span class="math display">\[
\mathbb E_{q_S}[\hat f_{\text{IS}}]
= \frac{1}{S} \mathbb E_{q_S}\biggl[\sum_s f(x^{(s)}) p(x^{(s)})/q(x^{(s)})\biggr]
= \frac{1}{S} \sum_s \underbrace{\mathbb E_{q_S}\biggl[f(x^{(s)}) p(x^{(s)})/q(x^{(s)})\biggr]}_{\mathbb E_p[f]}
= \mathbb E_p[f]\, .
\]</span></p>
<p>The law of large numbers guarantees that <span class="math inline">\(\hat f_{\text{IS}} \to \mathbb E_p[f]\)</span> for <span class="math inline">\(S\to\infty\)</span>.</p>
<p>Since the importance sampling (IS) estimator <span class="math inline">\(\hat f_{\text{IS}}\)</span> is simply the Monte Carlo estimator for <span class="math inline">\(w(x)f(x)\)</span> and sampling distribution <span class="math inline">\(q\)</span>, also the variance is readily available from Eq. (<a href="#eq-MCvariance">Equation&nbsp;9</a>):</p>
<p><span class="math display">\[
\text{var}[\hat f_{\text{IS}}] = \frac{1}{S} \text{var}_q[wf] \, .
\]</span></p>
<p>Using <span class="math inline">\(\text{var}[f] = \mathbb E[f^2] - (\mathbb E[f])^2\)</span>, we obtain</p>
<p><span class="math display">\[
\text{var}_q[wf] = \underbrace{\mathbb E_q[(wf)^2]}_{\mathbb E_p[wf^2]} - \bigl(\underbrace{\mathbb E_q[wf]}_{\mathbb E_p[f]}\bigr)^2 = \mathbb E_p[wf^2] - \bigl( \mathbb E_p[f] \bigr)^2
\]</span></p>
<p><span id="eq-ISvariance"><span class="math display">\[
    \text{var}[\hat f_{\text{IS}}] = \frac{1}{S} \biggl( \mathbb E_p[wf^2] - \bigl( \mathbb E_p[f] \bigr)^2 \biggr)\, .
\tag{27}\]</span></span></p>
<p>Like the error of the standard Monte Carlo approximation, the error of the IS estimator shrinks with <span class="math inline">\(1/\sqrt{S}\)</span>. We can also minimize the variance of the IS estimator as a functional of the proposal distribution (subject to the constraint <span class="math inline">\(\int q(x)\, dx = 1\)</span>), which can be achieved by minimizing the Lagrangian <span class="math inline">\(\mathbb E_p[p/q\, f^2] + \lambda (1-\int q(x) dx)\)</span> resulting in the optimal IS proposal:</p>
<p><span class="math display">\[
q_{\text{opt}}(x) \propto |f(x)|\, p(x)\, .
\]</span></p>
<p>This estimator achieves minimum variance <span class="math display">\[
\left(\mathbb E_p[|f|] \right)^2 - \left(\mathbb E_p[f] \right)^2
\]</span></p>
<p>which can approach zero if <span class="math inline">\(f(x) \ge 0\)</span>. However, this result is mostly of theoretical interest.</p>
</section>
<section id="comparison-between-classical-monte-carlo-and-importance-sampling" class="level3">
<h3 class="anchored" data-anchor-id="comparison-between-classical-monte-carlo-and-importance-sampling">Comparison between classical Monte Carlo and importance sampling</h3>
<ul>
<li><p>Importance sampling should be used when we cannot sample efficiently from the target model <span class="math inline">\(p\)</span></p></li>
<li><p>A reason to use importance sampling can be to reduce the variance over the classical Monte Carlo estimator</p></li>
<li><p>Importance sampling can be used if rejection sampling is not applicable (because we cannot find an upper bound for the ratio <span class="math inline">\(p(x)/q(x)\)</span>)</p></li>
</ul>
</section>
<section id="self-normalized-importance-sampling" class="level3">
<h3 class="anchored" data-anchor-id="self-normalized-importance-sampling">Self-normalized importance sampling</h3>
<p>In case of complex, high-dimensional probabilistic models the normalizing constants are often missing:</p>
<p><span class="math display">\[
p(x) = \frac{p^*(x)}{Z_p}, \, q(x) = \frac{q^*(x)}{Z_q}
\]</span></p>
<p>with <span class="math inline">\(Z_p = \int p^*(x)\, dx\)</span> and <span class="math inline">\(Z_q = \int q^*(x)\, dx\)</span>.</p>
<p>In this case the importance weights</p>
<p><span class="math display">\[
w(x) = \frac{p(x)}{q(x)} = \frac{Z_q}{Z_p} \frac{p^*(x)}{q^*(x)}
\]</span></p>
<p>are not readily available, because the ratio of normalizing constants <span class="math inline">\(Z_q/Z_p\)</span> is unknown.</p>
<p>However, we can use importance sampling to estimate this unknown ratio:</p>
<p><span class="math display">\[
\frac{Z_p}{Z_q} = \frac{\int p^*(x)\, dx}{Z_q} = \frac{\int \frac{p^*(x)}{q^*(x)}\, q^*(x)\, dx}{Z_q} = \int \frac{p^*(x)}{q^*(x)} \, q(x)\, dx = \mathbb E_q[p^*/q^*]
\]</span></p>
<p>The IS estimator for the ratio of normalizing constants is</p>
<p><span id="eq-ISratio"><span class="math display">\[
    (\widehat{Z_p/Z_q})_{\text{IS}} = \frac{1}{S} \sum_{s=1}^S \frac{p^*(x^{(s)})}{q^*(x^{(s)})} \, .
\tag{28}\]</span></span></p>
<p>Plugging this estimator into standard IS estimator (Eq. <a href="#eq-IS">Equation&nbsp;25</a>) yields the <em>self-normalized</em> importance sampling (NIS) estimator</p>
<p><span id="eq-ISselfnormalized"><span class="math display">\[
\hat f_{\text{NIS}} = \frac{\sum_{s=1}^S \frac{p^*(x^{(s)})}{q^*(x^{(s)})}\, f(x^{(s)})}{ \sum_{s=1}^S \frac{p^*(x^{(s)})}{q^*(x^{(s)})}} = \frac{\sum_{s=1}^S w^{(s)}\, f(x^{(s)})}{ \sum_{s=1}^S w^{(s)}}
\tag{29}\]</span></span></p>
<p>In contrast to <span class="math inline">\(\hat f_{\text{IS}}\)</span>, the self-normalized IS estimator <span class="math inline">\(\hat f_{\text{NIS}}\)</span> is biased, but strongly consistent, meaning that for <span class="math inline">\(S\to\infty\)</span> the NIS estimator converges to the correct estimate: <span class="math inline">\(\hat f_{\text{NIS}} \to \mathbb E_p[f]\)</span>. The asymptotic variance of the estimator can be approximated by</p>
<p><span id="eq-NISvar"><span class="math display">\[
\text{var}_{\text{as}}[\hat f_{\text{NIS}}] =  \frac{\frac{1}{S} \sum_s [w^{(s)}]^2 \bigl(f(x^{(s)}) - \hat f_{\text{NIS}}\bigr)^2}{\bigl[\frac{1}{S} \sum_s w^{(s)}\bigr]^2}\, .   
\tag{30}\]</span></span></p>
</section>
<section id="effective-sample-size" class="level3">
<h3 class="anchored" data-anchor-id="effective-sample-size">Effective sample size</h3>
<p>The <a href="https://en.wikipedia.org/wiki/Effective_sample_size">effective sample size (ESS)</a> is the number of <em>independent</em> samples <span class="math inline">\(S_{\text{eff}}\)</span> that would result in the same variance as the NIS estimator (Eq. <a href="#eq-NISvar">Equation&nbsp;30</a>). To compute ESS, we match the asymptotic variance of <span class="math inline">\(\hat f_{\text{NIS}}\)</span> with the variance resulting from <span class="math inline">\(S_{\text{eff}}\)</span>:</p>
<p><span class="math display">\[
\frac{1}{S} \text{var}_{\text{as}}[\hat f_{\text{NIS}}] = \frac{\sum_s \bigl[w^{(s)}\bigr]^2 \bigl(f(x^{(s)}) - \hat f_{\text{NIS}} \bigr)^2}{\bigl[\sum_s w^{(s)}\bigr]^2} = \frac{\sigma^2}{S_{\text{eff}}}
\]</span></p>
<p>where <span class="math inline">\(\sigma^2 = \text{var}_p[f]\)</span>. For <span class="math inline">\(f(x^{(s)}) - \hat f_{\text{NIS}} \approx \sigma\)</span>, we obtain:</p>
<p><span id="eq-ESS"><span class="math display">\[
S_{\text{eff}} = \frac{\bigl[\sum_s w^{(s)}\bigr]^2}{\sum_s \bigr[w^{(s)}\bigl]^2}
\tag{31}\]</span></span></p>
<p>The two extreme cases are:</p>
<ol type="1">
<li><p>All importance weights are the same, <span class="math inline">\(w^{(s)} = 1/S\)</span>, in which case <span class="math inline">\(S_{\text{eff}} = S\)</span>.</p></li>
<li><p>All but one weight are zero, <span class="math inline">\(w^{(1)} = 1\)</span> and <span class="math inline">\(w^{(s)} = 0\)</span> for <span class="math inline">\(s\ge 2\)</span>, in which case <span class="math inline">\(S_{\text{eff}} = 1\)</span>.</p></li>
</ol>
<p>ESS can be used as a diagnostic for the performance of importance sampling. The larger ESS, the more reliable are the estimates.</p>
</section>
</section>
<section id="drawbacks-of-importance-and-rejection-sampling" class="level2">
<h2 class="anchored" data-anchor-id="drawbacks-of-importance-and-rejection-sampling">Drawbacks of importance and rejection sampling</h2>
<p>Both rejection and importance sampling in <em>high dimensions</em> <span class="math inline">\(D\)</span> often suffer from various difficulties. High-dimensional probabilities tend to concentrate around a <a href="https://en.wikipedia.org/wiki/Typical_set"><em>typical set</em></a>. This is a general feature of high-dimensional probabilistic models, also known as <a href="https://en.wikipedia.org/wiki/Concentration_of_measure"><em>concentration of measure</em></a>. The most likely sets need <strong>not</strong> be members of the typical set, which can be counter-intuitive. In case of a <span class="math inline">\(D\)</span>-dimensional standard Gaussian <span class="math inline">\(\mathcal N(0, I_D)\)</span> (where <span class="math inline">\(I_D\)</span> is the <span class="math inline">\(D\)</span>-dimensional identity matrix) we have:</p>
<p><span class="math display">\[
\mathbb E[\|x\|^2] = \text{tr}I_D = D\, .
\]</span></p>
<p>This means that most states will have a distance of <span class="math inline">\(\sqrt{D}\)</span> from the origin, whereas the most likely state, <span class="math inline">\(x=0\)</span>, has zero distance. This phenomenon has been described as <a href="https://www.inference.vc/high-dimensional-gaussian-distributions-are-soap-bubble/">“high-dimensional Gaussian are soap bubbles”</a>.</p>
<p>To understand the implications for rejection and importance sampling, let us look at a toy example with Gaussian target and proposal in <span class="math inline">\(D\)</span> dimensions:</p>
<p><span class="math display">\[
p(x) = \mathcal N\bigl(0, I_D\bigr),\,\,\, q(x) = \mathcal N\bigl(0, \sigma^2 I_D\bigr), \,\,\, \sigma\ge 1\, .
\]</span></p>
<p>The ratio of both distributions is</p>
<p><span class="math display">\[
w(x) = \sigma^D \exp\left\{-\frac{\|x\|^2}{2} \bigl(1 - \sigma^{-2}\bigr)  \right\} \le \sigma^D = M\, .
\]</span></p>
<p>An implication for rejection sampling is that the acceptance probability <span class="math inline">\(p(a=1) = \sigma^{-D}\)</span> decays exponentially in <span class="math inline">\(D\)</span>, likewise the time we have to wait to generate a sample that can be accepted increases exponentially in <span class="math inline">\(D\)</span>.</p>
<p>The implications for importance sampling are similarly bad: The average importance weight is</p>
<p><span class="math display">\[
\mathbb E_q[w] = 1
\]</span></p>
<p>independent of <span class="math inline">\(D\)</span>, but the variance</p>
<p><span class="math display">\[
\text{var}[w] = \mathbb E_q[w^2] - \bigl(\mathbb E_q[w]\bigr)^2 = \left(\frac{\sigma^4}{2\sigma^2-1}\right)^{D/2} - 1
\]</span></p>
<p>grows exponentially in <span class="math inline">\(D\)</span>, since</p>
<p><span class="math display">\[
\frac{\sigma^4}{2\sigma^2 -1} \ge 1
\]</span></p>
<p>for <span class="math inline">\(\sigma &gt; 1\)</span>.</p>
<p>(To see the previous inequality: <span class="math inline">\(0 \le (\sigma^2 - 1)^2 = \sigma^4 - 2\sigma^2 + 1 \,\,\Rightarrow\,\, \sigma^4 &gt; 2\sigma^2 - 1\)</span>. If <span class="math inline">\(\sigma&gt;1\)</span>, then <span class="math inline">\(2\sigma^2 - 1 &gt; 0\)</span> and we can divide the last inequality by this factor without changing the direction.)</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> np.linspace(<span class="fl">1.</span>, <span class="fl">5.</span>, <span class="dv">100</span>) </span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>w2 <span class="op">=</span> sigma<span class="op">**</span><span class="dv">4</span> <span class="op">/</span> (<span class="fl">2.</span><span class="op">*</span>sigma<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>ax.plot(sigma, w2, color<span class="op">=</span><span class="st">'k'</span>, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>ax.axhline(<span class="fl">1.</span>, ls<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'r'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="vs">r'$\sigma$'</span>)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r'$E_q[w^2]$'</span>)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="fl">0.</span>, <span class="va">None</span>)</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-31-output-1.png" width="660" height="470"></p>
</div>
</div>
<p>The effective sample size is</p>
<p><span class="math display">\[
ESS = \frac{\bigl(\mathbb E_q[w]\bigr)^2}{\mathbb E_q[w^2]} = \left(\frac{\sigma^4}{2\sigma^2-1}\right)^{-D/2}
\]</span></p>
<p>and decays with <span class="math inline">\(D\)</span>.</p>
<p>To see this more directly, let’s try to characterize the typical set of a Gaussian model <span class="math inline">\(\mathcal N\bigl(0, \sigma^2 I_D\bigr)\)</span>. Since the distribution is spherically symmetric, the distance from the center, <span class="math inline">\(r=\|x\|\)</span>, follows the distribution</p>
<p><span class="math display">\[
p(r) \propto r^{D-1} e^{-r^2/2\sigma^2}\, .
\]</span></p>
<p>This implies that the pdf of the squared distance <span class="math inline">\(r^2\)</span> is a <a href="https://en.wikipedia.org/wiki/Gamma_distribution">Gamma distribution</a> with shape parameter <span class="math inline">\(D/2\)</span> and scale <span class="math inline">\(2\sigma^2\)</span>, therefore</p>
<p><span class="math display">\[
\mathbb E[\|x\|^2] = D\sigma^2, \,\,\, \text{var}[\|x\|^2] = 2\sigma^4 D .
\]</span></p>
<p>The typical set is characterized by states with</p>
<p><span class="math display">\[
\|x\|^2 \approx \sigma^2 \bigl(D \pm \sqrt{2D}\bigr) = \sigma^2 D \bigl(1 \pm \sqrt{2/D}\bigr)\, .
\]</span></p>
<p>These states have an ever increasing distance from the origin, <span class="math inline">\(\sim \sigma\sqrt{D}\)</span>, and concentrate in spherical shells that become thinner and thinner as <span class="math inline">\(D\)</span> increases. It will therefore be increasingly difficult to match the typical sets of the proposal and target pdf. At the same time, weights will fluctuate by factors of <span class="math inline">\(\exp\{\pm\sqrt{2D}\}\)</span>, resulting in only a few dominant states and an effective sample size that drops to one.</p>
<p>If we compare the probability of the maximum probability state <span class="math inline">\(x_{\max} = 0\)</span> with the probability of an element <span class="math inline">\(x_{\text{typical}}\)</span> of the typical set, we get:</p>
<p><span class="math display">\[
\frac{p(x_{\text{typical}})}{p(x_{\max})} \simeq \exp\left\{-\frac{1}{2}(D \pm \sqrt{2D}) \right\}
\]</span></p>
<p>In summary, the two major problems of importance sampling are:</p>
<ul>
<li><p>Finding a good proposal <span class="math inline">\(q\)</span> whose typical set (region of states <span class="math inline">\(x\)</span> that are representative of <span class="math inline">\(q\)</span>) overlaps with the typical set of the target <span class="math inline">\(p\)</span></p></li>
<li><p>Weights are likely to vary by large factors, because the probabilities of points in a typical set, although similar to each other, still differ by factors of order <span class="math inline">\(\exp(\sqrt{D})\)</span>, so the weights will too, unless <span class="math inline">\(q\)</span> is a near-perfect approximation to <span class="math inline">\(p\)</span></p></li>
</ul>
</section>
</section>
<section id="lecture-4-markov-chain-monte-carlo" class="level1">
<h1>Lecture 4: Markov chain Monte Carlo</h1>
<section id="outline-3" class="level2">
<h2 class="anchored" data-anchor-id="outline-3">Outline</h2>
<ul>
<li>Markov chains</li>
</ul>
</section>
<section id="where-do-we-stand" class="level2">
<h2 class="anchored" data-anchor-id="where-do-we-stand">Where do we stand?</h2>
<ul>
<li><p>Monte Carlo approximation: use stochastic simulations to estimate deterministic quantities. Only stochastic and asymptotic guarantees.</p></li>
<li><p>We can design special purpose solutions using variable transformation methods, but these are not broadly applicable. Examples: inversion of cdf, Cartesian to polar coordinates (Box-Muller), affine transformation for general multivariate Gaussians</p></li>
<li><p>We tried to overcome special purpose approaches by using a <em>helper</em> or <em>proposal</em> distribution (rejection and importance sampling)</p></li>
<li><p>Although these approaches are more versatile, we encountered several challenges: Finding a good proposal distribution in the first place. Depending on the method, guarantee that all requirements are met. For example, rejection sampling needs an upper bound on the ratio of target and proposal distribution.</p></li>
<li><p>Major challenges in high dimensions for both rejection and importance sampling</p></li>
</ul>
</section>
<section id="markov-chains" class="level2">
<h2 class="anchored" data-anchor-id="markov-chains">Markov chains</h2>
<p>Up to this point, we have only considered sampling approaches based on identically and independently distributed samples: all <span class="math inline">\(x^{(s)}\)</span> are generated from the same distribution independent of each other, either by drawing from</p>
<p><span class="math display">\[
x^{(s)}\sim p(x)
\]</span></p>
<p>where <span class="math inline">\(p(x)\)</span> is the target distribution, or by drawing from</p>
<p><span class="math display">\[
x^{(s)}\sim q(x)
\]</span></p>
<p>where <span class="math inline">\(q(x)\)</span> is a proposal or helper distribution.</p>
<p>The idea of <a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo"><strong>Markov chain Monte Carlo (MCMC)</strong></a> methods is to give up the <em>independence</em> of successive samples and generate sequences of states where <span class="math inline">\(x^{(s)}\)</span> depends on the previous sample <span class="math inline">\(x^{(s-1)}\)</span>. Our hope is that also when introducing these correlations, the Monte Carlo approximation</p>
<p><span id="eq-MCapproximation"><span class="math display">\[
\frac{1}{S} \sum_{s=1}^S f\bigl(x^{(s)}\bigr) \approx \mathbb E_p[f]
\tag{32}\]</span></span></p>
<p>is still valid. This is justified by our intuition that as long as we run the simulation long enough correlations between two states <span class="math inline">\(x^{(s')}\)</span> and <span class="math inline">\(x^{(s)}\)</span> will vanish with large <span class="math inline">\(|s'-s|\)</span>, and the samples <span class="math inline">\(x^{(s)}\)</span> will approximately follow <span class="math inline">\(p(x)\)</span> for large <span class="math inline">\(s\)</span></p>
<p>In the following, we will restrict ourselves to <em>discrete</em> sample spaces <span class="math inline">\(\mathcal X\)</span> (finite or countably infinite). Markov chains defined on continuous sample spaces can be treated in a similar, yet mathematically much more involved fashion (measure theory, etc.).</p>
<section id="definition-of-markov-chains" class="level3">
<h3 class="anchored" data-anchor-id="definition-of-markov-chains">Definition of Markov chains</h3>
<p>Markov chains are models for dynamical systems with possibly uncertain transitions between various system states. In our context, the state space of the stochastic dynamics is the sample space <span class="math inline">\(\mathcal X\)</span>.</p>
<p>A (first order) <a href="https://en.wikipedia.org/wiki/Markov_chain"><em>Markov chain</em></a> is a <em>memoryless</em> stochastic process <span class="math inline">\(\left(x^{(s)}\right)_{s\ge 0}\)</span> that has the following <a href="https://en.wikipedia.org/wiki/Markov_property">property</a></p>
<p><span id="eq-MarkovChain"><span class="math display">\[
\Pr\bigl(x^{(s+1)} \mid x^{(s)}, \ldots, x^{(1)}\bigr) = \Pr\bigl(x^{(s+1)} \mid x^{(s)}\bigr)
\tag{33}\]</span></span></p>
<p>with <span class="math inline">\(x^{(s)} \in \mathcal X\)</span>. That is, the probability of finding the system in state <span class="math inline">\(x^{(s+1)}\)</span> only depends on the <em>last</em> state <span class="math inline">\(x^{(s)}\)</span>, not on the previous states before the last state. In this sense, Markov chains have no memory. A Markov chain is uniquely characterized by</p>
<ol type="1">
<li><p>the <em>distribution of the initial state</em> <span class="math inline">\(x^{(0)} \sim p^{(0)}\)</span> and</p></li>
<li><p>the <em>transition probabilities</em> <span class="math inline">\(\Pr(y\mid x)\)</span> for all <span class="math inline">\(x, y \in \mathcal X\)</span>.</p></li>
</ol>
<p>Note that we are dealing with <em>time-homogeneous</em> Markov chains whose transition probabilities do not depend on <span class="math inline">\(s\)</span>.</p>
<p><em>Remark:</em> I am here sticking to our convention of using the superscript <span class="math inline">\((\cdot)^{(s)}\)</span> to denote samples, because we will later use Markov chains to generate samples from a probabilistic model. However, at this point we should think of <span class="math inline">\(s\)</span> as a discrete time.</p>
</section>
<section id="graph-representation" class="level3">
<h3 class="anchored" data-anchor-id="graph-representation">Graph representation</h3>
<p>Due to its simple structure, Markov chains can be represented as directed graphs where the nodes of the graph represent the different states in <span class="math inline">\(\mathcal X\)</span> and the edges transition probabilities between nodes <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> that are greater than zero, i.e.&nbsp;if <span class="math inline">\(\Pr(x\mid y) &gt; 0\)</span>, then we introduce an arrow between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>. These graphs are called <em>transition graphs</em>.</p>
<p>For example the two-state Markov chain with transition probabilities <span class="math inline">\(\Pr(x_2\mid x_1) = \alpha\)</span> and <span class="math inline">\(\Pr(x_1\mid x_2)=\beta\)</span> can be represented by the transition graph:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="images/twostate.png" title="Two-state Markov model" class="img-fluid figure-img">
</figure>
<p></p><figcaption class="figure-caption">Two-state model</figcaption><p></p>
</figure>
</div>
</section>
<section id="transition-matrix" class="level3">
<h3 class="anchored" data-anchor-id="transition-matrix">Transition matrix</h3>
<p>Since the transition probability depends only on the last state, we can summarize all probabilities in a <em>transition matrix</em></p>
<p><span id="eq-transition-matrix"><span class="math display">\[
    P(x, y) = \Pr(x \mid y)
\tag{34}\]</span></span></p>
<p>For continuous sample spaces, the transition matrix becomes a transition operator or <a href="https://en.wikipedia.org/wiki/Markov_kernel">Markov kernel</a>. For the above two-state system we have</p>
<p><span id="eq-twostate"><span class="math display">\[
P(x, y) = \begin{pmatrix}
1 - \alpha &amp; \beta \\
\alpha &amp; 1 - \beta \\
\end{pmatrix}
\tag{35}\]</span></span></p>
<p>where <span class="math inline">\(\alpha, \beta \in [0, 1]\)</span>.</p>
<p>In discrete sample spaces, <span class="math inline">\(P(x, y)\)</span> is the probability that we jump from state <span class="math inline">\(y\in\mathcal X\)</span> to state <span class="math inline">\(x\in\mathcal X\)</span>. Because we are dealing with conditional probabilities, we have</p>
<p><span id="eq-transition-matrix2"><span class="math display">\[
\sum_{x\in\mathcal X} P(x, y) = 1, \,\,\, P(x, y) \ge 0\, .
\tag{36}\]</span></span></p>
<p>The first condition can be written in matrix-vector notation</p>
<p><span id="eq-leftstochastic"><span class="math display">\[
\mathbb 1^T\!P = \mathbb 1^T
\tag{37}\]</span></span></p>
<p>where</p>
<p><span id="eq-one"><span class="math display">\[
\mathbb 1 = \begin{pmatrix}
1 \\
\vdots\\
1\\
\end{pmatrix}
\tag{38}\]</span></span></p>
<p>is a column vector whose elements are all one, and <span class="math inline">\(P\)</span> is the transition matrix. Non-negative square matrices that satisfy condition (<a href="#eq-leftstochastic">Equation&nbsp;37</a>) are called <a href="https://en.wikipedia.org/wiki/Stochastic_matrix"><em>(left) stochastic</em></a> matrices. The qualifier “left” stems from the fact that the <em>columns</em> of <span class="math inline">\(P(x, y)\)</span> are probability vectors, so multiplication with <span class="math inline">\(\mathbb 1^T\)</span> from the left produces one for each column.</p>
<section id="left-versus-right-stochastic-matrices" class="level4">
<h4 class="anchored" data-anchor-id="left-versus-right-stochastic-matrices">Left versus right stochastic matrices</h4>
<ul>
<li><p>Beware that there are different conventions for how to define transition matrices. Mathematicians tend to use the convention <span class="math inline">\(P(x, y) = \Pr(y\mid x)\)</span>, whereas Physicists tend to use <span class="math inline">\(P(x, y) = \Pr(x\mid y)\)</span>. In the first case, the row sums are one, whereas in the second case the column sums are one.</p></li>
<li><p>Note that, as a consequence of the previous comment, in the mathematics literature, and probably also the computer science literature, transition matrices of first-order Markov processes are <em>right stochastic</em>. Here, we follow the physics convention of using <em>left stochastic</em> transition matrices to represent the transition probabilities of Markov processes. One reason is that in the left stochastic convention the position of the arguments in <span class="math inline">\(P(x, y)\)</span> directly reflects the dependence in the conditional probability <span class="math inline">\(\Pr(x\mid y)\)</span>. Another reason is that we will later see that some of the <em>right</em> eigenvectors of <span class="math inline">\(P\)</span> (in the left stochastic convention) play a crucial role (stationary distributions). Linear algebra packages typically compute right eigenvectors (so we don’t have to remember to transpose the matrix when we compute eigenvectors…)</p></li>
</ul>
</section>
</section>
<section id="eigenvalues-of-transition-matrices" class="level3">
<h3 class="anchored" data-anchor-id="eigenvalues-of-transition-matrices">Eigenvalues of transition matrices</h3>
<p>A direct consequence of the stochasticity of the transition matrix of a Markov chain is that the absolute value (<a href="https://en.wikipedia.org/wiki/Absolute_value">modulus</a>) of the (complex) eigenvalues <span class="math inline">\(\lambda\)</span> of <span class="math inline">\(P\)</span> are smaller than or equal to one.</p>
<p>This is straightforward to see: Let <span class="math inline">\(u\)</span> be a left eigenvector of <span class="math inline">\(P\)</span> with <span class="math inline">\(u^T\!P = \lambda u^T\)</span>. Therefore,</p>
<p><span class="math display">\[
\lambda u(x) = \sum_{y\in \mathcal X} P(y, x) u(y)\,\,\, \Rightarrow\,\,\, |\lambda|  = \left|\sum_{y\in \mathcal X} P(y, x) \frac{u(y)}{|u(x)|}\right|
\]</span></p>
<p>for all <span class="math inline">\(x\)</span> with <span class="math inline">\(|u(x)|&gt;0\)</span>. If we pick the element <span class="math inline">\(x\)</span> with the largest absolute value, then all ratios <span class="math inline">\(|u(y)/u(x)|\)</span> are smaller than or equal to one. Therefore, by applying the triangle inequality we get</p>
<p><span class="math display">\[
|\lambda| \le \sum_{y\in \mathcal X} |P(y, x)|\, |u(y)|/|u(x)| \le \sum_{y\in\mathcal X} P(y, x) = 1\, .
\]</span></p>
<p>We see that <span class="math inline">\(\mathbb 1\)</span> is a left eigenvector that attains the maximum (absolute) eigenvalue.</p>
<p>Since left and right eigenvectors have the same eigenvalues, the above upper limit is also valid for right eigenvalues (the <a href="https://en.wikipedia.org/wiki/Characteristic_polynomial">characteristic polynomials</a> of <span class="math inline">\(P\)</span> and <span class="math inline">\(P^T\)</span> are the identical).</p>
</section>
<section id="simulation-of-markov-chains" class="level3">
<h3 class="anchored" data-anchor-id="simulation-of-markov-chains">Simulation of Markov chains</h3>
<p>The algorithm for simulating a Markov chain is very simple:</p>
<ol type="1">
<li><p><span class="math inline">\(x^{(0)} \sim p^{(0)}(x)\)</span></p></li>
<li><p><span class="math inline">\(x^{(s+1)} \sim P\bigl(x, x^{(s)}\bigr)\)</span></p></li>
</ol>
<p>In the first step, <span class="math inline">\(p^{(0)}\)</span> is the initial distribution of states, for example a uniform distribution. In the second step, we simply pick the column vector corresponding to state <span class="math inline">\(x^{(s)}\)</span> and draw a random sample from it. This generates a random walk on the graph representing a Markov chain. We have previously discussed how to sample states according to a pmf by using uniformly distributed pseudo-random numbers:</p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pylab <span class="im">as</span> plt</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>plt.rc(<span class="st">'font'</span>, size<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transition_matrix(alpha, beta):</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array([[<span class="dv">1</span><span class="op">-</span>alpha, beta], </span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>                     [alpha, <span class="dv">1</span><span class="op">-</span>beta]])</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_chain(S, alpha<span class="op">=</span><span class="fl">0.5</span>, beta<span class="op">=</span><span class="fl">0.5</span>, x0<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> [x0]</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>    P <span class="op">=</span> transition_matrix(alpha, beta)</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="bu">len</span>(X) <span class="op">&lt;</span> S:</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> P[:,X[<span class="op">-</span><span class="dv">1</span>]]</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>        X.append(np.random.multinomial(<span class="dv">1</span>, p).argmax())</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(X)</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>kw <span class="op">=</span> <span class="bu">dict</span>(yticks<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>], ylim<span class="op">=</span>[<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">1.1</span>], yticklabels<span class="op">=</span>[<span class="st">'$x_1$'</span>, <span class="st">'$x_2$'</span>], xlabel<span class="op">=</span><span class="st">'$s$'</span>)</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">3</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">9</span>), subplot_kw<span class="op">=</span>kw)</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> <span class="bu">list</span>(ax.flat)</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (alpha, beta) <span class="kw">in</span> <span class="bu">enumerate</span>([(<span class="fl">0.5</span>, <span class="fl">0.5</span>), (<span class="fl">0.1</span>, <span class="fl">0.9</span>), (<span class="fl">1.</span>, <span class="fl">1.</span>)]):</span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> sample_chain(S, alpha, beta)</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>    ax[i].set_title(<span class="vs">r'$\alpha=</span><span class="sc">{0:.1f}</span><span class="vs">$, $\beta=</span><span class="sc">{1:.1f}</span><span class="vs">$'</span>.<span class="bu">format</span>(alpha, beta))</span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>    ax[i].plot(X, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, marker<span class="op">=</span><span class="st">'o'</span>)<span class="op">;</span></span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-32-output-1.png" width="921" height="824"></p>
</div>
</div>
</section>
<section id="distribution-propagation" class="level3">
<h3 class="anchored" data-anchor-id="distribution-propagation">Distribution propagation</h3>
<p>If the initial state <span class="math inline">\(x^{(0)}\)</span> follows <span class="math inline">\(p^{(0)}\)</span>, the marginal distribution of the next state <span class="math inline">\(x^{(1)}\)</span> is</p>
<p><span class="math display">\[
p^{(1)}(x) = \sum_{y\in\mathcal X} P(x, y)\, p^{(0)}(y)\, .
\]</span></p>
<p>The marginal distribution of the <span class="math inline">\((s+1)\)</span>-th state in a Markov chain follows the distribution</p>
<p><span id="eq-marginalMC"><span class="math display">\[
p^{(s+1)}(x) = \sum_{y\in\mathcal X} P(x, y)\, p^{(s)}(y)\, .
\tag{39}\]</span></span></p>
<p>Repeating the same argument for <span class="math inline">\(p^{(s)}\)</span>, we have</p>
<p><span id="eq-marginalMC2"><span class="math display">\[
p^{(s+1)}(x) = \sum_{y, z\in\mathcal X} P(x, y)\, P(y, z) \, p^{(s-1)}(z) =
\sum_{z\in\mathcal X} \left(\sum_{y\in\mathcal X} P(x, y)\, P(y, z) \right) \, p^{(s-1)}(z)\, .
\tag{40}\]</span></span></p>
<p>The expression in brackets, <span class="math inline">\(\sum_{y\in\mathcal X} P(x, y)\, P(y, z)\)</span>, is the transition matrix for making two successive transitions. By generalizing the argument, we obtain the <a href="https://en.wikipedia.org/wiki/Chapman%E2%80%93Kolmogorov_equation">Chapman-Kolmogorov equation</a>.</p>
<p>In matrix-vector notation, we have</p>
<p><span class="math display">\[
p^{(s+1)} = Pp^{(s)}
\]</span></p>
<p>where <span class="math inline">\(p^{(s)}\)</span> are now vectors in the <a href="https://en.wikipedia.org/wiki/Simplex#Probability">probability simplex</a> and <span class="math inline">\(P\)</span> is the transition matrix (always assuming the <em>left stochastic</em> convention in the context of this notebook!).</p>
<p>By applying the argument successively, we obtain a representation of the marginal distribution of the <span class="math inline">\(s\)</span>-th state in terms of <a href="https://mathworld.wolfram.com/MatrixPower.html">matrix powers</a> of the transition matrix:</p>
<p><span class="math display">\[
x^{(s)}\sim P^s p^{(0)}
\]</span></p>
<p>The matrix power <span class="math inline">\(P^s\)</span> is the matrix analog of the power of scalar quantities:</p>
<p><span id="eq-matrix_power"><span class="math display">\[
P^s = \underbrace{P \cdot P\cdots P}_{s\text{ terms}}
\tag{41}\]</span></span></p>
<p>where the dot “<span class="math inline">\(\cdot\)</span>” indicates matrix multiplication. It is straightforward to see that if <span class="math inline">\(P\)</span> is stochastic, then <span class="math inline">\(P^s\)</span> for <span class="math inline">\(s\ge 1\)</span> is also stochastic. The matrix power <span class="math inline">\(P^s\)</span> <em>propagates</em> the distribution of states by <span class="math inline">\(s\)</span> time steps.</p>
<p>Multiplication of the transition matrix <span class="math inline">\(P\)</span> from the right advances a distribution <span class="math inline">\(p^{(s)} \to p^{(s+1)}\)</span>, whereas multiplication from the left corresponds to computing the expectation of some function defined on sample space <span class="math inline">\(\mathcal X\)</span>:</p>
<p><span class="math display">\[
\mathbb E_{p^{(s)}}[f] = \sum_{x\in\mathcal X} f(x) p^{(s)}(x) = \sum_{x\in\mathcal X} f(x) \bigl(P p^{(s-1)}\bigr)(x) = \sum_{x,y\in\mathcal X} f(x) P(x, y) p^{(s-1)}(y) = f^T\!Pp^{(s-1)}
\]</span></p>
</section>
<section id="asymptotic-behavior" class="level3">
<h3 class="anchored" data-anchor-id="asymptotic-behavior">Asymptotic behavior</h3>
<p>What happens if we generate a very long Markov chain? To think about this question, let us represent <span class="math inline">\(P\)</span> using its <a href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix">eigendecomposition</a>:</p>
<p><span class="math display">\[
P = U \Lambda U^{-1}
\]</span></p>
<p>where <span class="math inline">\(\Lambda\)</span> is a diagonal matrix with eigenvalues of <span class="math inline">\(P\)</span> on the diagonal, and <span class="math inline">\(U\)</span> is a square matrix whose columns are the right eigenvectors of <span class="math inline">\(P\)</span>: <span class="math inline">\(PU = U\Lambda\)</span>.</p>
<p>The marginal distribution of the <span class="math inline">\(S\)</span>-th state is then characterized by</p>
<p><span class="math display">\[
P^S = (U\Lambda U^{-1}) (U\Lambda U^{-1}) \cdots (U\Lambda U^{-1}) = U\Lambda^S U^{-1}\, .
\]</span></p>
<p>We know that the magnitude of the eigenvalues is smaller than or equal to one, <span class="math inline">\(|\lambda| \le 1\)</span>. Let us write <span class="math inline">\(\lambda = |\lambda| \exp(i\varphi)\)</span> with modulus <span class="math inline">\(|\lambda|\)</span> and phase <span class="math inline">\(\varphi\)</span>, then all eigenvalues whose magnitude is <em>strictly</em> smaller than one, will die out in the long run</p>
<p><span class="math display">\[
\lambda^S\overset{S\to\infty}{\,\,\,\,\,\,\longrightarrow\,\,\, 0}\,\,\, \text{if }  \,\,|\lambda| &lt; 1
\]</span></p>
<p>If we keep on taking powers of <span class="math inline">\(P\)</span>, the resulting matrix will converge to a low-rank matrix.</p>
</section>
<section id="stationary-distribution" class="level3">
<h3 class="anchored" data-anchor-id="stationary-distribution">Stationary distribution</h3>
<p>The states with <span class="math inline">\(|\lambda |=1\)</span> play a crucial role in the long term behavior of the Markov chain. The left stochasticity of <span class="math inline">\(P\)</span> is the requirement that <span class="math inline">\(\mathbb 1\)</span> is a left eigenvector with eigenvalue 1. Since left and right eigenvalues coincide, there is at least one <em>right</em> eigenvector <span class="math inline">\(\pi\)</span> with eigenvalue one:</p>
<p><span id="eq-stationary"><span class="math display">\[
P\pi = \pi
\tag{42}\]</span></span></p>
<p>If <span class="math inline">\(\pi\)</span> is normalized such that <span class="math inline">\(\mathbb 1^T\!\pi=1\)</span>, then <span class="math inline">\(\pi\)</span> is a <strong>stationary</strong> or <strong>invariant</strong> distribution of <span class="math inline">\(P\)</span>.</p>
<p>Thanks to the <a href="https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem">Perron-Frobenius theorem</a> the elements in <span class="math inline">\(\pi\)</span> all have the same sign. We choose the sign to be positive to obtain a valid probability distribution. Moreover, we normalize <span class="math inline">\(\pi\)</span> to one (remember that the standard normalization for eigenvectors is <span class="math inline">\(u^T\!u=1\)</span>).</p>
<p>The stationary distribution is a <em>fixed point</em> of the propagation dynamics generated by <span class="math inline">\(P\)</span>. If <span class="math inline">\(\pi\)</span> is unique, then it is also called the <em>equilibrium distribution</em> in a physical context.</p>
<p>In the simplest case, we have <span class="math inline">\(P=\pi\mathbb 1^T\)</span>. Simulation of this Markov chain boils down to standard Monte Carlo simulation of <span class="math inline">\(\pi\)</span> (i.e.&nbsp;directly drawing samples from <span class="math inline">\(\pi\)</span>):</p>
<p><span class="math display">\[
x^{(s+1)} \sim \Pr\bigl(x \mid x^{(s)}\bigr) = \pi(x)
\]</span></p>
<p>However, a Markov chain can have more than one stationary distribution. For example, if <span class="math inline">\(P=I\)</span> where <span class="math inline">\(I\)</span> is the identity matrix, then any probability distribution over <span class="math inline">\(\mathcal X\)</span> is stationary. A finite Markov chain always has at least one stationary distribution.</p>
<p>We know that at least one stationary distribution exists. What are the requirements that it is unique?</p>
</section>
<section id="irreducible-markov-chains" class="level3">
<h3 class="anchored" data-anchor-id="irreducible-markov-chains">Irreducible Markov chains</h3>
<p>There are multiple equivalent definitions of the <em>irreducibility</em> of a matrix. A matrix is irreducible, if no subspaces exist that are mapped to themselves under the action of the matrix. An intuitive definition for Markov chains is that the graph representing a Markov chain is fully connected. There are no disconnected components in which the Markov chain cycles without ever exiting into another subspace. If a transition matrix <span class="math inline">\(P\)</span> is reducible, then we can find a <a href="https://en.wikipedia.org/wiki/Permutation_matrix">permutation matrix</a> <span class="math inline">\(\Pi\)</span> that transforms the transition matrix into block lower triangular form:</p>
<p><span class="math display">\[
\Pi\, P\, \Pi^T \not= \begin{pmatrix}
    E &amp; 0 \\ F &amp; G \\
\end{pmatrix}
\]</span> where matrices <span class="math inline">\(E\)</span> and <span class="math inline">\(G\)</span> square matrices. A simple example of a reducible two-state transition matrix is (<span class="math inline">\(\alpha=1/2, \beta=0\)</span>):</p>
<p><span class="math display">\[
P = \begin{pmatrix}
\tfrac{1}{2} &amp; 0 \\
\tfrac{1}{2} &amp; 1 \\
\end{pmatrix}
\]</span></p>
<p>We have <span class="math inline">\(\Pr(x=x_2\mid{}y=x_2) = 1\)</span>, so the subspace <span class="math inline">\(\{x_2\}\)</span> is mapped to itself.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="images/twostate_reducible.png" title="Reducible two-state Markov model" class="img-fluid figure-img">
</figure>
<p></p><figcaption class="figure-caption">Reducible two-state model</figcaption><p></p>
</figure>
</div>
<p>This is also an example of an <a href="https://en.wikipedia.org/wiki/Absorbing_Markov_chain"><em>absorbing Markov chain</em></a>, since even if we start in <span class="math inline">\(x=x_1\)</span>, as soon as we enter the second state, we can never escape from that state again.</p>
<p>An alternative definition of an irreducible Markov chain goes as follows: For all pairs <span class="math inline">\(x, y \in \mathcal X\)</span> there exists <span class="math inline">\(s(x,y)\in \mathbb N\)</span> such that</p>
<p><span id="eq-irreducible"><span class="math display">\[
\Pr\bigl(x^{(s)}=x\mid x^{(0)} = y\bigr) = (P^s)(x, y) &gt; 0
\tag{43}\]</span></span></p>
<p>note that <span class="math inline">\(s(x,y)\)</span> is in general different for every pair of states <span class="math inline">\(x, y \in\mathcal X\)</span>. The intuition behind this notion of irreducibility is that “all states can be reached from all other states”. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.24.9739&amp;rep=rep1&amp;type=pdf">Häggström</a> uses the following terminology: If two states satisfy the irreducibility condition (<a href="#eq-irreducible">Equation&nbsp;43</a>), then <span class="math inline">\(x\)</span> is said to <em>communicate</em> with <span class="math inline">\(y\)</span>, i.e.&nbsp;<span class="math inline">\(y\)</span> can be reached from <span class="math inline">\(x\)</span> in a finite time, which is symbolized by <span class="math inline">\(x\to y\)</span>. Two states <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> <em>intercommunicate</em>, if <span class="math inline">\(x\to y\)</span> and <span class="math inline">\(y\to x\)</span>, which is denoted by <span class="math inline">\(x\leftrightarrow y\)</span> (so there exists a path from <span class="math inline">\(x\)</span> to <span class="math inline">\(y\)</span> with non-vanishing probability, and likewise a path from <span class="math inline">\(y\)</span> back to <span class="math inline">\(x\)</span>). Using this terminology, a Markov chain is irreducible, if <span class="math inline">\(x \leftrightarrow y\)</span> for all <span class="math inline">\(x, y \in \mathcal X\)</span>. This gives us also a hint for verifying irreducibility by checking if the <em>transition graph</em> of a Markov chain is <a href="https://en.wikipedia.org/wiki/Strongly_connected_component">strongly connected</a>.</p>
<p>On the other hand, if a Markov chain is reducible, then the analysis of its long-term behavior can be reduced to the analysis of the long-term behavior of one or more Markov chains with smaller state space.</p>
<p>To illustrate the concept of irreducibility let us come back to the linear congruential generators that we discussed in the context of pseudo random number generation.</p>
<section id="example-lcg-with-bad-magic-numbers" class="level4">
<h4 class="anchored" data-anchor-id="example-lcg-with-bad-magic-numbers">Example: LCG with bad magic numbers</h4>
<p>In lecture 2, we studied linear congruential generators based on the recurrence relation</p>
<p><span id="eq-LCG"><span class="math display">\[
x^{(s+1)} = (a x^{(s)} + c)\, \text{mod}\, m
\tag{44}\]</span></span></p>
<p>with</p>
<ul>
<li><p><strong>modulus</strong> <span class="math inline">\(m &gt; 0\)</span></p></li>
<li><p><strong>multiplier</strong> <span class="math inline">\(a\)</span> where <span class="math inline">\(0 &lt; a &lt; m\)</span></p></li>
<li><p><strong>increment</strong> <span class="math inline">\(c\)</span> where <span class="math inline">\(0 \le c &lt; m\)</span></p></li>
<li><p><strong>seed</strong> <span class="math inline">\(x^{(0)}\)</span> where <span class="math inline">\(0 \le x^{(0)} &lt; m\)</span></p></li>
</ul>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="co">Pseudo random number generator</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pylab <span class="im">as</span> plt</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PRNG:</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""PRNG</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Pseudo-random number generator implemented as iterator.  Using a linear </span></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a><span class="co">    congruential generator (LCG) to generate random numbers. Default settings</span></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a><span class="co">    for modulus, multiplier and period are taken from Numerical Recipes. </span></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Example</span></span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a><span class="co">    -------</span></span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; prng = PRNG(maximum=1e4)</span></span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a><span class="co">    x = list(prng)</span></span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; len(x)</span></span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a><span class="co">    10000</span></span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a><span class="co">    Details:</span></span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a><span class="co">    * https://en.wikipedia.org/wiki/Linear_congruential_generator</span></span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, m<span class="op">=</span><span class="dv">2</span><span class="op">**</span><span class="dv">32</span>, a<span class="op">=</span><span class="dv">1664525</span>, c<span class="op">=</span><span class="dv">1013904223</span>, seed<span class="op">=</span><span class="dv">10</span>, maximum<span class="op">=</span><span class="fl">1e6</span>):</span>
<span id="cb37-26"><a href="#cb37-26" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb37-27"><a href="#cb37-27" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters</span></span>
<span id="cb37-28"><a href="#cb37-28" aria-hidden="true" tabindex="-1"></a><span class="co">        ----------</span></span>
<span id="cb37-29"><a href="#cb37-29" aria-hidden="true" tabindex="-1"></a><span class="co">        m : int &gt; 0</span></span>
<span id="cb37-30"><a href="#cb37-30" aria-hidden="true" tabindex="-1"></a><span class="co">          modulus or period</span></span>
<span id="cb37-31"><a href="#cb37-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-32"><a href="#cb37-32" aria-hidden="true" tabindex="-1"></a><span class="co">        a : int &gt; 0</span></span>
<span id="cb37-33"><a href="#cb37-33" aria-hidden="true" tabindex="-1"></a><span class="co">          multiplier (should be smaller than modulus)</span></span>
<span id="cb37-34"><a href="#cb37-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-35"><a href="#cb37-35" aria-hidden="true" tabindex="-1"></a><span class="co">        c : int &gt;= 0</span></span>
<span id="cb37-36"><a href="#cb37-36" aria-hidden="true" tabindex="-1"></a><span class="co">          increment (should be smaller than modulus)</span></span>
<span id="cb37-37"><a href="#cb37-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-38"><a href="#cb37-38" aria-hidden="true" tabindex="-1"></a><span class="co">        seed : int &gt;= 0</span></span>
<span id="cb37-39"><a href="#cb37-39" aria-hidden="true" tabindex="-1"></a><span class="co">          initial state (should be smaller than modulus)</span></span>
<span id="cb37-40"><a href="#cb37-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-41"><a href="#cb37-41" aria-hidden="true" tabindex="-1"></a><span class="co">        maximum : float or int</span></span>
<span id="cb37-42"><a href="#cb37-42" aria-hidden="true" tabindex="-1"></a><span class="co">          maximum number of random numbers to be generated by PRNG</span></span>
<span id="cb37-43"><a href="#cb37-43" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb37-44"><a href="#cb37-44" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> check_int(i, lower<span class="op">=</span><span class="dv">0</span>, upper<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb37-45"><a href="#cb37-45" aria-hidden="true" tabindex="-1"></a>            valid <span class="op">=</span> <span class="bu">type</span>(i) <span class="kw">is</span> <span class="bu">int</span> <span class="kw">and</span> i <span class="op">&gt;=</span> lower</span>
<span id="cb37-46"><a href="#cb37-46" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> upper <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb37-47"><a href="#cb37-47" aria-hidden="true" tabindex="-1"></a>                valid <span class="op">&amp;=</span> i <span class="op">&lt;</span> upper</span>
<span id="cb37-48"><a href="#cb37-48" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> valid</span>
<span id="cb37-49"><a href="#cb37-49" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb37-50"><a href="#cb37-50" aria-hidden="true" tabindex="-1"></a>        msg <span class="op">=</span> <span class="st">'"</span><span class="sc">{0}</span><span class="st">" must be int &gt;= </span><span class="sc">{1}</span><span class="st">'</span></span>
<span id="cb37-51"><a href="#cb37-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> check_int(m, <span class="dv">1</span>), msg.<span class="bu">format</span>(<span class="st">'m'</span>, <span class="dv">1</span>)</span>
<span id="cb37-52"><a href="#cb37-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> check_int(a, <span class="dv">1</span>, m), msg.<span class="bu">format</span>(<span class="st">'a'</span>, <span class="dv">1</span>)</span>
<span id="cb37-53"><a href="#cb37-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> check_int(c, <span class="dv">0</span>, m), msg.<span class="bu">format</span>(<span class="st">'c'</span>, <span class="dv">0</span>)</span>
<span id="cb37-54"><a href="#cb37-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> check_int(seed, <span class="dv">0</span>, m), msg.<span class="bu">format</span>(<span class="st">'seed'</span>, <span class="dv">0</span>)</span>
<span id="cb37-55"><a href="#cb37-55" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb37-56"><a href="#cb37-56" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.a, <span class="va">self</span>.c, <span class="va">self</span>.m, <span class="va">self</span>.seed <span class="op">=</span> a, c, m, seed</span>
<span id="cb37-57"><a href="#cb37-57" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._max <span class="op">=</span> <span class="bu">int</span>(maximum)</span>
<span id="cb37-58"><a href="#cb37-58" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb37-59"><a href="#cb37-59" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._reset()</span>
<span id="cb37-60"><a href="#cb37-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb37-61"><a href="#cb37-61" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _reset(<span class="va">self</span>):        </span>
<span id="cb37-62"><a href="#cb37-62" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.x <span class="op">=</span> <span class="va">self</span>.seed</span>
<span id="cb37-63"><a href="#cb37-63" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._counter <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb37-64"><a href="#cb37-64" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb37-65"><a href="#cb37-65" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__next__</span>(<span class="va">self</span>):</span>
<span id="cb37-66"><a href="#cb37-66" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb37-67"><a href="#cb37-67" aria-hidden="true" tabindex="-1"></a><span class="co">        Using recurrence relation </span></span>
<span id="cb37-68"><a href="#cb37-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-69"><a href="#cb37-69" aria-hidden="true" tabindex="-1"></a><span class="co">            X_{n+1} = (a X_n + c) mod m</span></span>
<span id="cb37-70"><a href="#cb37-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-71"><a href="#cb37-71" aria-hidden="true" tabindex="-1"></a><span class="co">        to generate new random number</span></span>
<span id="cb37-72"><a href="#cb37-72" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb37-73"><a href="#cb37-73" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>._counter <span class="op">&gt;=</span> <span class="va">self</span>._max:</span>
<span id="cb37-74"><a href="#cb37-74" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">StopIteration</span></span>
<span id="cb37-75"><a href="#cb37-75" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb37-76"><a href="#cb37-76" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.x <span class="op">=</span> (<span class="va">self</span>.a <span class="op">*</span> <span class="va">self</span>.x <span class="op">+</span> <span class="va">self</span>.c) <span class="op">%</span> <span class="va">self</span>.m</span>
<span id="cb37-77"><a href="#cb37-77" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._counter <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb37-78"><a href="#cb37-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-79"><a href="#cb37-79" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.x</span>
<span id="cb37-80"><a href="#cb37-80" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb37-81"><a href="#cb37-81" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__iter__</span>(<span class="va">self</span>):</span>
<span id="cb37-82"><a href="#cb37-82" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._reset()</span>
<span id="cb37-83"><a href="#cb37-83" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb37-84"><a href="#cb37-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-85"><a href="#cb37-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-86"><a href="#cb37-86" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Uniform(PRNG):</span>
<span id="cb37-87"><a href="#cb37-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-88"><a href="#cb37-88" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__next__</span>(<span class="va">self</span>):</span>
<span id="cb37-89"><a href="#cb37-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">super</span>().<span class="fu">__next__</span>() <span class="op">/</span> <span class="bu">float</span>(<span class="va">self</span>.m)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For the choice <span class="math inline">\(a=40, c=0, m=181\)</span> (studied in lecture 2) we obtained a flawed LCG:</p>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>prng <span class="op">=</span> PRNG(m<span class="op">=</span><span class="dv">181</span>, a<span class="op">=</span><span class="dv">40</span>, c<span class="op">=</span><span class="dv">0</span>, maximum<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array(<span class="bu">list</span>(prng))</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.fft.fft(x)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>kw <span class="op">=</span> <span class="bu">dict</span>(s<span class="op">=</span><span class="dv">40</span>, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].scatter(np.arange(<span class="dv">100</span>), x[:<span class="dv">100</span>], <span class="op">**</span>kw)</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(x[:<span class="dv">100</span>], color<span class="op">=</span><span class="st">'k'</span>, lw<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="vs">r'iteration $s$'</span>)</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="vs">r'pseudo random number $x^{(s)}$'</span>)</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].scatter(x[:<span class="dv">100</span>], x[<span class="dv">1</span>:<span class="dv">101</span>], <span class="op">**</span>kw)</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="vs">r'$x^{(s)}$'</span>)</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="vs">r'$x^{(s+1)}$'</span>)</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].plot(np.<span class="bu">abs</span>(np.fft.fftshift(X))[<span class="dv">1</span>:<span class="bu">len</span>(x)<span class="op">//</span><span class="dv">2</span>], </span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>                  lw<span class="op">=</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_xlabel(<span class="st">'spatial frequency'</span>)</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_ylabel(<span class="vs">r'spectrum $|FT|$'</span>)</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-34-output-1.png" width="1125" height="342"></p>
</div>
</div>
<p>We can now analyze this LCG by using Markov chain methods. The recurrence relation (<a href="#eq-LCG">Equation&nbsp;44</a>) defines a Markov chain with deterministic transitions:</p>
<p><span class="math display">\[
P(x, y) = \left\{\begin{array}{c l}
1 \,\,;&amp;x = (a y + c) \, \text{mod}\, m\\
0 \,\,;&amp;\text{else}\\
\end{array}\right.
\]</span></p>
<p>By evaluation of this relation for all pairs of states <span class="math inline">\(x, y \in \{0, 1, \ldots, m-1\}\)</span> we obtain a permutation matrix as transition matrix. (Permutation matrices are special doubly stochastic matrices: they just shuffle states around in a deterministic fashion.)</p>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>prng._reset()</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="co"># construct transition matrix</span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> np.zeros((prng.m, prng.m))</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(prng.m):</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    prng.x <span class="op">=</span> j</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>    P[<span class="bu">next</span>(prng), j] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="co"># P is a permutation matrix, therefore </span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="co"># (see https://en.wikipedia.org/wiki/Permutation_matrix#Properties)</span></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. P has only entries in {0, 1}</span></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.allclose(np.sort(np.unique(P)), [<span class="fl">0.</span>, <span class="fl">1.</span>])</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. P is doubly stochastic</span></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.allclose(P.<span class="bu">sum</span>(<span class="dv">0</span>), <span class="dv">1</span>)</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.allclose(P.<span class="bu">sum</span>(<span class="dv">1</span>), <span class="dv">1</span>)</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. P is orthogonal</span></span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.allclose(P.T <span class="op">@</span> P, np.eye(<span class="bu">len</span>(P)))</span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.allclose(P <span class="op">@</span> P.T, np.eye(<span class="bu">len</span>(P)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Since P is a permutation matrix, all eigenvalues of P </span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="co"># are roots of one</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>v, U <span class="op">=</span> np.linalg.eig(P)</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.<span class="bu">all</span>(np.isclose(np.<span class="bu">abs</span>(v), <span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># There are four stationary distributions</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> np.where(np.isclose(v, <span class="fl">1.</span>))[<span class="dv">0</span>]</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'#eigenvalues close to one:'</span>, <span class="bu">len</span>(indices))</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>U <span class="op">=</span> U[:,indices].real</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>U <span class="op">/=</span> U.<span class="bu">sum</span>(<span class="dv">0</span>)</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>U <span class="op">=</span> U.T</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a><span class="co"># There is one trivially periodic state (x^{(0)}=0 with entropy 0)</span></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a><span class="co"># and there are three equally large subspaces</span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'entropy of stationary distribution:'</span>, </span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>      np.<span class="bu">round</span>([<span class="op">-</span> p <span class="op">@</span> np.log(p<span class="op">+</span><span class="fl">1e-100</span>) <span class="cf">for</span> p <span class="kw">in</span> U], <span class="dv">3</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#eigenvalues close to one: 4
entropy of stationary distribution: [4.094 4.094 4.094 0.   ]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>np.<span class="bu">sum</span>(U[<span class="dv">0</span>]<span class="op">&gt;</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>60</code></pre>
</div>
</div>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [color[<span class="st">'color'</span>] <span class="cf">for</span> color <span class="kw">in</span> plt.rcParams[<span class="st">'axes.prop_cycle'</span>]]</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>orbits <span class="op">=</span> []</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>ticks <span class="op">=</span> (<span class="dv">0</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">150</span>)</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>limits <span class="op">=</span> (<span class="op">-</span><span class="dv">5</span>, prng.m<span class="op">+</span><span class="dv">4</span>)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">5</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="fl">3.5</span>), sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, u <span class="kw">in</span> <span class="bu">enumerate</span>(U):</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>    prng.seed <span class="op">=</span> u.argmax()</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.array(<span class="bu">list</span>(prng))</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>    orbits.append(<span class="bu">set</span>(x.tolist()))</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>].scatter(x[:<span class="op">-</span><span class="dv">1</span>], x[<span class="dv">1</span>:], color<span class="op">=</span>colors[i])</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>    ax[i<span class="op">+</span><span class="dv">1</span>].plot(x[:<span class="op">-</span><span class="dv">1</span>], x[<span class="dv">1</span>:], marker<span class="op">=</span><span class="st">'o'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, lw<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>                 color<span class="op">=</span>colors[i], ls<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xticks(ticks)</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_yticks(ticks)</span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlim(limits)</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylim(limits)</span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-39-output-1.png" width="1402" height="298"></p>
</div>
</div>
</section>
</section>
<section id="aperiodic-markov-chains" class="level3">
<h3 class="anchored" data-anchor-id="aperiodic-markov-chains">Aperiodic Markov chains</h3>
<p>The period of a state <span class="math inline">\(x\in\mathcal X\)</span> is defined as</p>
<p><span id="eq-period"><span class="math display">\[
d(x) = \text{gcd}\left\{s \ge 1 : P^s(x, x) &gt; 0\right\}
\tag{45}\]</span></span></p>
<p>where <span class="math inline">\(\text{gcd}\{a_1, a_2, \ldots\}\)</span> is the <a href="https://en.wikipedia.org/wiki/Greatest_common_divisor">greatest common divisor</a> of the natural numbers <span class="math inline">\(a_1, a_2, \ldots \in \mathbb N\)</span>. The period of a state <span class="math inline">\(x\)</span> is the greatest common divisor of the times that the chain can return (i.e.&nbsp;has positive probability of returning) to <span class="math inline">\(x\)</span>, given that we start in <span class="math inline">\(x\)</span>.</p>
<p>A Markov chain is <em>aperiodic</em> if the periods of all states are one: <span class="math inline">\(d(x) = 1\)</span> for all <span class="math inline">\(x\in\mathcal X\)</span>.</p>
<p>So if a state is periodic, then the return times to <span class="math inline">\(x\)</span> are multiples of a factor greater than one, its period <span class="math inline">\(d(x)\)</span>.</p>
<p>Coming back to our two-state example, if we set <span class="math inline">\(\alpha=\beta=1\)</span> we have:</p>
<p><span class="math display">\[
P(x, y) = \begin{pmatrix}
0 &amp; 1 \\
1 &amp; 0\\
\end{pmatrix}
\]</span></p>
<p>For this Markov chain both states, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, have a period of two, since the state is visited again after an even number of steps.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="images/twostate_periodic.png" title="Periodic two-state Markov model" class="img-fluid figure-img">
</figure>
<p></p><figcaption class="figure-caption">Periodic two-state model</figcaption><p></p>
</figure>
</div>
<p><em>In summary:</em></p>
<ul>
<li><p>If a Markov chain is reducible, then it generates states only in a subspace of <span class="math inline">\(\mathcal X\)</span>; which subspace is selected depends on the initial state.</p></li>
<li><p>If a Markov chain is periodic, then it cycles between multiple stationary distributions.</p></li>
</ul>
<section id="illustration-using-the-two-state-system" class="level4">
<h4 class="anchored" data-anchor-id="illustration-using-the-two-state-system">Illustration using the two-state system</h4>
<p>We have</p>
<p><span id="eq-twostate2"><span class="math display">\[
P(x, y) = \begin{pmatrix}
1 - \alpha &amp; \beta \\
\alpha &amp; 1 - \beta \\
\end{pmatrix}
\tag{46}\]</span></span></p>
<p>with eigenvalues <span class="math inline">\(1\)</span> and <span class="math inline">\(1-\alpha-\beta\)</span> and corresponding (right) eigenvectors:</p>
<p><span id="eq-twostate-decomposition"><span class="math display">\[
\pi = \frac{1}{\alpha+\beta} \begin{pmatrix}
\beta \\ \alpha
\end{pmatrix}, \,\,\,
\text{and}\,\,\,  \begin{pmatrix}
1 \\ -1 \\
\end{pmatrix}
\tag{47}\]</span></span></p>
<p>assuming <span class="math inline">\(\alpha&gt;0\)</span> or <span class="math inline">\(\beta&gt;0\)</span>.</p>
<p>In case <span class="math inline">\(\alpha=\beta=0\)</span>:</p>
<p><span id="eq-twostate3"><span class="math display">\[
P(x, y) = \begin{pmatrix}
1  &amp; 0 \\
0 &amp; 1  \\
\end{pmatrix}
\tag{48}\]</span></span></p>
<p>all two state distributions are stationary with eigenvalue one. In particular, we have eigenvectors</p>
<p><span id="eq-decomposition2"><span class="math display">\[
\begin{pmatrix}
1 \\ 0
\end{pmatrix}, \,\,\,
\begin{pmatrix}
0 \\ 1 \\
\end{pmatrix}
\tag{49}\]</span></span></p>
<p>corresponding to two recurrent classes, and <span class="math inline">\(P\)</span> is reducible.</p>
<p>In case <span class="math inline">\(\alpha=\beta=1\)</span>:</p>
<p><span id="eq-twostate4"><span class="math display">\[
P(x, y) = \begin{pmatrix}
0  &amp; 1 \\
1 &amp; 0  \\
\end{pmatrix}
\tag{50}\]</span></span></p>
<p>the eigenvalues are <span class="math inline">\(1\)</span> and <span class="math inline">\(-1\)</span> with eigenvectors</p>
<p><span class="math display">\[
\begin{pmatrix}
1 \\ 1
\end{pmatrix}, \,\,\,
\begin{pmatrix}
1 \\ -1 \\
\end{pmatrix}
\]</span></p>
<p>and the chain is periodic: <span class="math inline">\(P^{2s} = I\)</span>, <span class="math inline">\(P^{2s+1} = P\)</span>.</p>
</section>
</section>
</section>
</section>
<section id="lecture-5-the-metropolis-hastings-algorithm-1" class="level1">
<h1>Lecture 5: The Metropolis-Hastings algorithm</h1>
<section id="outline-4" class="level2">
<h2 class="anchored" data-anchor-id="outline-4">Outline</h2>
<ul>
<li>Fundamental theorem of Markov chains</li>
<li>Metropolis-Hastings algorithm</li>
</ul>
</section>
<section id="recap-1" class="level2">
<h2 class="anchored" data-anchor-id="recap-1">Recap</h2>
<ul>
<li><p>Idea: improve on direct sampling by allowing for dependence of successive samples (Markov property)</p></li>
<li><p>Markov chains are defined by stochastic matrices; they have at least one stationary distribution</p></li>
</ul>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pylab <span class="im">as</span> plt</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>plt.rc(<span class="st">'font'</span>, size<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="co"># transition matrix as defined in last lecture</span></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transition_matrix(alpha, beta):</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array([[<span class="dv">1</span><span class="op">-</span>alpha, beta], </span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>                     [alpha, <span class="dv">1</span><span class="op">-</span>beta]])</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a><span class="co"># sampling method as defined in last lecture</span></span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_chain(S, alpha<span class="op">=</span><span class="fl">0.5</span>, beta<span class="op">=</span><span class="fl">0.5</span>, x0<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> [x0]</span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>    P <span class="op">=</span> transition_matrix(alpha, beta)</span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="bu">len</span>(X) <span class="op">&lt;</span> S:</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> P[:,X[<span class="op">-</span><span class="dv">1</span>]]</span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>        X.append(np.random.multinomial(<span class="dv">1</span>, p).argmax())</span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Reducible and periodic chains will not converge to a unique stationary distribution</li>
</ul>
<p>So only if the Markov is both <em>irreducible</em> and <em>aperiodic</em>, then we have a unique stationary distribution. This is detailed in the following theorems.</p>
</section>
<section id="fundamental-theorem-of-markov-chains" class="level2">
<h2 class="anchored" data-anchor-id="fundamental-theorem-of-markov-chains">Fundamental Theorem of Markov Chains</h2>
<p>Irreducibility and aperiodicity implies that <span class="math inline">\(\pi\)</span> is unique (see, for example, <a href="https://www.ams.org/journals/bull/2009-46-02/S0273-0979-08-01238-X/">Diaconis: The Markov Chain Monte Carlo Revolution</a>) and powers of <span class="math inline">\(P\)</span> converge to a rank one matrix</p>
<p><span id="eq-fundamental"><span class="math display">\[
P^S(x, y) \to \pi(x)
\tag{51}\]</span></span></p>
<p>for <span class="math inline">\(S\to\infty\)</span> and all <span class="math inline">\(x, y \in\mathcal X\)</span>. Equation (<a href="#eq-fundamental">Equation&nbsp;51</a>) means that we can start from any initial state <span class="math inline">\(y\in\mathcal X\)</span> with <span class="math inline">\(\pi(y) &gt; 0\)</span> and will eventually produce samples from the stationary distribution <span class="math inline">\(\pi\)</span>. Another way to express this <em>convergence in distribution</em> is:</p>
<p><span class="math display">\[
|P^S p - \pi| \to 0
\]</span></p>
<p>for <span class="math inline">\(S\to\infty\)</span> for any <span class="math inline">\(p\)</span>. That is, we can start from an arbitrary initial distribution and converge to the stationary distribution. In matrix-vector notation</p>
<p><span class="math display">\[
P^S \to \pi\mathbb 1^T\, .
\]</span></p>
<p>That is, if <span class="math inline">\(P\)</span> is irreducible and aperiodic, then matrix powers of <span class="math inline">\(P\)</span> converge to a rank-1 matrix.</p>
<p>The fundamental theorem for Markov chains follows from the <a href="https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem">Perron-Frobenius theorem</a> for non-negative matrices and the irreducibility of <span class="math inline">\(P\)</span>.</p>
<p>Why does theorem (<a href="#eq-fundamental">Equation&nbsp;51</a>) have implications for sampling? As we saw in the previous lectures, it might be difficult to sample a probabilistic model directly. Sometimes variable transformations allow us to sample a model directly, but this is only rarely the case for complex models. When resorting to rejection or importance sampling, it is generally difficult to find a good proposal distribution. On the other hand, simulation of a Markov chain is simple to implement (see algorithm above): we just have to move from <span class="math inline">\(x\)</span> to <span class="math inline">\(y\)</span> according to <span class="math inline">\(P(y, x)\)</span>. No matter where we start in sample space, the states that we produce by simulating a Markov chain will eventually follow the stationary distribution.</p>
<p>But there is still something missing in order to use the simulation of a Markov chain for probabilistic inference. In our setting, we are given a probabilistic model <span class="math inline">\(p\)</span> (our target distribution) rather than a transition matrix <span class="math inline">\(P\)</span>. So we are still facing the challenge of designing a suitable Markov chain that has the desired target as its stationary distribution. This problem has been solved in a very ingenious fashion by Metropolis et al., as we will see soon.</p>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># matrix power converges to rank-1 matrix</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_pi(alpha, beta):</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array([beta, alpha]) <span class="op">/</span> (alpha <span class="op">+</span> beta)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>chains <span class="op">=</span> [(<span class="fl">0.1</span>, <span class="fl">0.1</span>), (<span class="fl">0.1</span>, <span class="fl">0.</span>), (<span class="fl">1e-2</span>, <span class="fl">0.</span>), (<span class="fl">0.9</span>, <span class="fl">0.9</span>), (<span class="fl">0.99</span>, <span class="fl">0.99</span>), (<span class="fl">0.1</span>, <span class="fl">0.7</span>)]</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>kw <span class="op">=</span> <span class="bu">dict</span>(ylim<span class="op">=</span>[<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">2.1</span>], ylabel<span class="op">=</span><span class="vs">r'$|P^s-\pi\mathbb</span><span class="sc">{1}</span><span class="vs">^T|$'</span>, xlabel<span class="op">=</span><span class="st">'$s$'</span>)</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>), sharex<span class="op">=</span><span class="st">'all'</span>, sharey<span class="op">=</span><span class="st">'all'</span>, subplot_kw<span class="op">=</span>kw)</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> <span class="bu">list</span>(ax.flat)</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (alpha, beta) <span class="kw">in</span> <span class="bu">enumerate</span>(chains):</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>    P <span class="op">=</span> transition_matrix(alpha, beta)</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>    pi <span class="op">=</span> np.array([beta, alpha]) <span class="op">/</span> (alpha <span class="op">+</span> beta)</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a>    P_inf <span class="op">=</span> np.multiply.outer(pi, np.ones(<span class="dv">2</span>))</span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a>    P_s <span class="op">=</span> P.copy()</span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> []</span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="bu">len</span>(d)  <span class="op">&lt;</span> S:</span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a>        d.append(np.fabs(P_s <span class="op">-</span> P_inf).<span class="bu">sum</span>())</span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a>        P_s <span class="op">=</span> P_s.dot(P)</span>
<span id="cb47-23"><a href="#cb47-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-24"><a href="#cb47-24" aria-hidden="true" tabindex="-1"></a>    ax[i].set_title(<span class="vs">r'$\alpha=</span><span class="sc">{0:.3f}</span><span class="vs">$, $\beta=</span><span class="sc">{1:.3f}</span><span class="vs">$'</span>.<span class="bu">format</span>(alpha, beta),</span>
<span id="cb47-25"><a href="#cb47-25" aria-hidden="true" tabindex="-1"></a>                   fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb47-26"><a href="#cb47-26" aria-hidden="true" tabindex="-1"></a>    ax[i].plot(d, lw<span class="op">=</span><span class="dv">4</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb47-27"><a href="#cb47-27" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-41-output-1.png" width="1110" height="537"></p>
</div>
</div>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># convergence in distribution</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> estimate_pi0(X):</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span><span class="op">-</span>np.add.accumulate(X) <span class="op">/</span> np.add.accumulate(np.ones(<span class="bu">len</span>(X)))</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>kw <span class="op">=</span> <span class="bu">dict</span>(ylim<span class="op">=</span>[<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">1.1</span>], ylabel<span class="op">=</span><span class="vs">r'$p^{(s)}(x=x_1)$'</span>, xlabel<span class="op">=</span><span class="st">'$s$'</span>)</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>), sharex<span class="op">=</span><span class="st">'all'</span>, sharey<span class="op">=</span><span class="st">'all'</span>, subplot_kw<span class="op">=</span>kw)</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> <span class="bu">list</span>(ax.flat)</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (alpha, beta) <span class="kw">in</span> <span class="bu">enumerate</span>(chains):</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> sample_chain(S, alpha, beta, x0<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>    pi <span class="op">=</span> compute_pi(alpha, beta)</span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>    pi_est <span class="op">=</span> estimate_pi0(X)</span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>    ax[i].set_title(<span class="vs">r'$\alpha=</span><span class="sc">{0:.3f}</span><span class="vs">$, $\beta=</span><span class="sc">{1:.3f}</span><span class="vs">$'</span>.<span class="bu">format</span>(alpha, beta),</span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a>                   fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a>    ax[i].plot(pi_est, lw<span class="op">=</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a>    ax[i].axhline(pi[<span class="dv">0</span>], ls<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'r'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-42-output-1.png" width="1108" height="537"></p>
</div>
</div>
<section id="strong-law-of-large-numbers-lln-for-markov-chains" class="level3">
<h3 class="anchored" data-anchor-id="strong-law-of-large-numbers-lln-for-markov-chains">Strong law of large numbers (LLN) for Markov chains</h3>
<p>Before we explain the Metropolis algorithm, let us briefly state the convergence result for irreducible and aperiodic Markov chains in a fashion that is closer to the <em>Monte Carlo approximation</em> introduced in the first lecture.</p>
<p>Irreducibility and aperiodicity of a stochastic transition matrix implies a strong law of large numbers for Markov chains:</p>
<p><span id="eq-lln_markov"><span class="math display">\[
\frac{1}{S} \sum_{s=1}^S f\bigl(x^{(s)}\bigr) \to \mathbb E_{\pi}[f]
\tag{52}\]</span></span></p>
<p>where <span class="math inline">\(x^{(s)} \sim \Pr\bigl(x\mid x^{(s-1)}\bigr) = P\bigl(x, x^{(s-1)}\bigr)\)</span> is an irreducible, aperiodic Markov chain with stationary distribution <span class="math inline">\(\pi\)</span>. Therefore, simulating a Markov chain produces samples that can be used to approximate an expectation similar to the approximation in standard Monte Carlo or importance sampling.</p>
<p>Analogous to the standard Monte Carlo approximation, Markov chain Monte Carlo (MCMC) sampling produces estimates of expectations that coincide with the true expectation in the long run. Again, this is only a stochastic guarantee, and it will be generally difficult to know how close we are to the correct value. But nevertheless we have a guarantee that running a Markov chain longer should help.</p>
</section>
<section id="reversible-markov-chains" class="level3">
<h3 class="anchored" data-anchor-id="reversible-markov-chains">Reversible Markov chains</h3>
<p>A very important concept to verify stationarity of a Markov chain <span class="math inline">\(P\)</span> is <em>reversibility</em>. A Markov chain is <em><span class="math inline">\(\pi\)</span>-reversible</em> if the transition matrix satisfies the <em>detailed balance</em> equations</p>
<p><span id="eq-reversible"><span class="math display">\[
P(x, y)\, \pi(y) = P(y, x)\, \pi(x)
\tag{53}\]</span></span></p>
<p>for all <span class="math inline">\(x, y \in \mathcal X\)</span> for some distribution <span class="math inline">\(\pi\)</span>. From the point of view of probability flow across the transition graph, the detailed balance equations (<a href="#eq-reversible">Equation&nbsp;53</a>) state that “the amount of probability mass flowing from a source state <span class="math inline">\(x\)</span> to a sink <span class="math inline">\(y\)</span> via the directed edge with capacity <span class="math inline">\(P(y, x)\)</span> equals the probability mass flowing backwards.” So the dynamics across the transition graph is in a steady state. As a consequence, reversibility implies that <span class="math inline">\(\pi\)</span> is an invariant distribution:</p>
<p><span id="eq-reversible_invariance"><span class="math display">\[
\sum_{y\in\mathcal X} P(x, y)\pi(y) = \sum_{y\in\mathcal X} P(y, x) \pi(x) = \pi(x)\, .
\tag{54}\]</span></span></p>
<p>Therefore, to verify that a distribution of interest (our target distribution) is the invariant distribution of a Markov chain, we can simply check if the transition matrix satisfies detailed balance with respect to the target distribution.</p>
<p>The contrary to (<a href="#eq-reversible_invariance">Equation&nbsp;54</a>) is <strong>not</strong> true: The fact that <span class="math inline">\(\pi\)</span> is a stationary distribution of the transition matrix <span class="math inline">\(P\)</span> does not imply, that <span class="math inline">\(P\)</span> is <span class="math inline">\(\pi\)</span>-reversible.</p>
<p>If we start a Markov chain in the stationary distribution, <span class="math inline">\(p^{(0)} = \pi\)</span>, then <span id="eq-forward-backward"><span class="math display">\[
\begin{aligned}
    \Pr\bigl(x_S = x^{(S)}, \ldots x_0=x^{(0)}\bigr) &amp;=
    \prod_{s=1}^S P\bigl(x^{(s)}, x^{(s-1)}\bigr)\,  \pi\bigl(x^{(0)}\bigr)\\
    &amp;=
    \prod_{s=1}^S P\bigl(x^{(s-1)}, x^{(s)}\bigr)\, \pi\bigl(x^{(S)}\bigr) \\
    &amp;= \Pr\bigl(x_S = x^{(0)}, \ldots x_0=x^{(S)}\bigr)
\end{aligned}
\tag{55}\]</span></span></p>
<p>The probability of generating a Markov chain when starting in the stationary distribution is the same in forward and backward direction. This is why the Markov chain is called <em>reversible</em>.</p>
<p>If the transition matrix <span class="math inline">\(P\)</span> is symmetric, <span class="math inline">\(P^T=P\)</span>, then the uniform distribution is the stationary distribution because the detailed balance equations are satisfied for <span class="math inline">\(\pi(x) = 1/|\mathcal X|\)</span>.</p>
</section>
</section>
<section id="the-metropolis-hastings-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="the-metropolis-hastings-algorithm">The Metropolis-Hastings Algorithm</h2>
<p>We are now ready to discuss the <a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm"><strong>Metropolis-Hastings algorithm</strong></a> which solves the following fundamental problem:</p>
<blockquote class="blockquote">
<p>For a given target distribution <span class="math inline">\(p(x)\)</span>, how can we construct an irreducible and aperiodic Markov chain such that <span class="math inline">\(p(x)\)</span> is its stationary distribution?</p>
</blockquote>
<p>The Metropolis-Hastings algorithm solves this problem in a very elegant and simple fashion. Due to its simplicity, it is very widely applicable and ranks among the <a href="https://www.andrew.cmu.edu/course/15-355/misc/Top%20Ten%20Algorithms.html">top 10 algorithm of the 20th century</a>.</p>
<section id="algorithm-metropolis-hastings" class="level3">
<h3 class="anchored" data-anchor-id="algorithm-metropolis-hastings">Algorithm: Metropolis-Hastings</h3>
<p>Assume <span class="math inline">\(\mathcal X\)</span> is discrete and <span class="math inline">\(p\)</span> a pmf on <span class="math inline">\(\mathcal X\)</span>. Moreover, <span class="math inline">\(Q(y, x)\)</span> is a proposal Markov chain on <span class="math inline">\(\mathcal X\)</span>, that is <span class="math inline">\(Q(\cdot, x)\)</span> is a pmf on <span class="math inline">\(\mathcal X\)</span> that allows us to generate samples from a given state <span class="math inline">\(x\)</span>. The trick of the Metropolis-Hastings algorithm is to modify the Markov chain <span class="math inline">\(Q\)</span>, by auxiliary coin tossing, to a new transition kernel with stationary distribution <span class="math inline">\(p\)</span>.</p>
<p>The Metropolis-Hastings (MH) algorithm proceeds as follows:</p>
<p>Generate some initial <span class="math inline">\(x^{(0)} \sim p^{(0)}\)</span> and iterate for <span class="math inline">\(s=1, 2, \ldots\)</span>:</p>
<ol type="1">
<li><p>propose a new state by generating <span class="math inline">\(y \sim Q\bigl(\,\cdot\,, x^{(s-1)}\bigr)\)</span></p></li>
<li><p>generate a uniform random number <span class="math inline">\(u \sim \mathcal U(0, 1)\)</span>; if <span class="math inline">\(u \le A\bigl(y, x^{(s-1)}\bigr)\)</span> then <em>accept</em> and set <span class="math inline">\(x^{(s)} = y\)</span>, else <em>reject</em> and set <span class="math inline">\(x^{(s)} = x^{(s-1)}\)</span>. The acceptance probability is given by</p></li>
</ol>
<p><span id="eq-MHaccept"><span class="math display">\[
    A(y, x) = \min\left\{1, \frac{Q(x, y)}{Q(y, x)}\frac{p(y)}{p(x)} \right\}
\tag{56}\]</span></span></p>
<p>The MH algorithm is the first and most important Markov chain Monte Carlo (MCMC) algorithm. Most other MCMC algorithms are specialized versions of MH sampling.</p>
<p><em>Remarks</em></p>
<ol type="1">
<li><p>The distribution <span class="math inline">\(p\)</span> will turn out to be a stationary distribution of the Markov chain that is simulated with the MH algorithm; <span class="math inline">\(p\)</span> is called the <strong>target distribution</strong> or simply the <strong>target</strong></p></li>
<li><p>The Metropolis-Hastings algorithm is also valid in continuous sample spaces <span class="math inline">\(\mathcal X\)</span></p></li>
<li><p>It is crucial to reject, i.e.&nbsp;to really duplicate the current state and store it as a sample in case of a rejected proposal. Otherwise the statistics will be wrong!</p></li>
<li><p>We don’t need to build up and store the full transition matrix <span class="math inline">\(Q\)</span> of the proposal chain in memory. If <span class="math inline">\(Q\)</span> is symmetric, it suffices to be able to <em>simulate</em> <span class="math inline">\(Q\)</span> (this fact is used, for example, in Hamiltonian Monte Carlo where the proposal state is generated by solving a system of differential equations)</p></li>
<li><p>We don’t need to know the normalizing constants of the target distribution and the proposal chain, since the MH algorithm only involves <em>ratios</em> of the target distribution and the transition rates of the proposal chain. If the unnormalized target and proposal chain are denoted</p></li>
</ol>
<p><span id="eq-unnormalized"><span class="math display">\[
    p(x) = \frac{1}{Z_p} p^*(x), \,\,\, Q(x, y) = \frac{1}{Z_Q} Q^*(x, y)
\tag{57}\]</span></span></p>
<p>where <span class="math inline">\(p^*(x) \ge 0\)</span> and <span class="math inline">\(Z_p = \sum_x p^*(x)\)</span> etc., then the <em>acceptance ratio</em> is</p>
<p><span class="math display">\[
\frac{Q(x, y)}{Q(y, x)}\frac{p(y)}{p(x)} =
\frac{Q^*(x, y)}{Q^*(y, x)}\frac{Z_Q}{Z_Q}\, \frac{p^*(y)}{p^*(x)}\frac{Z_p}{Z_p}\, .
\]</span></p>
<p>This is <strong>very</strong> convenient!</p>
</section>
<section id="special-cases" class="level3">
<h3 class="anchored" data-anchor-id="special-cases">Special cases</h3>
<section id="symmetric-proposal-distribution-and-relation-to-statistical-physics" class="level4">
<h4 class="anchored" data-anchor-id="symmetric-proposal-distribution-and-relation-to-statistical-physics">Symmetric proposal distribution (and relation to statistical physics)</h4>
<p>If the proposal distribution is symmetric, <span class="math inline">\(Q^T=Q\)</span>, then the stationary distribution of <span class="math inline">\(Q\)</span> is uniform. This was assumed in the original publication by <a href="https://aip.scitation.org/doi/abs/10.1063/1.1699114">Metropolis <em>et al.</em></a>. In this case the acceptance ratio simplifies to</p>
<p><span class="math display">\[
\frac{Q(x,y)\, p(y)}{Q(y, x)\, p(x)} = \frac{p(y)}{p(x)}
\]</span></p>
<p>Physicists tend to work with energies, i.e.&nbsp;negative log probabilities, rather than probabilities. So the logarithm</p>
<p><span class="math display">\[
\Delta E = \log\{ p(x) / p(y) \}
\]</span></p>
<p>is the <em>energy difference</em> when jumping from state <span class="math inline">\(x\)</span> to the proposed state <span class="math inline">\(y\)</span>. The acceptance probability is then</p>
<p><span class="math display">\[
\min\left\{1, e^{-\Delta E} \right\}
\]</span></p>
<p>If the energy of the new state is lower than the energy of the current state, the proposal is always accepted. Otherwise the acceptance probability depends on the <a href="https://en.wikipedia.org/wiki/Boltzmann_distribution"><em>Boltzmann factor</em></a> <span class="math inline">\(\exp(-\Delta E)\)</span>.</p>
</section>
<section id="independence-sampler" class="level4">
<h4 class="anchored" data-anchor-id="independence-sampler">Independence sampler</h4>
<p>If the proposal distribution is independent of the current state, <span class="math inline">\(Q(y, x) = q(y)\)</span>, the acceptance ratio simplifies to</p>
<p><span class="math display">\[
\frac{Q(x,y)\, p(y)}{Q(y, x)\, p(x)} = \frac{q(x)\, p(y)}{q(y)\, p(x)}
\]</span></p>
<p>If <span class="math inline">\(q(x)\)</span> is the target <span class="math inline">\(q=p\)</span>, then we always accept and we are back to direct sampling <span class="math inline">\(x\sim p\)</span>.</p>
<p>There is also a connection to importance and rejection sampling. The importance weight of some state <span class="math inline">\(x\)</span> is</p>
<p><span class="math display">\[
w(x) = \frac{p(x)}{q(x)}\, ,
\]</span></p>
<p>and the acceptance ratio of the independence sampler involves ratios of importance weights</p>
<p><span class="math display">\[
\frac{w(y)}{w(x)}
\]</span></p>
<p>If the proposal state <span class="math inline">\(y\)</span> has a higher importance weight than the current state <span class="math inline">\(x\)</span>, then we always accept. Otherwise the acceptance probability is <span class="math inline">\(\min\{1, w(y)/w(x)\}\)</span>. So Metropolis sampling with an independent proposal is a kind of hybrid of rejection and importance sampling. Still it has some advantages over rejection sampling, since we do not need to establish an upper bound <span class="math inline">\(M\)</span> such that <span class="math inline">\(p(x) \le Mq(x)\)</span>. In contrast to importance sampling, it has some built-in pruning because states that have a very small importance weight (relative to the current state) have only a small chance of being accepted.</p>
</section>
</section>
<section id="why-does-the-mh-algorithm-work" class="level3">
<h3 class="anchored" data-anchor-id="why-does-the-mh-algorithm-work">Why does the MH algorithm work?</h3>
<p>The MH algorithm works because of the validity of the following statements:</p>
<section id="transition-probabilities-of-the-mh-algorithm" class="level4">
<h4 class="anchored" data-anchor-id="transition-probabilities-of-the-mh-algorithm">Transition probabilities of the MH algorithm</h4>
<p>The MH algorithm generates a Markov chain on <span class="math inline">\(\mathcal X\)</span>. The transition probabilities of the Markov chain are given by</p>
<p><span id="eq-MHtransitions"><span class="math display">\[
P(y, x) = Q(y, x)\, A(y, x) + \delta(y,x)\, r(x)
\tag{58}\]</span></span></p>
<p>where the acceptance probability <span class="math inline">\(A(y, x)\)</span> is defined above, and the rejection probability is</p>
<p><span id="eq-MHrejection"><span class="math display">\[
r(x) = 1 - \sum_{y\in \mathcal X} Q(y, x)\, A(y, x)
\tag{59}\]</span></span></p>
<p>The transition probability of <span class="math inline">\(y\not= x\)</span> is <span class="math inline">\(A(y, x)\, Q(y, x)\)</span> by construction of the algorithm. The term for <span class="math inline">\(y=x\)</span> is obtained by subtracting the sum <span class="math inline">\(\sum_y A(y, x)\, Q(y, x)\)</span> from one, which is just <span class="math inline">\(r(x)\)</span>. In general, it holds that the diagonal entries of the transition matrix are fixed by column stochasticity:</p>
<p><span class="math display">\[
1 = \sum_y P(y, x) \,\,\,\Rightarrow\,\,\, P(x, x) = 1 - \sum_{y\not= x} P(y, x)\, .
\]</span></p>
<p>So it is sufficient to know the off-diagonal elements.</p>
</section>
<section id="stationarity-of-the-target-distribution" class="level4">
<h4 class="anchored" data-anchor-id="stationarity-of-the-target-distribution">Stationarity of the target distribution</h4>
<p>To show that the target distribution <span class="math inline">\(p(x)\)</span> is indeed the stationary distribution of the Markov chain generated by the MH algorithm, we check if the transition matrix (<a href="#eq-MHtransitions">Equation&nbsp;58</a>) is <span class="math inline">\(p\)</span>-reversible. For <span class="math inline">\(y\not= x\)</span> we have:</p>
<p><span id="eq-MHreversible"><span class="math display">\[
\begin{aligned}
    P(y, x)\, p(x)
    &amp;= Q(y, x)\, A(y, x)\, p(x) \\
    &amp;= Q(y, x)\, \min\left\{1, \frac{Q(x, y)}{Q(y, x)}\frac{p(y)}{p(x)} \right\}\, p(x) \\
    &amp;=\min\left\{ Q(y, x)\, p(x), Q(x, y)\, p(y) \right\}\\
    &amp;= Q(x, y)\, \min\left\{\frac{Q(y, x)}{Q(x, y)}\frac{p(x)}{p(y)}, 1 \right\}\, p(y) \\
    &amp;= Q(x, y)\, A(x, y)\, p(y) \\
    &amp;= P(x, y)\, p(y)
\end{aligned}
\tag{60}\]</span></span></p>
<p>The MH chain satisfies the detailed balance equations with regard to our target distribution. Therefore, <span class="math inline">\(p\)</span> is a stationary distribution and will be sampled in the long run.</p>
<p>To ensure that the simulation converges to <span class="math inline">\(p\)</span>, we have to ensure irreducibility by proper choice of the proposal chain <span class="math inline">\(Q(y, x)\)</span>. The proposal chain needs to be irreducible: every point <span class="math inline">\(y \in \mathcal X\)</span> is reachable from any <span class="math inline">\(x \in\mathcal X\)</span> in a finite number of steps.</p>
</section>
</section>
<section id="example-vihola-example-6.19" class="level3">
<h3 class="anchored" data-anchor-id="example-vihola-example-6.19">Example (Vihola, Example 6.19)</h3>
<p>Let’s run the MH algorithm on a simple staircase distribution with uniform proposals. The target distribution is</p>
<p><span class="math display">\[
p(x) = \frac{x}{Z}, \,\,\, x\in\{1, \ldots, m\}=:\mathcal X, \,\,\, Z=\sum_{x=1}^m x = m(m+1)/2
\]</span></p>
<p>To design an MH algorithm for simulating <span class="math inline">\(p\)</span>, we have to choose a suitable proposal distribution. A simple choice is to use a uniform distribution over <span class="math inline">\(\mathcal X\)</span>. That is, <span class="math inline">\(Q(y, x)\)</span> is independent of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>: <span class="math inline">\(Q(y, x) = 1/m\)</span> for all <span class="math inline">\(x, y \in\mathcal X\)</span>.</p>
<p>The resulting MH chain is irreducible:</p>
<p><span class="math display">\[
\Pr(x_1=y\mid{} x_0=x) = Q(y, x) \min\left\{1, \frac{Q(x, y)}{Q(y, x)}\frac{p(y)}{p(x)} \right\} = \frac{1}{m} \min\left\{1, \frac{y}{x}\right\} &gt; 0
\]</span></p>
<p>for all <span class="math inline">\(x, y\in\mathcal X\)</span>. We can get from from any <span class="math inline">\(x\in \mathcal X\)</span> to any <span class="math inline">\(y\)</span> within one step (<span class="math inline">\(P(y,x) &gt; 0\)</span> for all <span class="math inline">\(x, y\)</span>).</p>
<p>The MH algorithm for this special case is very simple:</p>
<ol type="1">
<li><p>Pick <span class="math inline">\(x^{(0)}\)</span> uniformly in <span class="math inline">\(\{1, \ldots, m\}\)</span>, e.g.&nbsp;<span class="math inline">\(x^{(0)}=1\)</span></p></li>
<li><p>Generate proposal <span class="math inline">\(y \sim \mathcal U(\{1, \ldots, m\})\)</span></p></li>
<li><p>Generate <span class="math inline">\(u \sim \mathcal U(0, 1)\)</span> and if <span class="math inline">\(u \le \frac{y}{x^{(s-1)}}\)</span>, set <span class="math inline">\(x^{(s)} = y\)</span>, otherwise <span class="math inline">\(x^{(s)} = x^{(s-1)}\)</span></p></li>
</ol>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example 6.19 from Vihola's lecture notes</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="co"># size of sample space</span></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a><span class="co"># sample space</span></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.arange(m)</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a><span class="co"># target distribution</span></span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> X <span class="op">+</span> <span class="fl">1.</span></span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>p <span class="op">*=</span> <span class="dv">2</span> <span class="op">/</span> m <span class="op">/</span> (m<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a><span class="co"># uniform proposal</span></span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> <span class="kw">lambda</span> x<span class="op">=</span><span class="va">None</span>: np.random.choice(X)</span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Q()</span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> [x]</span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="bu">len</span>(samples) <span class="op">&lt;</span> <span class="fl">1e5</span>:</span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-21"><a href="#cb49-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># proposal step</span></span>
<span id="cb49-22"><a href="#cb49-22" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> Q(x)</span>
<span id="cb49-23"><a href="#cb49-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-24"><a href="#cb49-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># acceptance probability</span></span>
<span id="cb49-25"><a href="#cb49-25" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> p[y] <span class="op">/</span> p[x]</span>
<span id="cb49-26"><a href="#cb49-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-27"><a href="#cb49-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># accept / reject?</span></span>
<span id="cb49-28"><a href="#cb49-28" aria-hidden="true" tabindex="-1"></a>    u <span class="op">=</span> np.random.uniform()</span>
<span id="cb49-29"><a href="#cb49-29" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> y <span class="cf">if</span> (u <span class="op">&lt;=</span> A) <span class="cf">else</span> x</span>
<span id="cb49-30"><a href="#cb49-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-31"><a href="#cb49-31" aria-hidden="true" tabindex="-1"></a>    samples.append(x)</span>
<span id="cb49-32"><a href="#cb49-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-33"><a href="#cb49-33" aria-hidden="true" tabindex="-1"></a>bins, counts <span class="op">=</span> np.unique(samples, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb49-34"><a href="#cb49-34" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> counts <span class="op">/</span> <span class="bu">float</span>(counts.<span class="bu">sum</span>())</span>
<span id="cb49-35"><a href="#cb49-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-36"><a href="#cb49-36" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">4</span>))</span>
<span id="cb49-37"><a href="#cb49-37" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].bar(bins, counts, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb49-38"><a href="#cb49-38" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].step(np.append(<span class="op">-</span><span class="dv">1</span>,X) <span class="op">+</span> <span class="fl">0.5</span>, np.append(<span class="dv">0</span>,p), color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb49-39"><a href="#cb49-39" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(samples[<span class="op">-</span><span class="dv">500</span>:], color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.75</span>)</span>
<span id="cb49-40"><a href="#cb49-40" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-43-output-1.png" width="922" height="346"></p>
</div>
</div>
</section>
<section id="pros-and-cons-of-mcmc" class="level3">
<h3 class="anchored" data-anchor-id="pros-and-cons-of-mcmc">Pros and cons of MCMC</h3>
<p>Pros:</p>
<ul>
<li><p>Very versatile framework: the Metropolis-Hastings algorithm allows us to simulate a Markov chain with a desired stationary distribution in a highly flexible manner. The requirements are much easier to satisfy than the requirements for importance or rejection sampling</p></li>
<li><p>We introduce local correlations which allows the simulation to zoom into the relevant regions of sample space</p></li>
</ul>
<p>Cons:</p>
<ul>
<li><p>We sample locally and pay the price of introducing correlations between successive samples. Local sampling might get stuck and <a href="https://en.wikipedia.org/wiki/Ergodicity#Criterion_for_ergodicity">ergodicity</a> might be hard to achieve.</p></li>
<li><p>We don’t know how far away we are from the stationary distribution and are only given statistical guarantees for convergence in the long run</p></li>
</ul>
</section>
<section id="geometric-interpretation-of-the-metropolis-hastings-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="geometric-interpretation-of-the-metropolis-hastings-algorithm">Geometric interpretation of the Metropolis-Hastings algorithm</h3>
<p>The MH algorithm takes a base chain <span class="math inline">\(Q\)</span>, the proposal chain, that does not yet have the desired target distribution <span class="math inline">\(p\)</span> and tweaks it in such a way that the new chain has the correct distribution. This is achieved by constructing a new chain <span class="math inline">\(P\)</span> that is <span class="math inline">\(p\)</span>-reversible:</p>
<p><span id="eq-p-reversible"><span class="math display">\[
    P(x, y)\, p(y) = P(y, x)\, p(x)
\tag{61}\]</span></span></p>
<p>The mapping from <span class="math inline">\(Q\)</span> to <span class="math inline">\(P\)</span> involves the acceptance ratio</p>
<p><span id="eq-acceptance-ratio"><span class="math display">\[
    R(y, x) = \frac{Q(x, y)\, p(y)}{Q(y, x)\, p(x)}
\tag{62}\]</span></span></p>
<p>and is defined as</p>
<p><span id="eq-metropolized"><span class="math display">\[
P(y, x) = \left\{
\begin{array}{c c}
Q(y, x)\, \min\{1, R(y, x)\} &amp; \text{ if } y\not= x \\
\sum_z Q(z, x) \bigl(1 - \min\{1, R(z, x)\}\bigr) &amp; \text{ if } y=x\\
\end{array}\right.
\tag{63}\]</span></span></p>
<p>If <span class="math inline">\(Q\)</span> is irreducible, then <span class="math inline">\(P\)</span> is also irreducible.</p>
<p>The acceptance ratio <span class="math inline">\(R(y, x)\)</span> (Eq. <a href="#eq-acceptance-ratio">Equation&nbsp;62</a>) assesses how unbalanced the proposal chain is, i.e.&nbsp;how strongly <span class="math inline">\(Q\)</span> deviates from <span class="math inline">\(p\)</span>-reversibility. If <span class="math inline">\(Q\)</span> were already <span class="math inline">\(p\)</span>-reversible, then the ratio <span class="math inline">\(R\)</span> would always be one, and the proposal would always be accepted. The larger <span class="math inline">\(R(y, x)\)</span> deviates from one, the more unbalanced is the proposal chain with regard to the target. Since <span class="math inline">\(R(x, y) = 1 / R(y, x)\)</span>, a strong flux of probability in one direction, results in a reduced flux of probability in the backwards direction.</p>
<p>To better understand how the mapping from some irreducible proposal chain <span class="math inline">\(Q\)</span> to a <span class="math inline">\(p\)</span>-reversible Metropolis chain <span class="math inline">\(P\)</span> works, let me try to explain a very nice paper by <a href="https://projecteuclid.org/euclid.ss/1015346318">Billera &amp; Diaconis: A Geometric Interpretation of the Metropolis-Hastings Algorithm</a>. This paper sets out to provide a global view on why the MH algorithm in some sense provides the optimal way of turning some arbitrary Markov chain <span class="math inline">\(Q\)</span> into a Markov chain with the desired stationary distribution.</p>
<p>First let us think of the space of all possible Markov chains indexed by states from the finite sample space <span class="math inline">\(\mathcal X\)</span>. This space is formed by left stochastic square matrices of size <span class="math inline">\(|\mathcal X|\)</span> and will be called <span class="math inline">\(\mathcal S(\mathcal X)\)</span>. <span class="math inline">\(\mathcal S(\mathcal X)\)</span> is convex, because the convex combination of two Markov matrices is again a stochastic matrix. The dimension of <span class="math inline">\(\mathcal S(\mathcal X)\)</span> is <span class="math inline">\(|\mathcal X|(|\mathcal X| -1 )\)</span>: there are <span class="math inline">\(|X|^2\)</span> non-negative entries in total from which we need to subtract <span class="math inline">\(|X|\)</span> diagonal entries that are fixed by column stochasticity (Eq. <a href="#eq-leftstochastic">Equation&nbsp;37</a>).</p>
<p>For a fixed target distribution <span class="math inline">\(p\)</span>, the subset <span class="math inline">\(\mathcal R(p)\)</span> of all Markov matrices that are <span class="math inline">\(p\)</span>-reversible <span id="eq-p-reversible-chains"><span class="math display">\[
\mathcal R(p) = \left\{ P \in \mathcal S(\mathcal X): P(x, y)\, p(y) = P(y, x)\, p(x) \right\}
\tag{64}\]</span></span> has dimension <span class="math inline">\(|\mathcal X|(|\mathcal X| - 1) / 2\)</span>, because <span class="math inline">\(p\)</span>-reversibility (Eq. <a href="#eq-p-reversible">Equation&nbsp;61</a>) fixes a triangular portion of the transition matrix</p>
<p><span class="math display">\[
P(y, x) = P(x, y) \frac{p(y)}{p(x)}
\]</span></p>
<p>We can either choose <span class="math inline">\(P(x, y)\)</span> upon which <span class="math inline">\(P(y, x)\)</span> is fixed, or vice versa. <span class="math inline">\(\mathcal R(p)\)</span> is a convex subspace of <span class="math inline">\(\mathcal S\)</span>: If <span class="math inline">\(P, P' \in \mathcal R(p)\)</span>, then <span class="math inline">\(\lambda P + (1-\lambda) P' \in \mathcal R(p)\)</span> for <span class="math inline">\(\lambda\in[0,1]\)</span>.</p>
<p>To get a visual impression, let us display the relevant matrix spaces for sample spaces with only two states (Eq. <a href="#eq-twostate">Equation&nbsp;35</a>). Due to the stochasticity constraints, 2-state Markov chains can be represented by points in a two-dimensional unit square. The axes of this space are spanned by <span class="math inline">\(\alpha = \Pr(x_2\mid{} x_1)\)</span> and <span class="math inline">\(\beta=\Pr(x_1\mid{} x_2)\)</span>. The <span class="math inline">\(p\)</span>-reversible chains form a one-dimensional subspace</p>
<p><span class="math display">\[
\mathcal R(p) = \left\{(\alpha, \beta) \in [0,1]^2 : \beta = \frac{p(x_1)}{p(x_2)}\,\alpha  \right\}
\]</span></p>
<p>a straight line segment through the origin with slope <span class="math inline">\(p(x_1)/p(x_2)\)</span>.</p>
<p>The following figure shows <span class="math inline">\(\mathcal{R}(p)\)</span> for <span class="math inline">\(p(x_1) = 0.4\)</span>, <span class="math inline">\(p(x_2) = 1-p(x_1) = 0.6\)</span>.</p>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 2D visualization</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_plot(p0<span class="op">=</span><span class="fl">0.4</span>, limits<span class="op">=</span>(<span class="op">-</span><span class="fl">0.05</span>, <span class="fl">1.05</span>), ax<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>    kw <span class="op">=</span> <span class="bu">dict</span>(xticks<span class="op">=</span>[<span class="fl">0.</span>,<span class="fl">0.5</span>, <span class="fl">1.0</span>], yticks<span class="op">=</span>[<span class="fl">0.</span>,<span class="fl">0.5</span>, <span class="fl">1.0</span>])</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ax <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>        fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">5</span>), subplot_kw<span class="op">=</span>kw)</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>        fig <span class="op">=</span> <span class="va">None</span></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> beta <span class="op">=</span> np.linspace(<span class="fl">0.</span>, <span class="fl">1.</span>, <span class="dv">100</span>)</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>    ax.fill_between(alpha, beta<span class="op">*</span><span class="fl">0.</span>, beta<span class="op">*</span><span class="fl">0.</span><span class="op">+</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>    ax.axvline(<span class="fl">1.</span>, ls<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>    ax.axhline(<span class="fl">1.</span>, ls<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>    ax.axvline(<span class="fl">0.</span>, ls<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>    ax.axhline(<span class="fl">0.</span>, ls<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a>    ax.plot(alpha, alpha<span class="op">*</span>p0<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>p0), lw<span class="op">=</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>    ax.annotate(<span class="vs">r'$\mathcal</span><span class="sc">{S}</span><span class="vs">(\mathcal</span><span class="sc">{X}</span><span class="vs">)$'</span>, (<span class="fl">.2</span>, <span class="fl">.8</span>), xycoords<span class="op">=</span><span class="st">'axes fraction'</span>, fontsize<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a>    ax.annotate(<span class="vs">r'$\mathcal</span><span class="sc">{R}</span><span class="vs">(p)$'</span>, (<span class="fl">.65</span>, <span class="fl">.36</span>), color<span class="op">=</span><span class="st">'r'</span>, xycoords<span class="op">=</span><span class="st">'axes fraction'</span>, fontsize<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>    ax.set_xlim(<span class="op">*</span>limits)</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a>    ax.set_ylim(<span class="op">*</span>limits)</span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="vs">r'$\alpha$'</span>)</span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="vs">r'$\beta$'</span>)</span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> fig:</span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a>        fig.tight_layout()</span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fig, ax</span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a>p0 <span class="op">=</span> <span class="fl">0.4</span></span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> make_plot(p0)</span>
<span id="cb50-31"><a href="#cb50-31" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-44-output-1.png" width="440" height="442"></p>
</div>
</div>
<p>The Metropolis-Hastings algorithm maps an irreducible proposal chain <span class="math inline">\(Q\)</span> to <span class="math inline">\(\mathcal R(p)\)</span></p>
<p><span id="eq-Metropolis-map"><span class="math display">\[
M[Q](y, x) = \min\left\{Q(y, x), \frac{p(y)}{p(x)}\,Q(x, y) \right\}
\tag{65}\]</span></span></p>
<p>for <span class="math inline">\(y\not= x\)</span> (the diagonal entries are fixed by column stochasticity (Eq. <a href="#eq-leftstochastic">Equation&nbsp;37</a>)). The function <span class="math inline">\(M: \mathcal S(\mathcal X) \to \mathcal R(p)\)</span> is called <em>Metropolis map</em>. For a two-state system, the map is simply</p>
<p><span class="math display">\[
\begin{pmatrix}
\alpha \\ \beta
\end{pmatrix} \to
\min\bigl\{\alpha\, p(x_1), \beta\, (1-p(x_2)) \bigr\}
\begin{pmatrix}
1 / p(x_1) \\ 1 / p(x_2)
\end{pmatrix}
\]</span></p>
<p>Examples for the Metropolis map are shown in the following figure:</p>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> M(p0, alpha, beta):</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>    alpha_new <span class="op">=</span> <span class="bu">min</span>(alpha, (<span class="dv">1</span><span class="op">-</span>p0) <span class="op">*</span> beta <span class="op">/</span> p0) </span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>    beta_new <span class="op">=</span> <span class="bu">min</span>(beta, p0 <span class="op">*</span> alpha <span class="op">/</span> (<span class="dv">1</span><span class="op">-</span>p0))</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> alpha_new, beta_new</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> make_plot(p0)</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> Q <span class="kw">in</span> [(<span class="fl">0.1</span>, <span class="fl">0.8</span>), (<span class="fl">0.8</span>, <span class="fl">0.2</span>), (<span class="fl">0.9</span>, <span class="fl">0.5</span>), (<span class="fl">0.9</span>, <span class="fl">0.9</span>), (<span class="fl">0.5</span>, <span class="fl">0.5</span>)]:</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>    P <span class="op">=</span> M(p0, <span class="op">*</span>Q)</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>    ax.plot([Q[<span class="dv">0</span>],P[<span class="dv">0</span>]],[Q[<span class="dv">1</span>],P[<span class="dv">1</span>]], ls<span class="op">=</span><span class="st">'--'</span>, marker<span class="op">=</span><span class="st">'o'</span>, markersize<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>            markeredgecolor<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-45-output-1.png" width="440" height="442"></p>
</div>
</div>
<p>By construction, the off-diagonal entries in the Metropolis chain <span class="math inline">\(M[Q]\)</span> are <em>coordinate-wise decreasing</em>:</p>
<p><span id="eq-coordinatewise-decreasing"><span class="math display">\[
M[Q](y, x) \le Q(y, x)\,\,\,\text{for all}\,\, x, y \in \mathcal X \, .
\tag{66}\]</span></span></p>
<p>In the above figure, <span class="math inline">\(Q\)</span> is either shifted to the left along the <span class="math inline">\(\alpha\)</span> axis, i.e.&nbsp;towards smaller <span class="math inline">\(P(x_2, x_1)\)</span> values until <span class="math inline">\(\mathcal{R}(p)\)</span> is hit, or <span class="math inline">\(Q\)</span> is shifted downwards long the <span class="math inline">\(\beta\)</span> axis.</p>
<p>A suitable metric on <span class="math inline">\(\mathcal S(\mathcal{X})\)</span> is</p>
<p><span id="eq-metric"><span class="math display">\[
d(P, P') = \sum_{x\in\mathcal{X}} \sum_{y\not= x} p(x)\, \left|P(y, x) - P'(y, x)\right|
\tag{67}\]</span></span></p>
<p>which is only zero, if <span class="math inline">\(P'=P\)</span>. The following figure shows “circles” around some <span class="math inline">\(Q\in\mathcal S(\mathcal X)\)</span> which are of course not actual circles because <span class="math inline">\(d(P,P')\)</span> is a weighted L1 norm, so <span class="math inline">\(d\)</span>-circles are diamonds.</p>
<p>For the 2-state system, we have</p>
<p><span class="math display">\[
d(P, P') = p(x_1)\, |\alpha - \alpha'| + p(x_2)\, |\beta - \beta'|
\]</span></p>
<p>where <span class="math inline">\(\alpha, \alpha'\)</span> etc. are the off-diagonal entries of the transition matrices <span class="math inline">\(P, P'\)</span>.</p>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> distance(p, P, Q):</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> np.eye(<span class="bu">len</span>(p))</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">sum</span>(np.fabs(P<span class="op">-</span>Q)<span class="op">*</span>M<span class="op">*</span>p)</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> circle(p0, Q, factor<span class="op">=</span><span class="fl">0.95</span>):</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>    P <span class="op">=</span> M(p0, <span class="op">*</span>Q)</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> transition_matrix(<span class="op">*</span>Q)</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>    B <span class="op">=</span> transition_matrix(<span class="op">*</span>P)</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> np.array([p0, <span class="dv">1</span><span class="op">-</span>p0])                          </span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> distance(p, A, B) <span class="op">*</span> factor</span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> np.linspace(<span class="fl">0.</span>, <span class="fl">1.</span>, <span class="dv">100</span>)</span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a>    beta <span class="op">=</span> (d <span class="op">-</span> np.fabs(alpha<span class="op">-</span>Q[<span class="dv">0</span>]) <span class="op">*</span> p0) <span class="op">/</span> (<span class="dv">1</span><span class="op">-</span>p0)</span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> beta <span class="op">&gt;=</span> <span class="dv">0</span></span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> alpha[mask]</span>
<span id="cb52-19"><a href="#cb52-19" aria-hidden="true" tabindex="-1"></a>    beta <span class="op">=</span> beta[mask]</span>
<span id="cb52-20"><a href="#cb52-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-21"><a href="#cb52-21" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> np.append(alpha, alpha[::<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb52-22"><a href="#cb52-22" aria-hidden="true" tabindex="-1"></a>    beta <span class="op">=</span> np.append(Q[<span class="dv">1</span>]<span class="op">-</span>beta, Q[<span class="dv">1</span>]<span class="op">+</span>beta[::<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb52-23"><a href="#cb52-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-24"><a href="#cb52-24" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> np.logical_and(beta <span class="op">&gt;=</span> <span class="dv">0</span>, beta <span class="op">&lt;=</span> <span class="dv">1</span>)</span>
<span id="cb52-25"><a href="#cb52-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-26"><a href="#cb52-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> d, alpha[mask], beta[mask]</span>
<span id="cb52-27"><a href="#cb52-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-28"><a href="#cb52-28" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb52-29"><a href="#cb52-29" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> <span class="bu">list</span>(ax.flat)</span>
<span id="cb52-30"><a href="#cb52-30" aria-hidden="true" tabindex="-1"></a>counter <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb52-31"><a href="#cb52-31" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> Q <span class="kw">in</span> [(<span class="fl">0.1</span>, <span class="fl">0.8</span>), (<span class="fl">0.8</span>, <span class="fl">0.2</span>), (<span class="fl">0.9</span>, <span class="fl">0.5</span>), (<span class="fl">0.5</span>, <span class="fl">0.5</span>)][:<span class="dv">4</span>]:</span>
<span id="cb52-32"><a href="#cb52-32" aria-hidden="true" tabindex="-1"></a>    make_plot(p0, ax<span class="op">=</span>ax[counter])</span>
<span id="cb52-33"><a href="#cb52-33" aria-hidden="true" tabindex="-1"></a>    P <span class="op">=</span> M(p0, <span class="op">*</span>Q)</span>
<span id="cb52-34"><a href="#cb52-34" aria-hidden="true" tabindex="-1"></a>    R <span class="op">=</span> Q[<span class="dv">1</span>] <span class="op">*</span> p0 <span class="op">/</span> (<span class="dv">1</span><span class="op">-</span>p0) <span class="op">/</span> Q[<span class="dv">0</span>]</span>
<span id="cb52-35"><a href="#cb52-35" aria-hidden="true" tabindex="-1"></a>    ax[counter].set_title(<span class="vs">r'$|\log R| = </span><span class="sc">{0:.1f}</span><span class="vs">$'</span>.<span class="bu">format</span>(np.fabs(np.log(R))))</span>
<span id="cb52-36"><a href="#cb52-36" aria-hidden="true" tabindex="-1"></a>    ax[counter].plot([Q[<span class="dv">0</span>],P[<span class="dv">0</span>]],[Q[<span class="dv">1</span>],P[<span class="dv">1</span>]], ls<span class="op">=</span><span class="st">'--'</span>, marker<span class="op">=</span><span class="st">'o'</span>, markersize<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb52-37"><a href="#cb52-37" aria-hidden="true" tabindex="-1"></a>            markeredgecolor<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb52-38"><a href="#cb52-38" aria-hidden="true" tabindex="-1"></a>    d, alpha, beta <span class="op">=</span> circle(p0, Q, <span class="fl">0.97</span>)</span>
<span id="cb52-39"><a href="#cb52-39" aria-hidden="true" tabindex="-1"></a>    ax[counter].set_title(<span class="vs">r'$d(Q,M[Q]) = </span><span class="sc">{0:.2f}</span><span class="vs">$'</span>.<span class="bu">format</span>(d))</span>
<span id="cb52-40"><a href="#cb52-40" aria-hidden="true" tabindex="-1"></a>    ax[counter].plot(alpha, beta, color<span class="op">=</span><span class="st">'k'</span>, ls<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb52-41"><a href="#cb52-41" aria-hidden="true" tabindex="-1"></a>    counter <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb52-42"><a href="#cb52-42" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-46-output-1.png" width="919" height="920"></p>
</div>
</div>
<p>Billera and Diaconis demonstrate that the Metropolis map minimizes the distance <span class="math inline">\(d(Q,P)\)</span> between the proposal chain <span class="math inline">\(Q\)</span> and all <span class="math inline">\(P\in\mathcal{R}(p)\)</span>. Among all minimizers in the set of <span class="math inline">\(p\)</span>-reversible chains, it picks the unique element that is coordinate-wise decreasing (Eq. <a href="#eq-coordinatewise-decreasing">Equation&nbsp;66</a>). It makes sense to demand that the mapping is coordinate-wise decreasing, because otherwise it will be more difficult to guarantee that the mapped chain is still in <span class="math inline">\(\mathcal S(\mathcal X)\)</span>.</p>
</section>
<section id="variations-of-metropolis-hastings" class="level3">
<h3 class="anchored" data-anchor-id="variations-of-metropolis-hastings">Variations of Metropolis-Hastings</h3>
<p>Given a proposal chain <span class="math inline">\(Q\)</span>, our goal is to change it to a chain with stationary distribution <span class="math inline">\(p(x)\)</span>. The new chain should work as follows:</p>
<ul>
<li><p>Propose <span class="math inline">\(y \sim Q(\cdot, x)\)</span></p></li>
<li><p>Accept or reject <span class="math inline">\(y\)</span> as new <span class="math inline">\(x\)</span> with probability <span class="math inline">\(A(y, x)\in[0,1]\)</span></p></li>
</ul>
<p>The transition probabilities of the new chain will be</p>
<p><span class="math display">\[
Q(y, x)\, A(y, x)\,\,\,\text{for}\,\, y\not= x \, .
\]</span></p>
<p>Again, the diagonal entries a fixed by column stochasticity. To impose stationarity with regard to our target, we demand that the new chain is <span class="math inline">\(p\)</span>-reversible:</p>
<p><span class="math display">\[
Q(y, x)\, A(y, x)\, p(x) = Q(x, y)\, A(x, y)\, p(y)
\]</span></p>
<p>which gives us</p>
<p><span class="math display">\[
A(y, x) = \frac{Q(x, y)}{Q(y, x)}\frac{p(y)}{p(x)}\, A(x, y) = R(y, x)\, A(x, y)\, .
\]</span></p>
<p>Since <span class="math inline">\(A(y, x) \in [0, 1]\)</span> is a probability, it follows that</p>
<p><span class="math display">\[
A(x, y) = \frac{1}{R(y, x)} \, A(y, x) \le 1 \,\,\,\Rightarrow\,\,\,
A(y, x) \le R(y, x)
\]</span></p>
<p>Combined with <span class="math inline">\(A(y, x)\le 1\)</span>, we have</p>
<p><span class="math display">\[
A(y, x) \le \min\left\{1, R(y, x)\right\}
\]</span></p>
<p>That is, the acceptance probability of any propose-accept scheme is smaller than or equal to the acceptance probability of the Metropolis-Hastings algorithm. The MH algorithm maximizes the chance of moving away from <span class="math inline">\(x\)</span>.</p>
<p>Other choices for <span class="math inline">\(A(y, x)\)</span> have been proposed. These are typically of the form <span class="math inline">\(A(y, x) = f(R(y, x))\)</span> where</p>
<p><span class="math display">\[
0 \le f(r) \le \min\left\{1, r \right\},\,\,\, r\ge 0
\]</span></p>
<p>For example, Barker’s algorithm uses a sigmoidal function <span class="math inline">\(f(r) = r / (1 + r)\)</span> such that</p>
<p><span class="math display">\[
A(y, x) = \frac{R(y, x)}{1 + R(y, x)}
\]</span></p>
<p>An early investigation into different acceptance probabilities is: <a href="https://academic.oup.com/biomet/article/60/3/607/217255">Peskun, P. H. (1973). Optimum Monte Carlo sampling using Markov chains. Biometrika, 60:607–612.</a></p>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Barker's acceptance probability</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>R <span class="op">=</span> np.linspace(<span class="fl">0.</span>, <span class="fl">2.</span>, <span class="dv">100</span>)</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>acc_barker <span class="op">=</span> R <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> R)</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>acc_metropolis <span class="op">=</span> np.clip(R, <span class="fl">0.</span>, <span class="fl">1.</span>)</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">7</span>))</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>ax.plot(R, acc_barker, color<span class="op">=</span><span class="st">'r'</span>, label<span class="op">=</span><span class="st">'Barker'</span>, lw<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>ax.plot(R, acc_metropolis, color<span class="op">=</span><span class="st">'b'</span>, label<span class="op">=</span><span class="st">'Metropolis'</span>, lw<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="vs">r'acceptance ratio $R(y, x)$'</span>)</span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r'acceptance probability $A(y, x)$'</span>)</span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-47-output-1.png" width="920" height="632"></p>
</div>
</div>
</section>
<section id="metropolis-hastings-in-continuous-sample-spaces" class="level3">
<h3 class="anchored" data-anchor-id="metropolis-hastings-in-continuous-sample-spaces">Metropolis-Hastings in continuous sample spaces</h3>
<p>The MH algorithm works also for continuous sample space where <span class="math inline">\(p\)</span> is a pdf and <span class="math inline">\(Q(y, x)\)</span> is a Markov kernel. The following example uses a uniform proposal kernel</p>
<p><span class="math display">\[
Q(y, x) = \mathcal U(x - \epsilon, x + \epsilon)
\]</span></p>
<p>with <span class="math inline">\(\epsilon &gt; 0\)</span> being a step size. Since <span class="math inline">\(Q(y, x) = Q(x, y)\)</span>, the acceptance ratio simplifies to <span class="math inline">\(p(y)/p(x)\)</span>. Let us try to sample from the standard Gaussian distribution</p>
<p><span class="math display">\[
p(x) \propto \exp\left\{-\frac{1}{2} x^2 \right\}
\]</span></p>
<div class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sampling a standard Gaussian with a uniform proposal</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MetropolisHastings:</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, x, p, Q, S<span class="op">=</span><span class="fl">1e4</span>):</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a><span class="co">        ----------</span></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a><span class="co">        x : initial state</span></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a><span class="co">        p : target distribution</span></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a><span class="co">        Q : simulator of a symmetric proposal chain</span></span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a><span class="co">        S : number of samples</span></span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.p <span class="op">=</span> p</span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q <span class="op">=</span> Q</span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.S <span class="op">=</span> S</span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._initial <span class="op">=</span> x</span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _reset(<span class="va">self</span>):</span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._counter <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_accepted <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.x <span class="op">=</span> <span class="va">self</span>._initial</span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb54-24"><a href="#cb54-24" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb54-25"><a href="#cb54-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> acceptance_rate(<span class="va">self</span>):</span>
<span id="cb54-26"><a href="#cb54-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.n_accepted <span class="op">/</span> <span class="va">self</span>._counter</span>
<span id="cb54-27"><a href="#cb54-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb54-28"><a href="#cb54-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__next__</span>(<span class="va">self</span>):</span>
<span id="cb54-29"><a href="#cb54-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb54-30"><a href="#cb54-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>._counter <span class="op">&gt;=</span> <span class="va">self</span>.S:</span>
<span id="cb54-31"><a href="#cb54-31" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">StopIteration</span></span>
<span id="cb54-32"><a href="#cb54-32" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb54-33"><a href="#cb54-33" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> <span class="va">self</span>.Q(<span class="va">self</span>.x)</span>
<span id="cb54-34"><a href="#cb54-34" aria-hidden="true" tabindex="-1"></a>        u <span class="op">=</span> np.random.uniform()</span>
<span id="cb54-35"><a href="#cb54-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb54-36"><a href="#cb54-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> u <span class="op">&lt;=</span> <span class="va">self</span>.p(y) <span class="op">/</span> <span class="va">self</span>.p(<span class="va">self</span>.x):</span>
<span id="cb54-37"><a href="#cb54-37" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.x <span class="op">=</span> y</span>
<span id="cb54-38"><a href="#cb54-38" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.n_accepted <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb54-39"><a href="#cb54-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._counter <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb54-40"><a href="#cb54-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb54-41"><a href="#cb54-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.x</span>
<span id="cb54-42"><a href="#cb54-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-43"><a href="#cb54-43" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__iter__</span>(<span class="va">self</span>):</span>
<span id="cb54-44"><a href="#cb54-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._reset()</span>
<span id="cb54-45"><a href="#cb54-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb54-46"><a href="#cb54-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-47"><a href="#cb54-47" aria-hidden="true" tabindex="-1"></a><span class="co"># target</span></span>
<span id="cb54-48"><a href="#cb54-48" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="kw">lambda</span> x: np.exp(<span class="op">-</span>x<span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">2</span>)</span>
<span id="cb54-49"><a href="#cb54-49" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="fl">1.</span>, <span class="dv">1000</span>) <span class="op">*</span> <span class="dv">5</span></span>
<span id="cb54-50"><a href="#cb54-50" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> t, p(t) <span class="op">/</span> np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi)</span>
<span id="cb54-51"><a href="#cb54-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-52"><a href="#cb54-52" aria-hidden="true" tabindex="-1"></a><span class="co"># proposal</span></span>
<span id="cb54-53"><a href="#cb54-53" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> <span class="kw">lambda</span> x, eps<span class="op">=</span><span class="fl">0.1</span>: np.random.uniform(x<span class="op">-</span>eps, x<span class="op">+</span>eps)</span>
<span id="cb54-54"><a href="#cb54-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-55"><a href="#cb54-55" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> <span class="fl">1e4</span></span>
<span id="cb54-56"><a href="#cb54-56" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="fl">10.</span></span>
<span id="cb54-57"><a href="#cb54-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-58"><a href="#cb54-58" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">3</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">9</span>), sharex<span class="op">=</span><span class="st">'col'</span>)</span>
<span id="cb54-59"><a href="#cb54-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-60"><a href="#cb54-60" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, eps <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="fl">1e-1</span>, <span class="fl">1e0</span>, <span class="fl">1e1</span>]):</span>
<span id="cb54-61"><a href="#cb54-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-62"><a href="#cb54-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># run MH simulation</span></span>
<span id="cb54-63"><a href="#cb54-63" aria-hidden="true" tabindex="-1"></a>    mh <span class="op">=</span> MetropolisHastings(x, p, <span class="kw">lambda</span> x: Q(x, eps), S)</span>
<span id="cb54-64"><a href="#cb54-64" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> np.array(<span class="bu">list</span>(mh))</span>
<span id="cb54-65"><a href="#cb54-65" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-66"><a href="#cb54-66" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'acceptance rate: </span><span class="sc">{0:.1%}</span><span class="st"> (stepsize = </span><span class="sc">{1:.2e}</span><span class="st">)'</span>.<span class="bu">format</span>(</span>
<span id="cb54-67"><a href="#cb54-67" aria-hidden="true" tabindex="-1"></a>        mh.acceptance_rate, eps))</span>
<span id="cb54-68"><a href="#cb54-68" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-69"><a href="#cb54-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># plot results</span></span>
<span id="cb54-70"><a href="#cb54-70" aria-hidden="true" tabindex="-1"></a>    ax[i,<span class="dv">0</span>].plot(samples, color<span class="op">=</span><span class="st">'k'</span>, lw<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb54-71"><a href="#cb54-71" aria-hidden="true" tabindex="-1"></a>    ax[i,<span class="dv">1</span>].hist(samples, bins<span class="op">=</span><span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb54-72"><a href="#cb54-72" aria-hidden="true" tabindex="-1"></a>    ax[i,<span class="dv">2</span>].hist(samples[<span class="dv">1000</span>:], bins<span class="op">=</span><span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb54-73"><a href="#cb54-73" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> a <span class="kw">in</span> ax[i,<span class="dv">1</span>:]:</span>
<span id="cb54-74"><a href="#cb54-74" aria-hidden="true" tabindex="-1"></a>        a.plot(<span class="op">*</span>target, color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb54-75"><a href="#cb54-75" aria-hidden="true" tabindex="-1"></a>        a.set_ylim(<span class="fl">0.</span>, <span class="fl">0.45</span>)</span>
<span id="cb54-76"><a href="#cb54-76" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>acceptance rate: 95.7% (stepsize = 1.00e-01)
acceptance rate: 80.2% (stepsize = 1.00e+00)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>acceptance rate: 16.1% (stepsize = 1.00e+01)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-48-output-3.png" width="1114" height="824"></p>
</div>
</div>
</section>
</section>
</section>
<section id="lecture-6-gibbs-sampling-1" class="level1">
<h1>Lecture 6: Gibbs sampling</h1>
<section id="outline-5" class="level2">
<h2 class="anchored" data-anchor-id="outline-5">Outline</h2>
<ul>
<li>Recap: Metropolis-Hastings algorithm</li>
<li>Combining Markov chains</li>
<li>Gibbs sampling</li>
<li>Auxiliary variable methods</li>
</ul>
</section>
<section id="recap-2" class="level2">
<h2 class="anchored" data-anchor-id="recap-2">Recap</h2>
<ul>
<li><p>Markov chain Monte Carlo (MCMC) algorithms simulate a Markov chain to generate samples from a target distribution <span class="math inline">\(p\)</span></p></li>
<li><p>The Metropolis-Hastings (MH) algorithm is a very general scheme to generate a <em>reversible</em> Markov chain whose stationary distribution is a desired target distribution</p></li>
</ul>
<section id="metropolis-hastings-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="metropolis-hastings-algorithm">Metropolis-Hastings Algorithm</h3>
<p>The MH algorithm allows us to map (almost) any Markov chain <span class="math inline">\(Q\)</span> (the <em>proposal chain</em>) with whatever stationary distribution to a Markov chain <span class="math inline">\(M[Q]\)</span> (the <em>Metropolis map</em> of <span class="math inline">\(Q\)</span>) that has the desired stationary distribution, our target distribution <span class="math inline">\(p\)</span>. We have</p>
<p><span class="math display">\[
M[Q](y, x) = \min\left\{Q(y, x), Q(x, y)\frac{p(y)}{p(x)} \right\} \,\,\, \text{for}\,\,\, y\not=x\, .
\]</span></p>
<p>By construction, the Metropolis map is coordinate-wise decreasing, <span class="math inline">\(M[Q](y, x) \le Q(y, x)\)</span>. Among all maps that implement a <em>propose-accept/reject</em> scheme, <span class="math inline">\(M[Q]\)</span> <em>maximizes</em> the probability of moving from a current state to a new state proposed by <span class="math inline">\(Q\)</span>. The Metropolis map projects the proposal chain <span class="math inline">\(Q\)</span> onto the subspace <span class="math inline">\(\mathcal{R}(p)\)</span> of <span class="math inline">\(p\)</span>-reversible Markov chains. The projection minimizes the distance <span class="math inline">\(d(P,P') = \sum_{x\in\mathcal X} \sum_{y\not= x} p(x) |P(y, x) - P'(y, x)|\)</span>, and identifies <span class="math inline">\(M[Q]\in\mathcal{R}(p)\)</span> uniquely by demanding that the map is coordinate-wise decreasing.</p>
<div class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 2D visualization</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pylab <span class="im">as</span> plt</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>plt.rc(<span class="st">'font'</span>, size<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_plot(p0<span class="op">=</span><span class="fl">0.4</span>, limits<span class="op">=</span>(<span class="op">-</span><span class="fl">0.05</span>, <span class="fl">1.05</span>), ax<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>    kw <span class="op">=</span> <span class="bu">dict</span>(xticks<span class="op">=</span>[<span class="fl">0.</span>,<span class="fl">0.5</span>, <span class="fl">1.0</span>], yticks<span class="op">=</span>[<span class="fl">0.</span>,<span class="fl">0.5</span>, <span class="fl">1.0</span>])</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ax <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>        fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">5</span>), subplot_kw<span class="op">=</span>kw)</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>        fig <span class="op">=</span> <span class="va">None</span></span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> beta <span class="op">=</span> np.linspace(<span class="fl">0.</span>, <span class="fl">1.</span>, <span class="dv">100</span>)</span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>    ax.fill_between(alpha, beta<span class="op">*</span><span class="fl">0.</span>, beta<span class="op">*</span><span class="fl">0.</span><span class="op">+</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a>    ax.axvline(<span class="fl">1.</span>, ls<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a>    ax.axhline(<span class="fl">1.</span>, ls<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a>    ax.axvline(<span class="fl">0.</span>, ls<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a>    ax.axhline(<span class="fl">0.</span>, ls<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a>    ax.plot(alpha, alpha<span class="op">*</span>p0<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>p0), lw<span class="op">=</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a>    ax.annotate(<span class="vs">r'$\mathcal</span><span class="sc">{S}</span><span class="vs">(\mathcal</span><span class="sc">{X}</span><span class="vs">)$'</span>, (<span class="fl">.2</span>, <span class="fl">.8</span>), xycoords<span class="op">=</span><span class="st">'axes fraction'</span>, fontsize<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a>    ax.annotate(<span class="vs">r'$\mathcal</span><span class="sc">{R}</span><span class="vs">(p)$'</span>, (<span class="fl">.65</span>, <span class="fl">.36</span>), color<span class="op">=</span><span class="st">'r'</span>, xycoords<span class="op">=</span><span class="st">'axes fraction'</span>, fontsize<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb57-23"><a href="#cb57-23" aria-hidden="true" tabindex="-1"></a>    ax.set_xlim(<span class="op">*</span>limits)</span>
<span id="cb57-24"><a href="#cb57-24" aria-hidden="true" tabindex="-1"></a>    ax.set_ylim(<span class="op">*</span>limits)</span>
<span id="cb57-25"><a href="#cb57-25" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="vs">r'$\alpha$'</span>)</span>
<span id="cb57-26"><a href="#cb57-26" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="vs">r'$\beta$'</span>)</span>
<span id="cb57-27"><a href="#cb57-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> fig:</span>
<span id="cb57-28"><a href="#cb57-28" aria-hidden="true" tabindex="-1"></a>        fig.tight_layout()</span>
<span id="cb57-29"><a href="#cb57-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-30"><a href="#cb57-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fig, ax</span>
<span id="cb57-31"><a href="#cb57-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-32"><a href="#cb57-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-33"><a href="#cb57-33" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> M(p0, alpha, beta):</span>
<span id="cb57-34"><a href="#cb57-34" aria-hidden="true" tabindex="-1"></a>    alpha_new <span class="op">=</span> <span class="bu">min</span>(alpha, (<span class="dv">1</span><span class="op">-</span>p0) <span class="op">*</span> beta <span class="op">/</span> p0) </span>
<span id="cb57-35"><a href="#cb57-35" aria-hidden="true" tabindex="-1"></a>    beta_new <span class="op">=</span> <span class="bu">min</span>(beta, p0 <span class="op">*</span> alpha <span class="op">/</span> (<span class="dv">1</span><span class="op">-</span>p0))</span>
<span id="cb57-36"><a href="#cb57-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> alpha_new, beta_new</span>
<span id="cb57-37"><a href="#cb57-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-38"><a href="#cb57-38" aria-hidden="true" tabindex="-1"></a>p0 <span class="op">=</span> <span class="fl">0.4</span></span>
<span id="cb57-39"><a href="#cb57-39" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> make_plot(p0)</span>
<span id="cb57-40"><a href="#cb57-40" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> Q <span class="kw">in</span> [(<span class="fl">0.1</span>, <span class="fl">0.8</span>), (<span class="fl">0.8</span>, <span class="fl">0.2</span>), (<span class="fl">0.9</span>, <span class="fl">0.5</span>), (<span class="fl">0.9</span>, <span class="fl">0.9</span>), (<span class="fl">0.5</span>, <span class="fl">0.5</span>)]:</span>
<span id="cb57-41"><a href="#cb57-41" aria-hidden="true" tabindex="-1"></a>    P <span class="op">=</span> M(p0, <span class="op">*</span>Q)</span>
<span id="cb57-42"><a href="#cb57-42" aria-hidden="true" tabindex="-1"></a>    ax.plot([Q[<span class="dv">0</span>],P[<span class="dv">0</span>]],[Q[<span class="dv">1</span>],P[<span class="dv">1</span>]], ls<span class="op">=</span><span class="st">'--'</span>, marker<span class="op">=</span><span class="st">'o'</span>, markersize<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb57-43"><a href="#cb57-43" aria-hidden="true" tabindex="-1"></a>            markeredgecolor<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb57-44"><a href="#cb57-44" aria-hidden="true" tabindex="-1"></a>ax.scatter(<span class="dv">1</span><span class="op">-</span>p0, p0, s<span class="op">=</span><span class="dv">120</span>, color<span class="op">=</span><span class="st">'k'</span>, zorder<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb57-45"><a href="#cb57-45" aria-hidden="true" tabindex="-1"></a>ax.annotate(<span class="vs">r'$p\mathbb</span><span class="sc">{1}</span><span class="vs">^T$'</span>, (<span class="dv">1</span><span class="op">-</span>p0<span class="op">-</span><span class="fl">0.05</span>, p0<span class="op">+</span><span class="fl">0.05</span>))</span>
<span id="cb57-46"><a href="#cb57-46" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-49-output-1.png" width="440" height="442"></p>
</div>
</div>
</section>
</section>
<section id="algorithmic-parameters" class="level2">
<h2 class="anchored" data-anchor-id="algorithmic-parameters">Algorithmic parameters</h2>
<p>Algorithmic parameters such as the step size used in the proposal chain can have a strong effect on the performance of MH sampling. We will later discuss <em>adaptive</em> MCMC algorithms that can tune some of these parameters in a sound fashion.</p>
<div class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sampling a standard Gaussian with a uniform proposal</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MetropolisHastings:</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, x, p, Q, S<span class="op">=</span><span class="fl">1e4</span>):</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters</span></span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a><span class="co">        ----------</span></span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a><span class="co">        x : initial state</span></span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a><span class="co">        p : target distribution</span></span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a><span class="co">        Q : simulator of a symmetric proposal chain</span></span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a><span class="co">        S : number of samples</span></span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.p <span class="op">=</span> p</span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q <span class="op">=</span> Q</span>
<span id="cb58-16"><a href="#cb58-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.S <span class="op">=</span> S</span>
<span id="cb58-17"><a href="#cb58-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._initial <span class="op">=</span> x</span>
<span id="cb58-18"><a href="#cb58-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb58-19"><a href="#cb58-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _reset(<span class="va">self</span>):</span>
<span id="cb58-20"><a href="#cb58-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._counter <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb58-21"><a href="#cb58-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_accepted <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb58-22"><a href="#cb58-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.x <span class="op">=</span> <span class="va">self</span>._initial</span>
<span id="cb58-23"><a href="#cb58-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb58-24"><a href="#cb58-24" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb58-25"><a href="#cb58-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> acceptance_rate(<span class="va">self</span>):</span>
<span id="cb58-26"><a href="#cb58-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.n_accepted <span class="op">/</span> <span class="va">self</span>._counter</span>
<span id="cb58-27"><a href="#cb58-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb58-28"><a href="#cb58-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__next__</span>(<span class="va">self</span>):</span>
<span id="cb58-29"><a href="#cb58-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb58-30"><a href="#cb58-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>._counter <span class="op">&gt;=</span> <span class="va">self</span>.S:</span>
<span id="cb58-31"><a href="#cb58-31" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">StopIteration</span></span>
<span id="cb58-32"><a href="#cb58-32" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb58-33"><a href="#cb58-33" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> <span class="va">self</span>.Q(<span class="va">self</span>.x)</span>
<span id="cb58-34"><a href="#cb58-34" aria-hidden="true" tabindex="-1"></a>        u <span class="op">=</span> np.random.uniform()</span>
<span id="cb58-35"><a href="#cb58-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb58-36"><a href="#cb58-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> u <span class="op">&lt;=</span> <span class="va">self</span>.p(y) <span class="op">/</span> <span class="va">self</span>.p(<span class="va">self</span>.x):</span>
<span id="cb58-37"><a href="#cb58-37" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.x <span class="op">=</span> y</span>
<span id="cb58-38"><a href="#cb58-38" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.n_accepted <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb58-39"><a href="#cb58-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._counter <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb58-40"><a href="#cb58-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb58-41"><a href="#cb58-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.x</span>
<span id="cb58-42"><a href="#cb58-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb58-43"><a href="#cb58-43" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__iter__</span>(<span class="va">self</span>):</span>
<span id="cb58-44"><a href="#cb58-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._reset()</span>
<span id="cb58-45"><a href="#cb58-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb58-46"><a href="#cb58-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb58-47"><a href="#cb58-47" aria-hidden="true" tabindex="-1"></a><span class="co"># target</span></span>
<span id="cb58-48"><a href="#cb58-48" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="kw">lambda</span> x: np.exp(<span class="op">-</span>x<span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">2</span>)</span>
<span id="cb58-49"><a href="#cb58-49" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="fl">1.</span>, <span class="dv">1000</span>) <span class="op">*</span> <span class="dv">5</span></span>
<span id="cb58-50"><a href="#cb58-50" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> t, p(t) <span class="op">/</span> np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi)</span>
<span id="cb58-51"><a href="#cb58-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-52"><a href="#cb58-52" aria-hidden="true" tabindex="-1"></a><span class="co"># proposal</span></span>
<span id="cb58-53"><a href="#cb58-53" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> <span class="kw">lambda</span> x, eps<span class="op">=</span><span class="fl">0.1</span>: np.random.uniform(x<span class="op">-</span>eps, x<span class="op">+</span>eps)</span>
<span id="cb58-54"><a href="#cb58-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-55"><a href="#cb58-55" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> <span class="fl">1e4</span></span>
<span id="cb58-56"><a href="#cb58-56" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="fl">10.</span></span>
<span id="cb58-57"><a href="#cb58-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-58"><a href="#cb58-58" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">3</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">9</span>), sharex<span class="op">=</span><span class="st">'col'</span>)</span>
<span id="cb58-59"><a href="#cb58-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-60"><a href="#cb58-60" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, eps <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="fl">1e-1</span>, <span class="fl">1e0</span>, <span class="fl">1e1</span>]):</span>
<span id="cb58-61"><a href="#cb58-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-62"><a href="#cb58-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># run MH simulation</span></span>
<span id="cb58-63"><a href="#cb58-63" aria-hidden="true" tabindex="-1"></a>    mh <span class="op">=</span> MetropolisHastings(x, p, <span class="kw">lambda</span> x: Q(x, eps), S)</span>
<span id="cb58-64"><a href="#cb58-64" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> np.array(<span class="bu">list</span>(mh))</span>
<span id="cb58-65"><a href="#cb58-65" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb58-66"><a href="#cb58-66" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'acceptance rate: </span><span class="sc">{0:.1%}</span><span class="st"> (stepsize = </span><span class="sc">{1:.2e}</span><span class="st">)'</span>.<span class="bu">format</span>(</span>
<span id="cb58-67"><a href="#cb58-67" aria-hidden="true" tabindex="-1"></a>        mh.acceptance_rate, eps))</span>
<span id="cb58-68"><a href="#cb58-68" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb58-69"><a href="#cb58-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># plot results</span></span>
<span id="cb58-70"><a href="#cb58-70" aria-hidden="true" tabindex="-1"></a>    ax[i,<span class="dv">0</span>].plot(samples, color<span class="op">=</span><span class="st">'k'</span>, lw<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb58-71"><a href="#cb58-71" aria-hidden="true" tabindex="-1"></a>    ax[i,<span class="dv">1</span>].hist(samples, bins<span class="op">=</span><span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb58-72"><a href="#cb58-72" aria-hidden="true" tabindex="-1"></a>    ax[i,<span class="dv">2</span>].hist(samples[<span class="dv">1000</span>:], bins<span class="op">=</span><span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb58-73"><a href="#cb58-73" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> a <span class="kw">in</span> ax[i,<span class="dv">1</span>:]:</span>
<span id="cb58-74"><a href="#cb58-74" aria-hidden="true" tabindex="-1"></a>        a.plot(<span class="op">*</span>target, color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb58-75"><a href="#cb58-75" aria-hidden="true" tabindex="-1"></a>        a.set_ylim(<span class="fl">0.</span>, <span class="fl">0.45</span>)</span>
<span id="cb58-76"><a href="#cb58-76" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>acceptance rate: 97.2% (stepsize = 1.00e-01)
acceptance rate: 80.5% (stepsize = 1.00e+00)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>acceptance rate: 16.4% (stepsize = 1.00e+01)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-50-output-3.png" width="1114" height="824"></p>
</div>
</div>
<p>In the next example, we run the MH algorithm on a continuous pdf in two dimensions.</p>
<div class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># logsumexp(x) is semantically equivalent to log(sum(exp(x)))</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> logsumexp</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a><span class="co">Metroplis sampling of a banana-shaped pdf</span></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Banana:</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.C <span class="op">=</span> np.array([[<span class="fl">1.</span>, <span class="fl">0.9</span>],</span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a>                           [<span class="fl">0.9</span>, <span class="fl">1.</span>]])</span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.C_inv <span class="op">=</span> np.array([[<span class="fl">100.</span>, <span class="op">-</span><span class="fl">90.</span>],</span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a>                               [<span class="op">-</span><span class="dv">90</span>, <span class="fl">100.</span>]]) <span class="op">/</span> <span class="fl">19.</span></span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.a <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb61-20"><a href="#cb61-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb61-21"><a href="#cb61-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-22"><a href="#cb61-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> x.ndim <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb61-23"><a href="#cb61-23" aria-hidden="true" tabindex="-1"></a>            x1, x2 <span class="op">=</span> x.T</span>
<span id="cb61-24"><a href="#cb61-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb61-25"><a href="#cb61-25" aria-hidden="true" tabindex="-1"></a>            x1, x2 <span class="op">=</span> x</span>
<span id="cb61-26"><a href="#cb61-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-27"><a href="#cb61-27" aria-hidden="true" tabindex="-1"></a>        a, b <span class="op">=</span> <span class="va">self</span>.a, <span class="va">self</span>.b</span>
<span id="cb61-28"><a href="#cb61-28" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb61-29"><a href="#cb61-29" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> np.transpose([x1 <span class="op">/</span> a, a <span class="op">*</span> x2 <span class="op">+</span> a <span class="op">*</span> b <span class="op">*</span> (x1<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> a)])</span>
<span id="cb61-30"><a href="#cb61-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-31"><a href="#cb61-31" aria-hidden="true" tabindex="-1"></a>        logp <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> np.<span class="bu">sum</span>(y.dot(<span class="va">self</span>.C_inv.T) <span class="op">*</span> y, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb61-32"><a href="#cb61-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-33"><a href="#cb61-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logp</span>
<span id="cb61-34"><a href="#cb61-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-35"><a href="#cb61-35" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_marginal(pdf2d, axis<span class="op">=</span><span class="dv">0</span>, vals<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb61-36"><a href="#cb61-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-37"><a href="#cb61-37" aria-hidden="true" tabindex="-1"></a>    marginal <span class="op">=</span> logsumexp(pdf2d, axis<span class="op">=</span>axis)</span>
<span id="cb61-38"><a href="#cb61-38" aria-hidden="true" tabindex="-1"></a>    marginal <span class="op">-=</span> logsumexp(marginal)</span>
<span id="cb61-39"><a href="#cb61-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-40"><a href="#cb61-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> vals <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb61-41"><a href="#cb61-41" aria-hidden="true" tabindex="-1"></a>        marginal <span class="op">-=</span> np.log(vals[<span class="dv">1</span>]<span class="op">-</span>vals[<span class="dv">0</span>])</span>
<span id="cb61-42"><a href="#cb61-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-43"><a href="#cb61-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> marginal</span>
<span id="cb61-44"><a href="#cb61-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-45"><a href="#cb61-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-46"><a href="#cb61-46" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> <span class="kw">lambda</span> x, eps<span class="op">=</span><span class="fl">1.</span>: x <span class="op">+</span> np.random.standard_normal(<span class="dv">2</span>) <span class="op">*</span> eps</span>
<span id="cb61-47"><a href="#cb61-47" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> Banana()</span>
<span id="cb61-48"><a href="#cb61-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-49"><a href="#cb61-49" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.zeros(<span class="dv">2</span>)</span>
<span id="cb61-50"><a href="#cb61-50" aria-hidden="true" tabindex="-1"></a>p_x <span class="op">=</span> p(x)</span>
<span id="cb61-51"><a href="#cb61-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-52"><a href="#cb61-52" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> [x]</span>
<span id="cb61-53"><a href="#cb61-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-54"><a href="#cb61-54" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="bu">len</span>(samples) <span class="op">&lt;</span> <span class="fl">1e4</span>:</span>
<span id="cb61-55"><a href="#cb61-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-56"><a href="#cb61-56" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> Q(x)</span>
<span id="cb61-57"><a href="#cb61-57" aria-hidden="true" tabindex="-1"></a>    p_y <span class="op">=</span> p(y)</span>
<span id="cb61-58"><a href="#cb61-58" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb61-59"><a href="#cb61-59" aria-hidden="true" tabindex="-1"></a>    accept <span class="op">=</span> np.log(np.random.uniform()) <span class="op">&lt;</span> p_y <span class="op">-</span> p_x</span>
<span id="cb61-60"><a href="#cb61-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> accept:</span>
<span id="cb61-61"><a href="#cb61-61" aria-hidden="true" tabindex="-1"></a>        x, p_x <span class="op">=</span> y, p_y</span>
<span id="cb61-62"><a href="#cb61-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-63"><a href="#cb61-63" aria-hidden="true" tabindex="-1"></a>    samples.append(x)</span>
<span id="cb61-64"><a href="#cb61-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-65"><a href="#cb61-65" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> np.array(samples)</span>
<span id="cb61-66"><a href="#cb61-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-67"><a href="#cb61-67" aria-hidden="true" tabindex="-1"></a><span class="co"># plot results</span></span>
<span id="cb61-68"><a href="#cb61-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-69"><a href="#cb61-69" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">3.</span>, <span class="fl">3.</span>, <span class="dv">200</span>)</span>
<span id="cb61-70"><a href="#cb61-70" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">8.</span>, <span class="fl">1.</span>, <span class="bu">len</span>(x))</span>
<span id="cb61-71"><a href="#cb61-71" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> np.reshape(np.meshgrid(x, y), (<span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)).T</span>
<span id="cb61-72"><a href="#cb61-72" aria-hidden="true" tabindex="-1"></a>pdf <span class="op">=</span> p(grid).reshape(<span class="bu">len</span>(x), <span class="bu">len</span>(y))</span>
<span id="cb61-73"><a href="#cb61-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-74"><a href="#cb61-74" aria-hidden="true" tabindex="-1"></a>pdf_x <span class="op">=</span> compute_marginal(pdf, axis<span class="op">=</span><span class="dv">0</span>, vals<span class="op">=</span>x)</span>
<span id="cb61-75"><a href="#cb61-75" aria-hidden="true" tabindex="-1"></a>pdf_y <span class="op">=</span> compute_marginal(pdf, axis<span class="op">=</span><span class="dv">1</span>, vals<span class="op">=</span>y)</span>
<span id="cb61-76"><a href="#cb61-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-77"><a href="#cb61-77" aria-hidden="true" tabindex="-1"></a>kw_hist <span class="op">=</span> <span class="bu">dict</span>(bins<span class="op">=</span><span class="dv">30</span>, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb61-78"><a href="#cb61-78" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb61-79"><a href="#cb61-79" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb61-80"><a href="#cb61-80" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].contour(x, y, np.exp(pdf))</span>
<span id="cb61-81"><a href="#cb61-81" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].scatter(<span class="op">*</span>samples.T, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, s<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb61-82"><a href="#cb61-82" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="vs">r'$x_1$'</span>)</span>
<span id="cb61-83"><a href="#cb61-83" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="vs">r'$x_2$'</span>)</span>
<span id="cb61-84"><a href="#cb61-84" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb61-85"><a href="#cb61-85" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].hist(samples[:,<span class="dv">0</span>], <span class="op">**</span>kw_hist)</span>
<span id="cb61-86"><a href="#cb61-86" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(x, np.exp(pdf_x), color<span class="op">=</span><span class="st">'r'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb61-87"><a href="#cb61-87" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="vs">r'$x_1$'</span>)</span>
<span id="cb61-88"><a href="#cb61-88" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="vs">r'$p(x_1)$'</span>)</span>
<span id="cb61-89"><a href="#cb61-89" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb61-90"><a href="#cb61-90" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].hist(samples[:,<span class="dv">1</span>], <span class="op">**</span>kw_hist)                       </span>
<span id="cb61-91"><a href="#cb61-91" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].plot(y, np.exp(pdf_y), color<span class="op">=</span><span class="st">'r'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb61-92"><a href="#cb61-92" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_xlabel(<span class="vs">r'$x_2$'</span>)</span>
<span id="cb61-93"><a href="#cb61-93" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_ylabel(<span class="vs">r'$p(x_2)$'</span>)</span>
<span id="cb61-94"><a href="#cb61-94" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb61-95"><a href="#cb61-95" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-51-output-1.png" width="1114" height="346"></p>
</div>
</div>
</section>
<section id="combining-markov-chains" class="level2">
<h2 class="anchored" data-anchor-id="combining-markov-chains">Combining Markov Chains</h2>
<p>Let <span class="math inline">\(P_i\)</span> be <span class="math inline">\(N\)</span> Markov chains that share the same stationary distribution <span class="math inline">\(\pi\)</span>:</p>
<p><span id="eq-multiple_chains"><span class="math display">\[
P_i \pi = \pi, \,\,\, \mathbb{1}^T\!P_i = \mathbb{1}^T
\tag{68}\]</span></span></p>
<p>The product of all chains, <span class="math inline">\(P=\prod_i P_i\)</span> (here <span class="math inline">\(\prod_i\)</span> symbolizes a <em>matrix product</em>), is also a Markov chain with the same stationary distribution:</p>
<p><span id="eq-product_chain"><span class="math display">\[
P\pi = \pi, \,\,\, \mathbb{1}^T\!P = \mathbb{1}^T
\tag{69}\]</span></span></p>
<p>Therefore, the following algorithm will simulate <span class="math inline">\(P\)</span> and therefore eventually <span class="math inline">\(\pi\)</span>:</p>
<p><span class="math display">\[\begin{align}\label{eq-markov-sequence}
\begin{split}
  \tilde{x}^{(1)} &amp;\sim P_1\bigl(\,\cdot\,, x^{(s)}\bigr) \\
  \tilde{x}^{(2)} &amp;\sim P_2\bigl(\,\cdot\,, \tilde{x}^{(1)}\bigr) \\
  &amp; \vdots   \\
  x^{(s+1)} &amp;\sim P_N\bigl(\,\cdot\,, \tilde{x}^{(N-1)}\bigr) \\
\end{split}
\end{align}\]</span></p>
<section id="coordinate-wise-sampling" class="level3">
<h3 class="anchored" data-anchor-id="coordinate-wise-sampling">Coordinate-wise sampling</h3>
<p>Let’s look at a special but important case. Assume that the sample space decomposes into a product of <span class="math inline">\(N\)</span> sample spaces <span class="math inline">\(\mathcal X = \mathcal X_1 \times \cdots \times \mathcal X_N\)</span> with associated variables <span class="math inline">\(x_i\)</span> (<span class="math inline">\(i=1, \ldots, N\)</span>) where <span class="math inline">\(x_i\)</span> denotes a single variable or a group of variables that will be sampled jointly. The joint distribution of all variables is <span class="math inline">\(p(x) = p(x_1, \ldots, x_N)\)</span>.</p>
<p>By <span class="math inline">\(x_{\backslash{}i}\)</span> we denote the variable vector obtained by omitting the <span class="math inline">\(i\)</span>-th variable (or group of variables):</p>
<p><span id="eq-without-group"><span class="math display">\[
x_{\backslash{} i} := \begin{pmatrix}x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_N \end{pmatrix}
\tag{70}\]</span></span></p>
<p>Then <span class="math inline">\(x_{\backslash{} i}\)</span> follows the marginal distribution:</p>
<p><span id="eq-without-group-marginal"><span class="math display">\[
p_{\backslash{} i}(x_{\backslash{} i}) = \int p(x_1, \ldots, x_N)\, d x_i
\tag{71}\]</span></span></p>
<p>The marginal distribution of <span class="math inline">\(x_i\)</span> is:</p>
<p><span id="eq-group-marginal"><span class="math display">\[
p_{i}(x_{i}) = \int p(x_1, \ldots, x_N)\, d x_{\backslash{} i}
\tag{72}\]</span></span></p>
<p>The conditional distribution of <span class="math inline">\(x_i\)</span> also readily available:</p>
<p><span id="eq-group-conditional"><span class="math display">\[
p_{i}(x_{i}\mid{}x_{\backslash{} i}) = \frac{p(x)}{p_{\backslash{} i}(x_{\backslash{} i})}
\tag{73}\]</span></span></p>
<p>Let us consider a sequence of <span class="math inline">\(N\)</span> Markov chains <span class="math inline">\(P_i\)</span> where each chain only updates <span class="math inline">\(x_i\)</span> and does not change <span class="math inline">\(x_{\backslash{} i}\)</span>:</p>
<p><span id="eq-chain-group"><span class="math display">\[
Q_i(y, x) = q_i(y_i, x_i; x_{\backslash{} i})\, \delta(y_{\backslash{} i} - x_{\backslash{} i})  
\tag{74}\]</span></span></p>
<p>where we introduced a product of delta distributions:</p>
<p><span class="math display">\[
\delta(x_{\backslash{} i}) := \prod_{j\not= i} \delta(x_j)
\]</span></p>
<p>and <span class="math inline">\(q_i(y_i, x_i; x_{\backslash{} i})\)</span> is a Markov kernel on <span class="math inline">\(\mathcal X_i \times \mathcal X_i\)</span> with <span class="math inline">\(x_{\backslash{} i}\)</span> being treated like a set of parameters.</p>
<p>The Metropolis map of <span class="math inline">\(Q_i(y, x)\)</span> is</p>
<p><span class="math display">\[\begin{align}\label{eq-coordinatewise-map}
\begin{split}
M[Q_i](y, x)
&amp;= \min\left\{Q_i(y, x), Q_i(x, y) \frac{p(y)}{p(x)} \right\} \\
&amp;= \delta(y_{\backslash{} i}-x_{\backslash{} i})\, \min\left\{q_i(y_i, x_i; x_{\backslash{} i}), q_i(x_i, y_i; x_{\backslash{} i}) \frac{p_i(y_i\mid{}y_{\backslash{} i})\,p_{\backslash{} i}(y_{\backslash{} i})}{p_i(x_i \mid{} x_{\backslash{} i}) p_{\backslash{} i}(x_{\backslash{} i})} \right\} \\
&amp;= \delta(y_{\backslash{} i}-x_{\backslash{} i})\, \min\left\{q_i(y_i, x_i; x_{\backslash{} i}), q_i(x_i, y_i; x_{\backslash{} i}) \frac{p_i(y_i\mid{}x_{\backslash{} i})}{p_i(x_i \mid{} x_{\backslash{} i})} \right\}
\end{split}
\end{align}\]</span></p>
<p>If we run the Metropolis-Hastings algorithm with <span class="math inline">\(Q_i\)</span>, we simulate a Markov chain only on the conditional distribution of the <span class="math inline">\(i\)</span>-th variable (or group of variables). This involves a Markov kernel <span class="math inline">\(q_i\)</span> on the corresponding subspace <span class="math inline">\(\mathcal X_i\)</span> that could, in principle, depend on all of the current variables.</p>
<p>This produces a Markov chain with the correct stationary distribution, but since <span class="math inline">\(Q_i\)</span> changes only the <span class="math inline">\(i\)</span>-th variable, the resulting Markov chain is not ergodic. The trick is to update each parameter group successively using <span class="math inline">\(Q_i\)</span> in each subspace <span class="math inline">\(\mathcal X_i\)</span>, and thereby produce a sequence of Metropolis maps, <span class="math inline">\(M[Q_i]\)</span>, that share a common target distribution. Simulation of <span class="math inline">\(M[Q_i]\)</span> one after the other generates a simulation of the product chain, in a fashion analogous to equation (<span class="math inline">\(\ref{eq-markov-sequence}\)</span>). This scheme is sometimes called <em>Metropolis-within-Gibbs</em>.</p>
<p>Let’s apply the coordinate-wise sampling scheme to the banana-shaped distribution using uniform proposals in each direction:</p>
<div class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> Banana()</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> []</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>    Q_i <span class="op">=</span> <span class="kw">lambda</span> x, i<span class="op">=</span>i, eps<span class="op">=</span><span class="fl">2.</span>: <span class="op">\</span></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>    x <span class="op">+</span> eps <span class="op">*</span> np.random.uniform(<span class="op">-</span><span class="fl">1.</span>,<span class="fl">1.</span>,<span class="dv">2</span>) <span class="op">*</span> np.eye(<span class="dv">2</span>)[i]</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>    Q.append(Q_i)</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.zeros(<span class="dv">2</span>)</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>p_x <span class="op">=</span> p(x)</span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> [x]</span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="bu">len</span>(samples) <span class="op">&lt;</span> <span class="fl">1e5</span>:</span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> Q_i <span class="kw">in</span> Q:</span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> Q_i(x)</span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a>        p_y <span class="op">=</span> p(y)</span>
<span id="cb62-18"><a href="#cb62-18" aria-hidden="true" tabindex="-1"></a>        accept <span class="op">=</span> np.log(np.random.uniform()) <span class="op">&lt;</span> p_y <span class="op">-</span> p_x</span>
<span id="cb62-19"><a href="#cb62-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> accept:</span>
<span id="cb62-20"><a href="#cb62-20" aria-hidden="true" tabindex="-1"></a>            x, p_x <span class="op">=</span> y, p_y</span>
<span id="cb62-21"><a href="#cb62-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-22"><a href="#cb62-22" aria-hidden="true" tabindex="-1"></a>    samples.append(x)</span>
<span id="cb62-23"><a href="#cb62-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-24"><a href="#cb62-24" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> np.array(samples)</span>
<span id="cb62-25"><a href="#cb62-25" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> samples[<span class="bu">int</span>(<span class="fl">0.2</span><span class="op">*</span><span class="bu">len</span>(samples)):]</span>
<span id="cb62-26"><a href="#cb62-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-27"><a href="#cb62-27" aria-hidden="true" tabindex="-1"></a><span class="co"># plot results</span></span>
<span id="cb62-28"><a href="#cb62-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-29"><a href="#cb62-29" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">3.</span>, <span class="fl">3.</span>, <span class="dv">200</span>)</span>
<span id="cb62-30"><a href="#cb62-30" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">8.</span>, <span class="fl">1.</span>, <span class="bu">len</span>(x))</span>
<span id="cb62-31"><a href="#cb62-31" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> np.reshape(np.meshgrid(x, y), (<span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)).T</span>
<span id="cb62-32"><a href="#cb62-32" aria-hidden="true" tabindex="-1"></a>pdf <span class="op">=</span> p(grid).reshape(<span class="bu">len</span>(x), <span class="bu">len</span>(y))</span>
<span id="cb62-33"><a href="#cb62-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-34"><a href="#cb62-34" aria-hidden="true" tabindex="-1"></a>pdf_x <span class="op">=</span> compute_marginal(pdf, axis<span class="op">=</span><span class="dv">0</span>, vals<span class="op">=</span>x)</span>
<span id="cb62-35"><a href="#cb62-35" aria-hidden="true" tabindex="-1"></a>pdf_y <span class="op">=</span> compute_marginal(pdf, axis<span class="op">=</span><span class="dv">1</span>, vals<span class="op">=</span>y)</span>
<span id="cb62-36"><a href="#cb62-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-37"><a href="#cb62-37" aria-hidden="true" tabindex="-1"></a>kw_hist <span class="op">=</span> <span class="bu">dict</span>(bins<span class="op">=</span><span class="dv">50</span>, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb62-38"><a href="#cb62-38" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb62-39"><a href="#cb62-39" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb62-40"><a href="#cb62-40" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].contour(x, y, np.exp(pdf))</span>
<span id="cb62-41"><a href="#cb62-41" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].scatter(<span class="op">*</span>samples.T, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, s<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb62-42"><a href="#cb62-42" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="vs">r'$x_1$'</span>)</span>
<span id="cb62-43"><a href="#cb62-43" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="vs">r'$x_2$'</span>)</span>
<span id="cb62-44"><a href="#cb62-44" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb62-45"><a href="#cb62-45" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].hist(samples[:,<span class="dv">0</span>], <span class="op">**</span>kw_hist)</span>
<span id="cb62-46"><a href="#cb62-46" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(x, np.exp(pdf_x), color<span class="op">=</span><span class="st">'r'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb62-47"><a href="#cb62-47" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="vs">r'$x_1$'</span>)</span>
<span id="cb62-48"><a href="#cb62-48" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="vs">r'$p(x_1)$'</span>)</span>
<span id="cb62-49"><a href="#cb62-49" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb62-50"><a href="#cb62-50" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].hist(samples[:,<span class="dv">1</span>], <span class="op">**</span>kw_hist)                       </span>
<span id="cb62-51"><a href="#cb62-51" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].plot(y, np.exp(pdf_y), color<span class="op">=</span><span class="st">'r'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb62-52"><a href="#cb62-52" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_xlabel(<span class="vs">r'$x_2$'</span>)</span>
<span id="cb62-53"><a href="#cb62-53" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_ylabel(<span class="vs">r'$p(x_2)$'</span>)</span>
<span id="cb62-54"><a href="#cb62-54" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb62-55"><a href="#cb62-55" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-52-output-1.png" width="1114" height="346"></p>
</div>
</div>
</section>
</section>
<section id="gibbs-sampling" class="level2">
<h2 class="anchored" data-anchor-id="gibbs-sampling">Gibbs sampling</h2>
<p><a href="https://en.wikipedia.org/wiki/Gibbs_sampling"><strong>Gibbs sampling</strong></a> is a simple and powerful MCMC method that can be interpreted as a special variant of the component-wise MH algorithm outlined above. Gibbs sampling assumes that we can sample directly from the conditional distributions <span class="math inline">\(p_i(x_i \mid{} x_{\backslash{} i})\)</span>, and uses these as component-wise Markov chains:</p>
<p><span class="math display">\[
q_i(y_i, x_i; x_{\backslash{} i}) = p_i(y_i \mid{} x_{\backslash{} i})
\]</span></p>
<p>The acceptance ratio in the Metropolis map <span class="math inline">\(M[Q_i]\)</span> (Eq. <span class="math inline">\(\ref{eq-coordinatewise-map}\)</span>) simplifies to</p>
<p><span class="math display">\[
\frac{q_i(x_i, y_i; x_{\backslash{} i})}{q_i(y_i, x_i; x_{\backslash{} i})} \frac{p_i(y_i\mid{}x_{\backslash{} i})}{p_i(x_i\mid{}x_{\backslash{} i})} =
\frac{p_i(x_i \mid{} x_{\backslash{} i})}{p_i(y_i \mid{} x_{\backslash{} i})} \frac{p_i(y_i\mid{}x_{\backslash{} i})}{p_i(x_i\mid{}x_{\backslash{} i})} = 1
\]</span></p>
<p>That is, all proposals are accepted - Gibbs sampling is rejection-free.</p>
<section id="algorithm-gibbs-sampling" class="level3">
<h3 class="anchored" data-anchor-id="algorithm-gibbs-sampling">Algorithm: Gibbs sampling</h3>
<p>Let <span class="math inline">\(p(x_1, \ldots, x_N)\)</span> be the joint distribution of <span class="math inline">\(N\)</span> random variables or groups of random variables <span class="math inline">\(x_i\in\mathcal X_i\)</span> with conditional distributions <span class="math inline">\(p_i(x_i \mid{} x_{\backslash{} i})\)</span>, then the following iterative algorithm simulates a Markov chain whose stationary distribution is <span class="math inline">\(p(x_1, \ldots, x_N)\)</span>:</p>
<p><span class="math display">\[\begin{align}\label{eq-gibbs-sampling}
\begin{split}
  x^{(s+1)}_1 &amp;\sim p_1\bigl(\,\cdot\, \mid{} x^{(s)}_{\backslash{} 1}\bigr) \\
  x^{(s+1)}_2 &amp;\sim p_2\bigl(\,\cdot\, \mid{} {x}^{(s,s+1)}_{\backslash{} 2}\bigr) \\
  &amp; \vdots   \\
  x^{(s+1)}_N &amp;\sim p_N\bigl(\,\cdot\, \mid{} {x}^{(s,s+1)}_{\backslash{} N}\bigr) \\
\end{split}
\end{align}\]</span></p>
<p>where <span class="math inline">\({x}^{(s+1)}_i\)</span> are the components of the next sample and</p>
<p><span class="math display">\[{x}^{(s,s+1)}_{\backslash{} i} = \begin{pmatrix}{x}^{(s+1)}_1, \ldots, {x}^{(s+1)}_{i-1}, x^{(s)}_{i+1}, \ldots, x^{(s)}_{N}\end{pmatrix}\]</span></p>
<p>so <span class="math inline">\({x}^{(s,s+1)}_{\backslash{} N} = x^{(s+1)}_{\backslash{} N}\)</span>.</p>
<section id="collapsed-gibbs-sampler" class="level4">
<h4 class="anchored" data-anchor-id="collapsed-gibbs-sampler">Collapsed Gibbs Sampler</h4>
<p>A variant of the Gibbs sampler (Eq. <span class="math inline">\(\ref{eq-gibbs-sampling}\)</span>) is the <a href="https://en.wikipedia.org/wiki/Gibbs_sampling#Collapsed_Gibbs_sampler"><em>collapsed Gibbs sampler</em></a> where some of the conditional distributions <span class="math inline">\(p_i(x_i \mid{} x_{\backslash{} i})\)</span> are replaced by a marginal distribution, e.g.&nbsp;<span class="math inline">\(p_i(x_i) = \int p(x_1, \ldots, x_N) dx_{\backslash{} i}\)</span>. This scheme is equally valid and rejection-free. See also the original paper by <a href="https://www.tandfonline.com/doi/abs/10.1080/01621459.1994.10476829">Jun S. Liu</a>.</p>
</section>
<section id="example-sampling-a-bivariate-gaussian-model" class="level4">
<h4 class="anchored" data-anchor-id="example-sampling-a-bivariate-gaussian-model">Example: Sampling a bivariate Gaussian model</h4>
<p>In the following, let’s use Gibbs sampling to draw from a two-dimensional Gaussian distribution with general covariance matrix:</p>
<p><span class="math display">\[
p(x_1, x_2) = \frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}} \exp\left\{-\frac{1}{2} \begin{pmatrix} x_1 - \mu_1 \\ x_2 - \mu_2\end{pmatrix}^T \begin{pmatrix} \sigma_1^2 &amp; \sigma_1\sigma_2\rho \\ \sigma_1\sigma_2\rho &amp; \sigma_2^2 \end{pmatrix}^{-1} \begin{pmatrix} x_1 - \mu_1 \\ x_2 -\mu_2 \end{pmatrix}  \right\}
\]</span></p>
<p>The conditional distributions are given <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Bivariate_case_2">by</a>:</p>
<p><span class="math display">\[\begin{align*}
p(x_1 \mid{} x_2) &amp;= \frac{1}{\sqrt{2\pi\sigma_1^2(1-\rho^2)}} \exp\left\{-\frac{1}{2\sigma_1^2(1-\rho^2)} \bigl(x_1 - \mu_1 - \frac{\sigma_1}{\sigma_2}\rho (x_2 - \mu_2) \bigr)^2\right\} \\
p(x_2 \mid{} x_1) &amp;= \frac{1}{\sqrt{2\pi\sigma_2^2(1-\rho^2)}} \exp\left\{-\frac{1}{2\sigma_2^2(1-\rho^2)} \bigl(x_2 - \mu_2 - \frac{\sigma_2}{\sigma_1}\rho (x_1 - \mu_1) \bigr)^2\right\}
\end{align*}\]</span></p>
<p>and the marginal distributions are <span class="math inline">\(p_i(x_i) = \mathcal N(\mu_i, \sigma_i^2)\)</span>. We use the standard Gibbs sampler and the collapsed Gibbs samplers to generate samples from the joint bivariate Gaussian:</p>
<div class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Gaussian:</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Bivariate Gaussian</span></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, mu <span class="op">=</span> np.zeros(<span class="dv">2</span>), sigma1<span class="op">=</span><span class="fl">1.</span>, sigma2<span class="op">=</span><span class="fl">3.</span>, rho<span class="op">=</span><span class="fl">0.95</span>):</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mu <span class="op">=</span> mu</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sigma <span class="op">=</span> np.array([sigma1, sigma2])</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rho <span class="op">=</span> <span class="bu">float</span>(rho)</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sigma1(<span class="va">self</span>):</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.sigma[<span class="dv">0</span>]</span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sigma2(<span class="va">self</span>):</span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.sigma[<span class="dv">1</span>]</span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> Sigma(<span class="va">self</span>):</span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a><span class="co">        Covariance matrix</span></span>
<span id="cb63-22"><a href="#cb63-22" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb63-23"><a href="#cb63-23" aria-hidden="true" tabindex="-1"></a>        Sigma <span class="op">=</span> np.diag([<span class="va">self</span>.sigma1, <span class="va">self</span>.sigma2])</span>
<span id="cb63-24"><a href="#cb63-24" aria-hidden="true" tabindex="-1"></a>        Sigma <span class="op">=</span> Sigma <span class="op">@</span> np.array([[<span class="dv">1</span>, <span class="va">self</span>.rho], [<span class="va">self</span>.rho, <span class="dv">1</span>]]) <span class="op">@</span> Sigma</span>
<span id="cb63-25"><a href="#cb63-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> Sigma</span>
<span id="cb63-26"><a href="#cb63-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-27"><a href="#cb63-27" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb63-28"><a href="#cb63-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> Lambda(<span class="va">self</span>):</span>
<span id="cb63-29"><a href="#cb63-29" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb63-30"><a href="#cb63-30" aria-hidden="true" tabindex="-1"></a><span class="co">        Precision matrix</span></span>
<span id="cb63-31"><a href="#cb63-31" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb63-32"><a href="#cb63-32" aria-hidden="true" tabindex="-1"></a>        precision <span class="op">=</span> np.diag([<span class="dv">1</span><span class="op">/</span><span class="va">self</span>.sigma1, <span class="dv">1</span><span class="op">/</span><span class="va">self</span>.sigma2]) <span class="op">/</span> np.sqrt(<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.rho<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb63-33"><a href="#cb63-33" aria-hidden="true" tabindex="-1"></a>        precision <span class="op">=</span> precision <span class="op">@</span> np.array([[<span class="dv">1</span>, <span class="op">-</span><span class="va">self</span>.rho], [<span class="op">-</span><span class="va">self</span>.rho, <span class="dv">1</span>]])  <span class="op">@</span> precision</span>
<span id="cb63-34"><a href="#cb63-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> precision</span>
<span id="cb63-35"><a href="#cb63-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-36"><a href="#cb63-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> log_prob(<span class="va">self</span>, x):</span>
<span id="cb63-37"><a href="#cb63-37" aria-hidden="true" tabindex="-1"></a>        logp <span class="op">=</span> <span class="op">-</span><span class="fl">.5</span> <span class="op">*</span> np.<span class="bu">sum</span>(x <span class="op">*</span> x.dot(<span class="va">self</span>.Lambda), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb63-38"><a href="#cb63-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logp</span>
<span id="cb63-39"><a href="#cb63-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-40"><a href="#cb63-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample_conditional(<span class="va">self</span>, x, index<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb63-41"><a href="#cb63-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-42"><a href="#cb63-42" aria-hidden="true" tabindex="-1"></a>        ratio <span class="op">=</span> <span class="va">self</span>.rho <span class="op">*</span> <span class="va">self</span>.sigma[index] <span class="op">/</span> <span class="va">self</span>.sigma[<span class="dv">1</span><span class="op">-</span>index]</span>
<span id="cb63-43"><a href="#cb63-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb63-44"><a href="#cb63-44" aria-hidden="true" tabindex="-1"></a>        mu <span class="op">=</span> <span class="va">self</span>.mu[index] <span class="op">+</span> ratio <span class="op">*</span> (x[<span class="dv">1</span><span class="op">-</span>index] <span class="op">-</span> <span class="va">self</span>.mu[<span class="dv">1</span><span class="op">-</span>index])</span>
<span id="cb63-45"><a href="#cb63-45" aria-hidden="true" tabindex="-1"></a>        sigma <span class="op">=</span> np.sqrt(<span class="dv">1</span><span class="op">-</span><span class="va">self</span>.rho<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> <span class="va">self</span>.sigma[index]</span>
<span id="cb63-46"><a href="#cb63-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-47"><a href="#cb63-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.random.standard_normal() <span class="op">*</span> sigma <span class="op">+</span> mu</span>
<span id="cb63-48"><a href="#cb63-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-49"><a href="#cb63-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample_marginal(<span class="va">self</span>, index<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb63-50"><a href="#cb63-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-51"><a href="#cb63-51" aria-hidden="true" tabindex="-1"></a>        mu <span class="op">=</span> <span class="va">self</span>.mu[index]</span>
<span id="cb63-52"><a href="#cb63-52" aria-hidden="true" tabindex="-1"></a>        sigma <span class="op">=</span> <span class="va">self</span>.sigma[index]</span>
<span id="cb63-53"><a href="#cb63-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-54"><a href="#cb63-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.random.standard_normal() <span class="op">*</span> sigma <span class="op">+</span> mu</span>
<span id="cb63-55"><a href="#cb63-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-56"><a href="#cb63-56" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-57"><a href="#cb63-57" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_marginal(prob, axis<span class="op">=</span><span class="dv">0</span>, x<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb63-58"><a href="#cb63-58" aria-hidden="true" tabindex="-1"></a>    marginal <span class="op">=</span> logsumexp(prob, axis<span class="op">=</span>axis)</span>
<span id="cb63-59"><a href="#cb63-59" aria-hidden="true" tabindex="-1"></a>    marginal <span class="op">-=</span> logsumexp(marginal)</span>
<span id="cb63-60"><a href="#cb63-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> x <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb63-61"><a href="#cb63-61" aria-hidden="true" tabindex="-1"></a>        marginal <span class="op">-=</span> np.log(x[<span class="dv">1</span>]<span class="op">-</span>x[<span class="dv">0</span>])</span>
<span id="cb63-62"><a href="#cb63-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> marginal</span>
<span id="cb63-63"><a href="#cb63-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-64"><a href="#cb63-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-65"><a href="#cb63-65" aria-hidden="true" tabindex="-1"></a>pdf <span class="op">=</span> Gaussian(sigma1<span class="op">=</span><span class="fl">1.</span>, sigma2<span class="op">=</span><span class="fl">5.</span>, rho<span class="op">=</span><span class="fl">0.95</span>)</span>
<span id="cb63-66"><a href="#cb63-66" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>, <span class="dv">100</span>) <span class="op">*</span> <span class="dv">3</span> <span class="op">*</span> pdf.sigma1</span>
<span id="cb63-67"><a href="#cb63-67" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>, <span class="dv">100</span>) <span class="op">*</span> <span class="dv">3</span> <span class="op">*</span> pdf.sigma2</span>
<span id="cb63-68"><a href="#cb63-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-69"><a href="#cb63-69" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> np.meshgrid(x, y)</span>
<span id="cb63-70"><a href="#cb63-70" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> np.transpose([X.flatten(), Y.flatten()])</span>
<span id="cb63-71"><a href="#cb63-71" aria-hidden="true" tabindex="-1"></a>prob <span class="op">=</span> pdf.log_prob(grid).reshape(<span class="bu">len</span>(x), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb63-72"><a href="#cb63-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-73"><a href="#cb63-73" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> [(<span class="fl">20.</span>, <span class="fl">90.</span>)]</span>
<span id="cb63-74"><a href="#cb63-74" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="bu">len</span>(samples) <span class="op">&lt;</span> <span class="fl">1e4</span>:</span>
<span id="cb63-75"><a href="#cb63-75" aria-hidden="true" tabindex="-1"></a>    newstate <span class="op">=</span> <span class="bu">list</span>(samples[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb63-76"><a href="#cb63-76" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> index <span class="kw">in</span> [<span class="dv">0</span>, <span class="dv">1</span>]:</span>
<span id="cb63-77"><a href="#cb63-77" aria-hidden="true" tabindex="-1"></a>        newstate[index] <span class="op">=</span> pdf.sample_conditional(newstate, index)</span>
<span id="cb63-78"><a href="#cb63-78" aria-hidden="true" tabindex="-1"></a>    samples.append(<span class="bu">tuple</span>(newstate))</span>
<span id="cb63-79"><a href="#cb63-79" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> np.array(samples)</span>
<span id="cb63-80"><a href="#cb63-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-81"><a href="#cb63-81" aria-hidden="true" tabindex="-1"></a>logp <span class="op">=</span> pdf.log_prob(samples)</span>
<span id="cb63-82"><a href="#cb63-82" aria-hidden="true" tabindex="-1"></a>burnin <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.1</span><span class="op">*</span><span class="bu">len</span>(samples))</span>
<span id="cb63-83"><a href="#cb63-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-84"><a href="#cb63-84" aria-hidden="true" tabindex="-1"></a>kw_hist <span class="op">=</span> <span class="bu">dict</span>(bins<span class="op">=</span><span class="dv">30</span>, color<span class="op">=</span><span class="st">'k'</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb63-85"><a href="#cb63-85" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb63-86"><a href="#cb63-86" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> <span class="bu">list</span>(ax.flat)</span>
<span id="cb63-87"><a href="#cb63-87" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb63-88"><a href="#cb63-88" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].scatter(<span class="op">*</span>samples.T, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb63-89"><a href="#cb63-89" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].contour(x, y, np.exp(prob))</span>
<span id="cb63-90"><a href="#cb63-90" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="vs">r'$x_1$'</span>)</span>
<span id="cb63-91"><a href="#cb63-91" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="vs">r'$x_2$'</span>)</span>
<span id="cb63-92"><a href="#cb63-92" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb63-93"><a href="#cb63-93" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(logp[:<span class="dv">50</span>], color<span class="op">=</span><span class="st">'k'</span>, lw<span class="op">=</span><span class="dv">3</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb63-94"><a href="#cb63-94" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="vs">r'iteration $s$'</span>)</span>
<span id="cb63-95"><a href="#cb63-95" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="vs">r'$\log p(x_1^{(s)}, x_2^{(s)})$'</span>)</span>
<span id="cb63-96"><a href="#cb63-96" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb63-97"><a href="#cb63-97" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].hist(samples[burnin:,<span class="dv">0</span>], <span class="op">**</span>kw_hist)</span>
<span id="cb63-98"><a href="#cb63-98" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].plot(x, np.exp(compute_marginal(prob, <span class="dv">1</span>, x)), color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb63-99"><a href="#cb63-99" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_xlabel(<span class="vs">r'$x_1$'</span>)</span>
<span id="cb63-100"><a href="#cb63-100" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_ylabel(<span class="vs">r'$p_1(x_1)$'</span>)</span>
<span id="cb63-101"><a href="#cb63-101" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb63-102"><a href="#cb63-102" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">3</span>].hist(samples[burnin:,<span class="dv">1</span>], <span class="op">**</span>kw_hist)</span>
<span id="cb63-103"><a href="#cb63-103" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">3</span>].plot(y, np.exp(compute_marginal(prob, <span class="dv">0</span>, y)), color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb63-104"><a href="#cb63-104" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">3</span>].set_xlabel(<span class="vs">r'$x_2$'</span>)</span>
<span id="cb63-105"><a href="#cb63-105" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">3</span>].set_ylabel(<span class="vs">r'$p_2(x_2)$'</span>)</span>
<span id="cb63-106"><a href="#cb63-106" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb63-107"><a href="#cb63-107" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-53-output-1.png" width="922" height="922"></p>
</div>
</div>
<p>This collapsed Gibbs sampler uses the marginal distribution to sample <span class="math inline">\(x_1\)</span> and the conditional distribution to sample <span class="math inline">\(x_2\)</span>:</p>
<div class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># collapsed Gibbs 1</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> [(<span class="fl">20.</span>, <span class="fl">90.</span>)]</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="bu">len</span>(samples) <span class="op">&lt;</span> <span class="fl">1e4</span>:</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>    newstate <span class="op">=</span> <span class="bu">list</span>(samples[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>    newstate[<span class="dv">0</span>] <span class="op">=</span> pdf.sample_marginal(<span class="dv">0</span>)</span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>    newstate[<span class="dv">1</span>] <span class="op">=</span> pdf.sample_conditional(newstate, <span class="dv">1</span>)</span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>    samples.append(<span class="bu">tuple</span>(newstate))</span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> np.array(samples)</span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>logp <span class="op">=</span> pdf.log_prob(samples)</span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a>burnin <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.1</span><span class="op">*</span><span class="bu">len</span>(samples))</span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> <span class="bu">list</span>(ax.flat)</span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].scatter(<span class="op">*</span>samples.T, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].contour(x, y, np.exp(prob))</span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="vs">r'$x_1$'</span>)</span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="vs">r'$x_2$'</span>)</span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(logp[:<span class="dv">50</span>], color<span class="op">=</span><span class="st">'k'</span>, lw<span class="op">=</span><span class="dv">3</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="vs">r'iteration $s$'</span>)</span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="vs">r'$\log p(x_1^{(s)}, x_2^{(s)})$'</span>)</span>
<span id="cb64-25"><a href="#cb64-25" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb64-26"><a href="#cb64-26" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].hist(samples[burnin:,<span class="dv">0</span>], <span class="op">**</span>kw_hist)</span>
<span id="cb64-27"><a href="#cb64-27" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].plot(x, np.exp(compute_marginal(prob, <span class="dv">1</span>, x)), color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb64-28"><a href="#cb64-28" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_xlabel(<span class="vs">r'$x_1$'</span>)</span>
<span id="cb64-29"><a href="#cb64-29" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_ylabel(<span class="vs">r'$p_1(x_1)$'</span>)</span>
<span id="cb64-30"><a href="#cb64-30" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb64-31"><a href="#cb64-31" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">3</span>].hist(samples[burnin:,<span class="dv">1</span>], <span class="op">**</span>kw_hist)</span>
<span id="cb64-32"><a href="#cb64-32" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">3</span>].plot(y, np.exp(compute_marginal(prob, <span class="dv">0</span>, y)), color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb64-33"><a href="#cb64-33" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">3</span>].set_xlabel(<span class="vs">r'$x_2$'</span>)</span>
<span id="cb64-34"><a href="#cb64-34" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">3</span>].set_ylabel(<span class="vs">r'$p_2(x_2)$'</span>)</span>
<span id="cb64-35"><a href="#cb64-35" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb64-36"><a href="#cb64-36" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-54-output-1.png" width="920" height="922"></p>
</div>
</div>
<p>This collapsed Gibbs sampler uses the marginal distribution to sample <span class="math inline">\(x_2\)</span> and the conditional distribution to sample <span class="math inline">\(x_1\)</span>:</p>
<div class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="co"># collapsed Gibbs 2</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> [(<span class="fl">20.</span>, <span class="fl">90.</span>)]</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="bu">len</span>(samples) <span class="op">&lt;</span> <span class="fl">1e4</span>:</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>    newstate <span class="op">=</span> <span class="bu">list</span>(samples[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>    newstate[<span class="dv">1</span>] <span class="op">=</span> pdf.sample_marginal(<span class="dv">1</span>)</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>    newstate[<span class="dv">0</span>] <span class="op">=</span> pdf.sample_conditional(newstate, <span class="dv">0</span>)</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>    samples.append(<span class="bu">tuple</span>(newstate))</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> np.array(samples)</span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a>logp <span class="op">=</span> pdf.log_prob(samples)</span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a>burnin <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.1</span><span class="op">*</span><span class="bu">len</span>(samples))</span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb65-15"><a href="#cb65-15" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> <span class="bu">list</span>(ax.flat)</span>
<span id="cb65-16"><a href="#cb65-16" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb65-17"><a href="#cb65-17" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].scatter(<span class="op">*</span>samples.T, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb65-18"><a href="#cb65-18" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].contour(x, y, np.exp(prob))</span>
<span id="cb65-19"><a href="#cb65-19" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="vs">r'$x_1$'</span>)</span>
<span id="cb65-20"><a href="#cb65-20" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="vs">r'$x_2$'</span>)</span>
<span id="cb65-21"><a href="#cb65-21" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb65-22"><a href="#cb65-22" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(logp[:<span class="dv">50</span>], color<span class="op">=</span><span class="st">'k'</span>, lw<span class="op">=</span><span class="dv">3</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb65-23"><a href="#cb65-23" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="vs">r'iteration $s$'</span>)</span>
<span id="cb65-24"><a href="#cb65-24" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="vs">r'$\log p(x_1^{(s)}, x_2^{(s)})$'</span>)</span>
<span id="cb65-25"><a href="#cb65-25" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb65-26"><a href="#cb65-26" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].hist(samples[burnin:,<span class="dv">0</span>], <span class="op">**</span>kw_hist)</span>
<span id="cb65-27"><a href="#cb65-27" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].plot(x, np.exp(compute_marginal(prob, <span class="dv">1</span>, x)), color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb65-28"><a href="#cb65-28" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_xlabel(<span class="vs">r'$x_1$'</span>)</span>
<span id="cb65-29"><a href="#cb65-29" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_ylabel(<span class="vs">r'$p_1(x_1)$'</span>)</span>
<span id="cb65-30"><a href="#cb65-30" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb65-31"><a href="#cb65-31" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">3</span>].hist(samples[burnin:,<span class="dv">1</span>], <span class="op">**</span>kw_hist)</span>
<span id="cb65-32"><a href="#cb65-32" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">3</span>].plot(y, np.exp(compute_marginal(prob, <span class="dv">0</span>, y)), color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb65-33"><a href="#cb65-33" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">3</span>].set_xlabel(<span class="vs">r'$x_2$'</span>)</span>
<span id="cb65-34"><a href="#cb65-34" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">3</span>].set_ylabel(<span class="vs">r'$p_2(x_2)$'</span>)</span>
<span id="cb65-35"><a href="#cb65-35" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb65-36"><a href="#cb65-36" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-55-output-1.png" width="922" height="922"></p>
</div>
</div>
</section>
</section>
</section>
<section id="auxiliary-variables" class="level2">
<h2 class="anchored" data-anchor-id="auxiliary-variables">Auxiliary Variables</h2>
<p>The idea of sampling methods that use <em>auxiliary variables</em> is to <em>introduce</em> new variables rather than marginalizing them out. The target distribution is <span class="math inline">\(p(x)\)</span> defined over sample space <span class="math inline">\(\mathcal X\)</span>. But it might be beneficial to introduce helper variables <span class="math inline">\(y\)</span> and consider <span class="math inline">\(p(x, y)\)</span> defined over the extended sample space <span class="math inline">\(\mathcal X \times{} \mathcal Y\)</span> where</p>
<p><span class="math display">\[
p(x) = \int_{\mathcal{Y}} p(x, y)\, dy
\]</span></p>
<p>If we can generate samples <span class="math inline">\(\bigl(x^{(s)}, y^{(s)}\bigr) \sim p(x, y)\)</span>, then a valid estimator for expectations of <span class="math inline">\(p(x)\)</span> is</p>
<p><span class="math display">\[
\mathbb{E}_p[f] \approx \frac{1}{S} \sum_{s=1}^S f\bigl(x^{(s)}\bigr)\, .
\]</span></p>
<p>Why is this helpful? We can use Gibbs sampling to generate samples from <span class="math inline">\(p(x, y)\)</span>:</p>
<p><span class="math display">\[\begin{align}\label{eq-gibbs-auxiliary}
\begin{split}
x^{(s+1)} &amp;\sim p\bigl(x \mid{} y^{(s)}\bigr) \\
y^{(s+1)} &amp;\sim p\bigl(y \mid{} x^{(s+1)}\bigr) \\
\end{split}
\end{align}\]</span></p>
<p>where the marginal distributions might be easier to simulate than <span class="math inline">\(p(x)\)</span>.</p>
<section id="example-student-t-distribution" class="level4">
<h4 class="anchored" data-anchor-id="example-student-t-distribution">Example: Student-t distribution</h4>
<p>The <a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution">Student-t distribution</a> is defined as</p>
<p><span class="math display">\[
p(x \mid{} \nu) = \frac{1}{Z(\nu)} \left(1 + \frac{x^2}{\nu}\right)^{-\frac{\nu+1}{2}}
\]</span></p>
<p>with a normalization constant <span class="math inline">\(Z(\nu)\)</span> that depends on the degrees of freedom <span class="math inline">\(\nu &gt; 0\)</span>.</p>
<p>This integral can be written as a <a href="https://www.jstor.org/stable/2984774?seq=1#metadata_info_tab_contents">scale-mixture of normals</a>:</p>
<p><span class="math display">\[
\begin{aligned}
    f(x ; \alpha, \beta) &amp;= \int \underbrace{\sqrt{\frac{s}{2\pi}}\, e^{-\frac{s}{2} x^2 }}_{\text{Gaussian}}\,\,\, \underbrace{\frac{\beta^\alpha}{\Gamma(\alpha)} s^{\alpha -1} e^{-\beta s}}_{\text{Gamma distribution}} ds \\
    &amp;= \frac{\beta^\alpha}{\Gamma(\alpha)\,\sqrt{2\pi}} \int s^{\frac{2\alpha + 1}{2} - 1}\,\, \exp\left\{-s(\beta + x^2/2)\right\} ds \\
    &amp;= \frac{\beta^\alpha}{\Gamma(\alpha)\,\sqrt{2\pi}} \frac{\Gamma(\alpha+1/2)}{\left(\beta + x^2/2\right)^{\frac{2\alpha+1}{2}}} \\
    &amp;= \frac{1}{\sqrt{2\pi\beta}} \frac{\Gamma(\alpha+1/2)}{\Gamma(\alpha)} \left(1 + x^2/2\beta\right)^{-(\frac{2\alpha+1}{2})}
\end{aligned}
\]</span></p>
<p>So <span class="math inline">\(f(x; \alpha, \beta)\)</span> is identical to <span class="math inline">\(p(x\mid{}\nu)\)</span> for <span class="math inline">\(\alpha=\nu/2\)</span> and <span class="math inline">\(\beta=\nu/2\)</span>.</p>
<p>The joint distribution is</p>
<p><span class="math display">\[
p(x, s) = p(x \mid{} s) p(s) = \mathcal{N}(x; 0, s^{-1/2})\, \mathcal{G}(s; \nu/2, \nu/2)
\]</span></p>
<p>The conditional distributions are <span class="math inline">\(\mathcal{N}(x; 0, s^{-1/2})\)</span> and <span class="math inline">\(\mathcal{G}(s; (\nu+1)/2, (\nu+x^2)/2)\)</span>.</p>
<div class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sample student t with auxiliary variable</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>nu <span class="op">=</span> <span class="fl">2.</span></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> nu <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> nu <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>log_target <span class="op">=</span> <span class="kw">lambda</span> x : <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> (nu<span class="op">+</span><span class="fl">1.</span>) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">+</span> x<span class="op">**</span><span class="dv">2</span><span class="op">/</span>nu)</span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a><span class="co"># standard Gibbs sampler</span></span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> []</span>
<span id="cb66-12"><a href="#cb66-12" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="bu">len</span>(samples) <span class="op">&lt;</span> <span class="fl">1e5</span>:</span>
<span id="cb66-13"><a href="#cb66-13" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> np.random.gamma(alpha<span class="op">+</span><span class="fl">0.5</span>, <span class="dv">1</span><span class="op">/</span>(beta <span class="op">+</span> <span class="fl">0.5</span><span class="op">*</span>x<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb66-14"><a href="#cb66-14" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.random.standard_normal() <span class="op">/</span> np.sqrt(s)</span>
<span id="cb66-15"><a href="#cb66-15" aria-hidden="true" tabindex="-1"></a>    samples.append(x)</span>
<span id="cb66-16"><a href="#cb66-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb66-17"><a href="#cb66-17" aria-hidden="true" tabindex="-1"></a><span class="co"># collapsed Gibbs sampler</span></span>
<span id="cb66-18"><a href="#cb66-18" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> np.random.gamma(alpha, <span class="dv">1</span><span class="op">/</span>beta, size<span class="op">=</span><span class="bu">len</span>(samples))</span>
<span id="cb66-19"><a href="#cb66-19" aria-hidden="true" tabindex="-1"></a>samples2 <span class="op">=</span> np.random.standard_normal(<span class="bu">len</span>(s)) <span class="op">/</span> np.sqrt(s)</span>
<span id="cb66-20"><a href="#cb66-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb66-21"><a href="#cb66-21" aria-hidden="true" tabindex="-1"></a>burnin <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.1</span><span class="op">*</span><span class="bu">len</span>(samples))</span>
<span id="cb66-22"><a href="#cb66-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb66-23"><a href="#cb66-23" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>, <span class="dv">1000</span>) <span class="op">*</span> <span class="dv">10</span></span>
<span id="cb66-24"><a href="#cb66-24" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> log_target(t)</span>
<span id="cb66-25"><a href="#cb66-25" aria-hidden="true" tabindex="-1"></a>p <span class="op">-=</span> logsumexp(p) <span class="op">+</span> np.log(t[<span class="dv">1</span>]<span class="op">-</span>t[<span class="dv">0</span>])</span>
<span id="cb66-26"><a href="#cb66-26" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> np.exp(p)</span>
<span id="cb66-27"><a href="#cb66-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-28"><a href="#cb66-28" aria-hidden="true" tabindex="-1"></a>kw <span class="op">=</span> <span class="bu">dict</span>(xlim<span class="op">=</span>(<span class="op">-</span><span class="fl">10.</span>, <span class="fl">10.</span>))</span>
<span id="cb66-29"><a href="#cb66-29" aria-hidden="true" tabindex="-1"></a>kw_hist <span class="op">=</span> <span class="bu">dict</span>(bins<span class="op">=</span><span class="dv">1000</span>, density<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb66-30"><a href="#cb66-30" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>), sharey<span class="op">=</span><span class="st">'all'</span>, subplot_kw<span class="op">=</span>kw)</span>
<span id="cb66-31"><a href="#cb66-31" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].hist(samples[burnin:], <span class="op">**</span>kw_hist)</span>
<span id="cb66-32"><a href="#cb66-32" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].hist(samples2[burnin:], <span class="op">**</span>kw_hist)</span>
<span id="cb66-33"><a href="#cb66-33" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a <span class="kw">in</span> ax:</span>
<span id="cb66-34"><a href="#cb66-34" aria-hidden="true" tabindex="-1"></a>    a.plot(t, p, color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb66-35"><a href="#cb66-35" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-56-output-1.png" width="730" height="346"></p>
</div>
</div>
</section>
</section>
</section>
<section id="lecture-7-hamiltonian-monte-carlo-1" class="level1">
<h1>Lecture 7: Hamiltonian Monte Carlo</h1>
<section id="outline-6" class="level2">
<h2 class="anchored" data-anchor-id="outline-6">Outline</h2>
<ul>
<li>Recap: MCMC + Gibbs Sampling</li>
<li>More on auxiliary variable methods</li>
<li>Hamiltonian Monte Carlo</li>
</ul>
</section>
<section id="recap-3" class="level2">
<h2 class="anchored" data-anchor-id="recap-3">Recap</h2>
<ul>
<li><p><strong>Metropolis-Hastings algorithm:</strong> Take (almost) any Markov chain <span class="math inline">\(Q\)</span> (proposal chain) and map it to a new Markov chain <span class="math inline">\(M[Q]\)</span> with a desired proposal distribution <span class="math inline">\(p\)</span>: <span class="math display">\[
M[Q](y, x) = Q(y, x) \, \min\left\{1, \frac{Q(x, y)}{Q(y, x)} \frac{p(y)}{p(x)} \right\},\,\,\, y\not= x
\]</span></p></li>
<li><p><strong>Metropolis-within-Gibbs:</strong> Multiple proposal kernels <span class="math inline">\(P_i\)</span> that have the same stationary distribution <span class="math inline">\(p\)</span> can be combined to produce a new kernel <span class="math inline">\(P=\prod_i P_i\)</span> with the same stationary distribution. A special case is to use proposal chains <span class="math inline">\(Q_i\)</span> that only update a group of variables <span class="math inline">\(x_i\)</span> and leave the other variables <span class="math inline">\(x_{\setminus i}\)</span> untouched: <span class="math display">\[
Q_i(y, x) = \delta(y_{\setminus i} - x_{\setminus i})\, q_i(y_i, x_i; x_{\setminus i})
\]</span> The Metropolis map of <span class="math inline">\(Q_i\)</span> simulates a Markov chain in the subspace <span class="math inline">\(\mathcal X_i\)</span> with proposal kernel <span class="math inline">\(q_i\)</span>.</p></li>
<li><p><strong>Gibbs sampling:</strong> A special case is <span class="math inline">\(q_i(y_i, x_i; x_{\setminus i}) = p_i(y_i\mid{}x_{\setminus i})\)</span> (the proposal is the conditional distribution <span class="math inline">\(p_i\)</span> of the <span class="math inline">\(i\)</span>-th group of variables), which results in proposals that are always accepted.</p></li>
</ul>
</section>
<section id="auxiliary-variable-methods" class="level2">
<h2 class="anchored" data-anchor-id="auxiliary-variable-methods">Auxiliary variable methods</h2>
<p>The idea of auxiliary variable methods is to introduce helper variables <span class="math inline">\(y\in\mathcal Y\)</span> that facilitate sampling. The joint distribution <span class="math inline">\(p(x, y)\)</span> over the extended sample space <span class="math inline">\(\mathcal X \times{} \mathcal Y\)</span> has to be designed such that</p>
<p><span class="math display">\[
p(x) = \int_{\mathcal{Y}} p(x, y)\, dy\, .
\]</span></p>
<p>Samples <span class="math inline">\(\bigl(x^{(s)}, y^{(s)}\bigr) \sim p(x, y)\)</span> can then be used to estimate expectations with respect to <span class="math inline">\(p(x)\)</span>:</p>
<p><span class="math display">\[
\mathbb{E}_p[f] = \int_{\mathcal X} f(x)\, p(x)\, dx = \int_{\mathcal X \times \mathcal Y} f(x)\, p(x, y)\, dx dy \approx \frac{1}{S} \sum_{s=1}^S f\bigl(x^{(s)}\bigr)\, .
\]</span></p>
<p>Why is this helpful? We can use Gibbs sampling to generate samples from <span class="math inline">\(p(x, y)\)</span>:</p>
<p><span class="math display">\[\begin{align}\label{eq-gibbs-auxiliary2}
\begin{split}
x^{(s+1)} &amp;\sim p\bigl(x \mid{} y^{(s)}\bigr) \\
y^{(s+1)} &amp;\sim p\bigl(y \mid{} x^{(s+1)}\bigr) \\
\end{split}
\end{align}\]</span></p>
<p>where the marginal distributions might be easier to simulate than <span class="math inline">\(p(x)\)</span>.</p>
<section id="example-swendsen-wang-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="example-swendsen-wang-algorithm">Example: Swendsen-Wang algorithm</h3>
<p>An example of an auxiliary variable method is the <a href="https://en.wikipedia.org/wiki/Swendsen%E2%80%93Wang_algorithm">Swendsen-Wang algorithm</a> for sampling Ising models. The Ising model (see lecture 1) is defined on a two-dimensional square lattice of length <span class="math inline">\(L\)</span>. The sample space is <span class="math inline">\(\mathcal X = \{-1, +1\}^{L \times L}\)</span>. The probability is</p>
<p><span class="math display">\[
p(x) \propto \exp\left\{\beta \sum_{i\sim j} x_i x_j \right\}
\]</span></p>
<p>where <span class="math inline">\(i\sim j\)</span> indicates if two nodes <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are nearest neighbors on the square lattice (assuming periodic boundary conditions). In the first lecture, we saw that it is challenging to simulate the Ising model for <span class="math inline">\(\beta \simeq 0.44\)</span>, because of a phase transition: For <span class="math inline">\(\beta\)</span> larger than the critical value, spin variables become highly correlated which makes the sampling quite challenging.</p>
<p>The sampling strategy used in lecture 1 can be interpreted as “Metropolis-within-Gibbs”: The entire 2D lattice is scanned (a loop over all lattice sites is called a <em>sweep</em>). At each lattice site, we try to flip the spin: If <span class="math inline">\(x_i\)</span> is the current value of spin <span class="math inline">\(i\)</span>, then the flipped value is <span class="math inline">\(x_i'=-x_i\)</span>; all other spin variables are unchanged. That is, the proposal kernel <span class="math inline">\(Q_i\)</span> only affects a single site and is symmetric and deterministic. A spin flip is accepted with probability:</p>
<p><span class="math display">\[
\frac{p(x')}{p(x)} = \exp\left\{\beta\sum_{i\sim j} (x_i'x_j' - x_i x_j) \right\} = \exp\left\{-2\beta x_i S_i \right\}
\]</span></p>
<p>where <span class="math inline">\(S_i = \sum_{j\sim i} x_j\)</span> is the total spin of the nearest neighbors, which can be evaluated very efficiently.</p>
<div class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext Cython</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The Cython extension is already loaded. To reload it, use:
  %reload_ext Cython</code></pre>
</div>
</div>
<div class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>cython</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>cimport cython</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>cimport numpy <span class="im">as</span> np</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pylab <span class="im">as</span> plt</span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> libc.math cimport exp</span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> libc.stdlib cimport rand</span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a>cdef extern <span class="im">from</span> <span class="st">"limits.h"</span>:</span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> RAND_MAX</span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-15"><a href="#cb69-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-16"><a href="#cb69-16" aria-hidden="true" tabindex="-1"></a><span class="at">@cython.boundscheck</span>(<span class="va">False</span>)</span>
<span id="cb69-17"><a href="#cb69-17" aria-hidden="true" tabindex="-1"></a><span class="at">@cython.wraparound</span>(<span class="va">False</span>)</span>
<span id="cb69-18"><a href="#cb69-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ising_energy(np.int64_t[:, :] x):</span>
<span id="cb69-19"><a href="#cb69-19" aria-hidden="true" tabindex="-1"></a>    cdef <span class="bu">int</span> N <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb69-20"><a href="#cb69-20" aria-hidden="true" tabindex="-1"></a>    cdef <span class="bu">int</span> M <span class="op">=</span> x.shape[<span class="dv">1</span>]</span>
<span id="cb69-21"><a href="#cb69-21" aria-hidden="true" tabindex="-1"></a>    cdef <span class="bu">int</span> E <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb69-22"><a href="#cb69-22" aria-hidden="true" tabindex="-1"></a>    cdef <span class="bu">int</span> i, j</span>
<span id="cb69-23"><a href="#cb69-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb69-24"><a href="#cb69-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(M):</span>
<span id="cb69-25"><a href="#cb69-25" aria-hidden="true" tabindex="-1"></a>            E <span class="op">-=</span> x[i,j] <span class="op">*</span> (x[i,(j<span class="op">+</span><span class="dv">1</span>)<span class="op">%</span>M] <span class="op">+</span> x[(i<span class="op">+</span><span class="dv">1</span>)<span class="op">%</span>N, j])</span>
<span id="cb69-26"><a href="#cb69-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> E</span>
<span id="cb69-27"><a href="#cb69-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-28"><a href="#cb69-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-29"><a href="#cb69-29" aria-hidden="true" tabindex="-1"></a><span class="at">@cython.boundscheck</span>(<span class="va">False</span>)</span>
<span id="cb69-30"><a href="#cb69-30" aria-hidden="true" tabindex="-1"></a><span class="at">@cython.wraparound</span>(<span class="va">False</span>)</span>
<span id="cb69-31"><a href="#cb69-31" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ising_sweep(np.int64_t[:, :] x, <span class="bu">float</span> beta<span class="op">=</span><span class="fl">0.4</span>):</span>
<span id="cb69-32"><a href="#cb69-32" aria-hidden="true" tabindex="-1"></a>    cdef <span class="bu">int</span> N <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb69-33"><a href="#cb69-33" aria-hidden="true" tabindex="-1"></a>    cdef <span class="bu">int</span> M <span class="op">=</span> x.shape[<span class="dv">1</span>]</span>
<span id="cb69-34"><a href="#cb69-34" aria-hidden="true" tabindex="-1"></a>    cdef <span class="bu">int</span> start_i, start_j, i, j</span>
<span id="cb69-35"><a href="#cb69-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> start_i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb69-36"><a href="#cb69-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> start_j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb69-37"><a href="#cb69-37" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(start_i, N, <span class="dv">2</span>):</span>
<span id="cb69-38"><a href="#cb69-38" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(start_j, M, <span class="dv">2</span>):</span>
<span id="cb69-39"><a href="#cb69-39" aria-hidden="true" tabindex="-1"></a>                    ising_flip(x, i, j, beta)</span>
<span id="cb69-40"><a href="#cb69-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(x)</span>
<span id="cb69-41"><a href="#cb69-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-42"><a href="#cb69-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-43"><a href="#cb69-43" aria-hidden="true" tabindex="-1"></a><span class="at">@cython.boundscheck</span>(<span class="va">False</span>)</span>
<span id="cb69-44"><a href="#cb69-44" aria-hidden="true" tabindex="-1"></a><span class="at">@cython.wraparound</span>(<span class="va">False</span>)</span>
<span id="cb69-45"><a href="#cb69-45" aria-hidden="true" tabindex="-1"></a>cdef ising_flip(np.int64_t[:, :] x, <span class="bu">int</span> i, <span class="bu">int</span> j, <span class="bu">float</span> beta):</span>
<span id="cb69-46"><a href="#cb69-46" aria-hidden="true" tabindex="-1"></a>    cdef <span class="bu">int</span> N <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb69-47"><a href="#cb69-47" aria-hidden="true" tabindex="-1"></a>    cdef <span class="bu">int</span> M <span class="op">=</span> x.shape[<span class="dv">1</span>]</span>
<span id="cb69-48"><a href="#cb69-48" aria-hidden="true" tabindex="-1"></a>    cdef <span class="bu">int</span> S <span class="op">=</span> x[(i<span class="op">-</span><span class="dv">1</span>)<span class="op">%</span>N,j] <span class="op">+</span> x[(i<span class="op">+</span><span class="dv">1</span>)<span class="op">%</span>N,j] <span class="op">+</span> x[i,(j<span class="op">-</span><span class="dv">1</span>)<span class="op">%</span>M] <span class="op">+</span> x[i,(j<span class="op">+</span><span class="dv">1</span>)<span class="op">%</span>M]</span>
<span id="cb69-49"><a href="#cb69-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> exp(<span class="op">-</span><span class="dv">2</span> <span class="op">*</span> beta <span class="op">*</span> x[i, j] <span class="op">*</span> S) <span class="op">*</span> RAND_MAX <span class="op">&gt;</span> rand():</span>
<span id="cb69-50"><a href="#cb69-50" aria-hidden="true" tabindex="-1"></a>        x[i, j] <span class="op">*=</span> <span class="op">-</span><span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>In file included from /opt/hostedtoolcache/Python/3.10.9/x64/lib/python3.10/site-packages/numpy/core/include/numpy/ndarraytypes.h:1948,
                 from /opt/hostedtoolcache/Python/3.10.9/x64/lib/python3.10/site-packages/numpy/core/include/numpy/ndarrayobject.h:12,
                 from /opt/hostedtoolcache/Python/3.10.9/x64/lib/python3.10/site-packages/numpy/core/include/numpy/arrayobject.h:5,
                 from /home/runner/.cache/ipython/cython/_cython_magic_437ff226c04ce7b8553a40470de4e368.c:770:
/opt/hostedtoolcache/Python/3.10.9/x64/lib/python3.10/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning "Using deprecated NumPy API, disable it with " "#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
   17 | #warning "Using deprecated NumPy API, disable it with " \
      |  ^~~~~~~</code></pre>
</div>
</div>
<p>To improve the sampling of spin configuration, we introduce auxiliary variables <span class="math inline">\(b_{ij} \in \{0, 1\}\)</span>, also called <em>bond variables</em>, for each pair of spins <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> that are neighbors on the square lattice: <span class="math inline">\(i\sim j\)</span>. The bond variables indicate if two neighboring spins <span class="math inline">\(x_i\)</span>, <span class="math inline">\(x_j\)</span> are aligned (i.e.&nbsp;have the same spin or color):</p>
<p><span class="math display">\[
b_{ij} = \left\{\begin{array}{c c} 1 &amp; \text{if } x_i = x_j \\ 0 &amp; \text{else} \\
\end{array}\right.
\]</span></p>
<p>The bond variables can be seen as Bernoulli coins that are tossed independently of each other. For spins that are aligned, the probability for forming a bond between sites <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, <span class="math inline">\(b_{ij}=1\)</span>, is <span class="math inline">\(p=1-e^{-2\beta}\)</span>. So there is a bias for <span class="math inline">\(b_{ij}\)</span> to form that increases with increasing inverse temperature <span class="math inline">\(\beta\)</span>. For spins that are not aligned, the associated bond is <em>not</em> formed with probability one. The overall probability is</p>
<p><span class="math display">\[
p(x, b) \propto \prod_{i\sim j} [p\delta(x_i, x_j)]^{b_{ij}} (1-p)^{1-b_{ij}}
\]</span></p>
<p>where <span class="math inline">\(\delta(x_i, x_j)=1\)</span> if <span class="math inline">\(x_i=x_j\)</span> and <span class="math inline">\(\delta(x_i,x_j)=0\)</span> otherwise. For the Ising model with <span class="math inline">\(x_i=\pm 1\)</span>, we can write</p>
<p><span class="math display">\[
\delta(x_i, x_j) = \frac{x_ix_j + 1}{2}\, .
\]</span></p>
<p>To verify that the marginal distribution over <span class="math inline">\(x\)</span> is correct, let us compute it by summing over <span class="math inline">\(b_{ij}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\sum_{b} p(x, b) &amp;\propto \sum_{b} \prod_{i\sim j} [p\delta(x_i, x_j)]^{b_{ij}} (1-p)^{1-b_{ij}}\\
&amp;= \prod_{i\sim j} [p\delta(x_i, x_j) + 1 - p] \\
&amp;= \prod_{i\sim j} [1-p]^{1-\delta(x_i, x_j)} \\
&amp;= \prod_{i\sim j} \exp\left\{-2\beta(1-\delta(x_i, x_j))\right\} \\
&amp;= \prod_{i\sim j} \exp\left\{-\beta(1-x_ix_j)\right\} \\
&amp;\propto \prod_{i\sim j} \exp\left\{\beta x_ix_j\right\} \\
\end{align*}\]</span></p>
<p>The conditional distribution of a single bond variable <span class="math inline">\(b_{ij}\)</span> is</p>
<p><span class="math display">\[
p(b_{ij} \mid{} x) \propto [p\delta(x_i, x_j)]^{b_{ij}} (1-p)^{1-b_{ij}}
\]</span></p>
<p>These can be updated by simply generating Bernoulli variables for aligned spins, the other bond variables are set to zero.</p>
<p>The conditional distribution of the spins is sampled by assigning all spin variables that belong to the same <a href="https://en.wikipedia.org/wiki/Component_(graph_theory)">connected component</a> of the bond network to a single, randomly selected value <span class="math inline">\(\pm 1\)</span>.</p>
<div class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pure Python implementation of Swendsen-Wang</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.sparse <span class="im">as</span> sparse</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SwendsenWang:</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, L):</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.L <span class="op">=</span> <span class="bu">int</span>(L)</span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># create edges</span></span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>        iy, ix <span class="op">=</span> np.meshgrid(np.arange(L), np.arange(L))</span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a>        i <span class="op">=</span> np.arange(L<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a>        i <span class="op">=</span> np.concatenate([i, i])</span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a>        j <span class="op">=</span> [(L<span class="op">*</span>ix <span class="op">+</span> (iy<span class="op">+</span><span class="dv">1</span>)<span class="op">%</span>L).flatten(), (L<span class="op">*</span>((ix<span class="op">+</span><span class="dv">1</span>)<span class="op">%</span>L) <span class="op">+</span> iy).flatten()]</span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a>        j <span class="op">=</span> np.concatenate(j)</span>
<span id="cb71-16"><a href="#cb71-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-17"><a href="#cb71-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.edges <span class="op">=</span> np.sort([i, j], <span class="dv">0</span>)</span>
<span id="cb71-18"><a href="#cb71-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-19"><a href="#cb71-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> adjacency_matrix(<span class="va">self</span>, bonds<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb71-20"><a href="#cb71-20" aria-hidden="true" tabindex="-1"></a>        i, j <span class="op">=</span> <span class="va">self</span>.edges</span>
<span id="cb71-21"><a href="#cb71-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> bonds <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb71-22"><a href="#cb71-22" aria-hidden="true" tabindex="-1"></a>            i, j <span class="op">=</span> i[bonds], j[bonds]               </span>
<span id="cb71-23"><a href="#cb71-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> sparse.csr_matrix(</span>
<span id="cb71-24"><a href="#cb71-24" aria-hidden="true" tabindex="-1"></a>            (np.ones_like(i), (i, j)), shape<span class="op">=</span>(<span class="va">self</span>.L<span class="op">**</span><span class="dv">2</span>, <span class="va">self</span>.L<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb71-25"><a href="#cb71-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-26"><a href="#cb71-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample_bonds(<span class="va">self</span>, x, beta<span class="op">=</span><span class="fl">1.</span>):</span>
<span id="cb71-27"><a href="#cb71-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.reshape(<span class="op">-</span><span class="dv">1</span>,)</span>
<span id="cb71-28"><a href="#cb71-28" aria-hidden="true" tabindex="-1"></a>        i, j <span class="op">=</span> <span class="va">self</span>.edges</span>
<span id="cb71-29"><a href="#cb71-29" aria-hidden="true" tabindex="-1"></a>        aligned <span class="op">=</span> (x[i] <span class="op">==</span> x[j])</span>
<span id="cb71-30"><a href="#cb71-30" aria-hidden="true" tabindex="-1"></a>        prob <span class="op">=</span> aligned <span class="op">*</span> (<span class="dv">1</span><span class="op">-</span>np.exp(<span class="op">-</span><span class="dv">2</span><span class="op">*</span>beta)) </span>
<span id="cb71-31"><a href="#cb71-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.random.random(<span class="bu">len</span>(prob)) <span class="op">&lt;</span> prob</span>
<span id="cb71-32"><a href="#cb71-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-33"><a href="#cb71-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample_spins(<span class="va">self</span>, x, beta<span class="op">=</span><span class="fl">0.</span>):</span>
<span id="cb71-34"><a href="#cb71-34" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.copy().flatten()    </span>
<span id="cb71-35"><a href="#cb71-35" aria-hidden="true" tabindex="-1"></a>        bonds <span class="op">=</span> <span class="va">self</span>.sample_bonds(x, beta)</span>
<span id="cb71-36"><a href="#cb71-36" aria-hidden="true" tabindex="-1"></a>        adjacency <span class="op">=</span> <span class="va">self</span>.adjacency_matrix(bonds)</span>
<span id="cb71-37"><a href="#cb71-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb71-38"><a href="#cb71-38" aria-hidden="true" tabindex="-1"></a>        n_comp, labels <span class="op">=</span> sparse.csgraph.connected_components(</span>
<span id="cb71-39"><a href="#cb71-39" aria-hidden="true" tabindex="-1"></a>            adjacency, directed<span class="op">=</span><span class="va">False</span>, return_labels<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb71-40"><a href="#cb71-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-41"><a href="#cb71-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> label <span class="kw">in</span> <span class="bu">range</span>(n_comp):</span>
<span id="cb71-42"><a href="#cb71-42" aria-hidden="true" tabindex="-1"></a>            x[labels<span class="op">==</span>label] <span class="op">=</span> np.random.choice([<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb71-43"><a href="#cb71-43" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb71-44"><a href="#cb71-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x.reshape(<span class="va">self</span>.L, <span class="op">-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="co"># For comparison: mean energy per spin in infinite lattice from Onsager theory</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="co"># https://en.wikipedia.org/wiki/Square_lattice_Ising_model#Exact_solution</span></span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> integrate</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> energy_per_spin(beta):</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Average energy per lattice site according to Onsager.</span></span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> np.iterable(beta):</span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.array(<span class="bu">list</span>(<span class="bu">map</span>(energy_per_spin, beta)))</span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># scalar beta</span></span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> f(theta):</span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> np.sinh(<span class="dv">2</span><span class="op">*</span>beta)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb72-15"><a href="#cb72-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> np.sqrt(<span class="dv">1</span> <span class="op">-</span> <span class="dv">4</span><span class="op">*</span>k<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>k)<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> np.sin(theta)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb72-16"><a href="#cb72-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-17"><a href="#cb72-17" aria-hidden="true" tabindex="-1"></a>    I <span class="op">=</span> integrate.quad(f, <span class="fl">0.</span>, <span class="fl">0.5</span><span class="op">*</span>np.pi)[<span class="dv">0</span>]</span>
<span id="cb72-18"><a href="#cb72-18" aria-hidden="true" tabindex="-1"></a>    I <span class="op">*=</span> <span class="dv">2</span> <span class="op">*</span> (<span class="dv">2</span><span class="op">*</span>np.tanh(<span class="dv">2</span><span class="op">*</span>beta)<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">1</span>) <span class="op">/</span> np.pi</span>
<span id="cb72-19"><a href="#cb72-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span> (<span class="dv">1</span> <span class="op">+</span> I) <span class="op">/</span> np.tanh(<span class="dv">2</span><span class="op">*</span>beta)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> <span class="dv">2</span><span class="op">**</span><span class="dv">7</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.choice([<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>], (L, L))</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a><span class="co"># inverse temperature close to critical value</span></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">+</span> <span class="dv">2</span><span class="op">**</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>))</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>n_iter <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a><span class="co"># starting from random configuration (beta=0.) using Metropolis</span></span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a><span class="co"># algorithm to approach distribution at critical beta</span></span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>x_MH <span class="op">=</span> x.copy()</span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>E_MH <span class="op">=</span> [ising_energy(x_MH)<span class="op">/</span>L<span class="op">**</span><span class="dv">2</span>]</span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_iter):</span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a>    x_MH <span class="op">=</span> ising_sweep(x_MH)</span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a>    E_MH.append(ising_energy(x_MH)<span class="op">/</span>L<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb73-15"><a href="#cb73-15" aria-hidden="true" tabindex="-1"></a>E_MH <span class="op">=</span> np.array(E_MH)</span>
<span id="cb73-16"><a href="#cb73-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-17"><a href="#cb73-17" aria-hidden="true" tabindex="-1"></a><span class="co"># starting from random configuration (beta=0.) using Swendsen-</span></span>
<span id="cb73-18"><a href="#cb73-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Wang to approach distribution at critical beta</span></span>
<span id="cb73-19"><a href="#cb73-19" aria-hidden="true" tabindex="-1"></a>sampler <span class="op">=</span> SwendsenWang(L)</span>
<span id="cb73-20"><a href="#cb73-20" aria-hidden="true" tabindex="-1"></a>x_SW <span class="op">=</span> x.copy()</span>
<span id="cb73-21"><a href="#cb73-21" aria-hidden="true" tabindex="-1"></a>E_SW <span class="op">=</span> [ising_energy(x_SW)<span class="op">/</span>L<span class="op">**</span><span class="dv">2</span>]</span>
<span id="cb73-22"><a href="#cb73-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_iter):</span>
<span id="cb73-23"><a href="#cb73-23" aria-hidden="true" tabindex="-1"></a>    x_SW <span class="op">=</span> sampler.sample_spins(x_SW, beta)</span>
<span id="cb73-24"><a href="#cb73-24" aria-hidden="true" tabindex="-1"></a>    E_SW.append(ising_energy(x_SW)<span class="op">/</span>L<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb73-25"><a href="#cb73-25" aria-hidden="true" tabindex="-1"></a>E_SW <span class="op">=</span> np.array(E_SW)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>betas <span class="op">=</span> [np.linspace(<span class="fl">1e-3</span>, <span class="fl">1.</span>, <span class="dv">101</span>), </span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>         np.linspace(<span class="fl">0.2</span>, <span class="fl">0.8</span>, <span class="dv">101</span>)][<span class="dv">0</span>]</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>U <span class="op">=</span> energy_per_spin(betas)</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>plt.rc(<span class="st">'font'</span>, size<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>), sharey<span class="op">=</span><span class="st">'all'</span>)</span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> <span class="bu">list</span>(ax.flat)</span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(E_MH, lw<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'Metropolis'</span>)</span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(E_SW, lw<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'Swendsen-Wang'</span>)</span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="vs">r'iteration $s$'</span>)</span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="vs">r'energy per spin $E/L^2$'</span>)</span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].legend()</span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].scatter(beta, E_MH[<span class="op">-</span><span class="dv">1</span>], s<span class="op">=</span><span class="dv">200</span>, label<span class="op">=</span><span class="st">'Metropolis'</span>)</span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].scatter(beta, E_SW[<span class="op">-</span><span class="dv">1</span>], s<span class="op">=</span><span class="dv">200</span>, label<span class="op">=</span><span class="st">'Swendsen-Wang'</span>)</span>
<span id="cb74-15"><a href="#cb74-15" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(betas, U, color<span class="op">=</span><span class="st">'r'</span>, ls<span class="op">=</span><span class="st">'--'</span>, lw<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'Onsager'</span>)</span>
<span id="cb74-16"><a href="#cb74-16" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="vs">r'inverse temperature $\beta$'</span>)</span>
<span id="cb74-17"><a href="#cb74-17" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].legend()</span>
<span id="cb74-18"><a href="#cb74-18" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-62-output-1.png" width="921" height="453"></p>
</div>
</div>
<div class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>plt.rc(<span class="st">'image'</span>, interpolation<span class="op">=</span><span class="va">None</span>, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>kw <span class="op">=</span> <span class="bu">dict</span>(xticks<span class="op">=</span>[], yticks<span class="op">=</span>[])</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>), subplot_kw<span class="op">=</span>kw)</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'Metropolis'</span>)</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].imshow(x_MH.reshape(L, L))</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'Swendsen-Wang'</span>)</span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].imshow(x_SW.reshape(L, L))</span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_1901/3345378395.py:1: MatplotlibDeprecationWarning: Support for setting an rcParam that expects a str value to a non-str value is deprecated since 3.5 and support will be removed two minor releases later.
  plt.rc('image', interpolation=None, cmap='gray')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-63-output-2.png" width="914" height="473"></p>
</div>
</div>
</section>
</section>
<section id="hamiltonian-monte-carlo" class="level2">
<h2 class="anchored" data-anchor-id="hamiltonian-monte-carlo">Hamiltonian Monte Carlo</h2>
<p>Another auxiliary variable method is <a href="https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo">Hamiltonian Monte Carlo (HMC)</a> also known as <a href="https://doi.org/10.1016%2F0370-2693%2887%2991197-X"><em>Hybrid Monte Carlo</em></a>. HMC is among the most widely used methods for sampling probabilistic models over continuous sample spaces. Radford Neal, one of the inventors of HMC, has written a nice introduction that can be found <a href="http://www.mcmchandbook.net/HandbookChapter5.pdf">here</a>.</p>
<p>The idea of HMC is to exploit the following physical analogy: we interpret</p>
<p><span id="eq-energy-hmc"><span class="math display">\[
E(x) = - \log p(x)
\tag{75}\]</span></span></p>
<p>as a potential energy function of a physical system with degrees of freedom <span class="math inline">\(x\)</span>. Typically, <span class="math inline">\(x\in\mathbb{R}^D\)</span>.</p>
<p>We introduce auxiliary variables <span class="math inline">\(v\in\mathbb{R}^D\)</span> that follow a <span class="math inline">\(D\)</span>-dimensional standard Gaussian distribution:</p>
<p><span class="math display">\[
p(v) = (2\pi)^{-D/2} \exp\left\{-\|v\|^2 / 2 \right\}
\]</span></p>
<p>and construct the joint distribution:</p>
<p><span id="eq-hmc-joint"><span class="math display">\[
p(x, v) = p(x)\, p(v) \propto \exp\left\{- E(x) - \|v\|^2 / 2 \right\}\, .
\tag{76}\]</span></span></p>
<p>It seems that we didn’t gain anything by introducing <span class="math inline">\(v\)</span> other than artificially blowing up the problem and writing the joint distribution in some fancy, pseudo-physical way. However, the major insight comes from the fact that if we stretch the physical analogy further, the joint distribution can be viewed as the <a href="https://en.wikipedia.org/wiki/Canonical_ensemble">canonical ensemble</a> defined over <a href="https://en.wikipedia.org/wiki/Phase_space"><em>phase space</em></a>:</p>
<p><span id="eq-hmc-hamiltonian"><span class="math display">\[
p(x, v) \propto \exp\left\{- H(x, v)\right\}\,\,\,\text{where}\,\,\,  H(x, v) := \underbrace{\tfrac{1}{2} \|v\|^2}_{\text{kinetic energy}} + \underbrace{E(x)}_{\text{potential energy}}\, .
\tag{77}\]</span></span></p>
<p>Phase space is the joint space of positions <span class="math inline">\(x\)</span> and momenta (velocities) <span class="math inline">\(v\)</span> in our physical analogy, and the total energy is given by the sum of the kinetic and potential energy is called the <a href="https://en.wikipedia.org/wiki/Hamiltonian_mechanics"><em>Hamiltonian</em></a> of the system.</p>
<section id="hamiltonian-dynamics" class="level3">
<h3 class="anchored" data-anchor-id="hamiltonian-dynamics">Hamiltonian dynamics</h3>
<p>Classical systems with <span class="math inline">\(D\)</span> degrees of freedom evolve in time by the action of the Hamiltonian <span class="math inline">\(H(x, v)\)</span>. Trajectories in phase space are given by the time evolution of velocities and positions, <span class="math inline">\(v(t)\)</span> and <span class="math inline">\(x(t)\)</span>. The dynamics of the system is described by Hamilton’s equations of motion (an elegant generalization of Newton dynamics):</p>
<p><span class="math display">\[\begin{align}\label{eq-hmc-dynamics}
\begin{split}
\dot{v} &amp;= \frac{d}{dt} v = - \nabla_x H(x, v) \\
\dot{x} &amp;= \frac{d}{dt} x = + \nabla_v H(x, v) \\
\end{split}
\end{align}\]</span></p>
<p>where <span class="math inline">\(d/dt\)</span> is the derivative with respect to time <span class="math inline">\(t\)</span> and abbreviated by a dot (as in <span class="math inline">\(\dot{x}\)</span>), and <span class="math inline">\(\nabla_x, \nabla_v\)</span> are the gradients with respect to positions <span class="math inline">\(x\)</span> and momenta <span class="math inline">\(v\)</span>.</p>
<p>The hallmark of Hamiltonian dynamics (Eq. <span class="math inline">\(\ref{eq-hmc-dynamics}\)</span>) of an isolated classical system is that it conserves the total energy. This is clear from</p>
<p><span class="math display">\[
\frac{d}{dt} H = (\nabla_x H(x, v))^T\dot{x} + (\nabla_v H(x, v))^T\dot{v} = (\nabla_x H)^T\!(\nabla_v H) - (\nabla_v H)^T\!(\nabla_x H) = 0
\]</span></p>
<p>where we used the <a href="https://en.wikipedia.org/wiki/Chain_rule">chain rule</a> to compute the <a href="https://en.wikipedia.org/wiki/Total_derivative">total derivative</a> of the Hamiltonian with respect to time (note that there is no explicit time dependence of <span class="math inline">\(H\)</span>, i.e.&nbsp;<span class="math inline">\(\partial_t H=0\)</span>).</p>
<p>The trick of HMC is to sample the momenta <span class="math inline">\(v\)</span> from the collapsed distribution</p>
<p><span class="math display">\[
v^{(s)} \sim \mathcal N(0, I_D)
\]</span></p>
<p>and evolve the system to a proposal state starting from the previous positions <span class="math inline">\(x^{(s)}\)</span> and the newly sampled momenta <span class="math inline">\(v^{(s)}\)</span>. If the time evolution is based on Hamiltonian dynamics, we know that the Hamiltonian <span class="math inline">\(H(x, v)\)</span> is conserved. So if <span class="math inline">\(H(x^{(s)}, v^{(s)})\)</span> is the total energy of the current state, then the Hamiltonian of the proposal state <span class="math inline">\(\bigl(x(T), v(T)\bigr)\)</span> where <span class="math inline">\(T\)</span> is the integration time will be the same:</p>
<p><span class="math display">\[
H\bigl(x(T), v(T)\bigr) = H\bigl(x^{(s)}, v^{(s)}\bigr)\,\,\,\text{where}\,\,\,  \bigl(x^{(s)}, v^{(s)}\bigr) \xrightarrow[\text{dynamics}]{\text{Hamiltonian}} \bigl(x(T), v(T)\bigr) \,.
\]</span></p>
<p>The probability for accepting <span class="math inline">\(\bigl(x(T), v(T)\bigr)\)</span> as new state <span class="math inline">\(\bigl(x^{(s+1)}, v^{(s+1)}\bigr)\)</span> is given by the ratio</p>
<p><span class="math display">\[
\frac{p\bigl(x(T),v(T)\bigr)}{p\bigl(x^{(s)}, y^{(s)}\bigr)} = \exp\left\{-\Delta H \right\}\,\,\, \text{with}\,\,\, \Delta H = H\bigl(x(T), v(T) \bigr) -  H\bigl(x^{(s)}, v^{(s)} \bigr)\, .
\]</span></p>
<p>This ratio doesn’t depend on the proposal probabilities, because Hamiltonian dynamics is <a href="https://en.wikipedia.org/wiki/Liouville%27s_theorem_(Hamiltonian)">symplectic</a>, i.e.&nbsp;the chance for going from one point in phase space to another point in phase space is the same for the reverse dynamics (volumes in phase space don’t change under Hamiltonian dynamics, the dynamics of the system is incompressible). So the proposal chain is symmetric and proposal probabilities cancel out in the acceptance step.</p>
<p>Since the total energy of the start and proposal state are the same, we have <span class="math inline">\(\Delta H = 0\)</span> and the proposed state will be accepted with probability one.</p>
</section>
<section id="example-harmonic-oscillator" class="level3">
<h3 class="anchored" data-anchor-id="example-harmonic-oscillator">Example: Harmonic oscillator</h3>
<p>Let us consider a concrete example whose dynamics can be solved exactly: the <a href="https://en.wikipedia.org/wiki/Harmonic_oscillator">harmonic oscillator</a> which corresponds to a Gaussian model:</p>
<p><span class="math display">\[
H(x, v) = v^2 / 2 + k x^2 / 2
\]</span></p>
<p>with the following dynamics:</p>
<p><span class="math display">\[\begin{align}
\begin{split}
\dot{x} &amp;= +\frac{\partial H}{\partial v} = v \\
\dot{v} &amp;= -\frac{\partial H}{\partial x} = -k x\\
\end{split}
\end{align}\]</span></p>
<p>In matrix-vector form we have</p>
<p><span id="eq-oscillator"><span class="math display">\[
\frac{d}{dt} \begin{pmatrix} v\\ x\end{pmatrix} = \begin{pmatrix} 0 &amp; -k \\ 1 &amp; 0 \\ \end{pmatrix} \begin{pmatrix} v\\ x\end{pmatrix} = A \begin{pmatrix} v\\ x\end{pmatrix}\,\,\,\Rightarrow\,\,\, \begin{pmatrix} v(t)\\ x(t)\end{pmatrix} = \exp\{tA\} \begin{pmatrix} v_0\\ x_0\end{pmatrix}
\tag{78}\]</span></span></p>
<p>where <span class="math inline">\(\exp\{tA\}\)</span> is a matrix exponential <span class="math inline">\(\exp\{tA\} = \sum_n \frac{t^n}{n!} A^n\)</span>. The matrix powers have a simple structure:</p>
<p><span class="math display">\[
A^{2n} = (-k)^n I, \,\,\, A^{2n+1} = (-k)^n A
\]</span></p>
<p>So the solution of the Hamilton equations is (with <span class="math inline">\(\omega = \sqrt{k}\)</span>):</p>
<p><span class="math display">\[\begin{eqnarray*}\label{eq-oscillator2}
\begin{pmatrix} v(t)\\ x(t)\end{pmatrix} &amp;=&amp; I \sum_{n} \frac{(-)^n}{(2n)!} (\omega t)^{2n} \begin{pmatrix} v_0\\ x_0\end{pmatrix} + \omega^{-1}A \sum_{n} \frac{(-)^n}{(2n+1)!} (\omega t)^{2n+1} \begin{pmatrix} v_0\\ x_0\end{pmatrix} \\
&amp;=&amp; \biggl(I \cos(\omega t) + \omega^{-1}A \sin(\omega t)\biggr) \begin{pmatrix} v_0\\ x_0\end{pmatrix} \\
&amp;=&amp; \begin{pmatrix} \cos(\omega t) &amp; -\omega\sin(\omega t)\\ \sin(\omega t)/\omega  &amp; \cos(\omega t) \\ \end{pmatrix} \begin{pmatrix} v_0\\ x_0\end{pmatrix}
\end{eqnarray*}\]</span></p>
<p>The Hamiltonian <span class="math inline">\(H(x, v)\)</span> is indeed conserved (<em>Exercise</em>).</p>
<p>We can now sample a Gaussian model by exploiting the physical analogy to the harmonic oscillator. We pretend that we can only sample the momenta from a standard normal distribution, and then use the dynamics of the harmonic oscillator to generate a proposal state that is accepted with probability one:</p>
<div class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>plt.rc(<span class="st">'image'</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Oscillator:</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, k<span class="op">=</span><span class="fl">1.</span>, T<span class="op">=</span><span class="fl">1.</span>):</span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.k <span class="op">=</span> <span class="bu">float</span>(k)</span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.T <span class="op">=</span> <span class="bu">float</span>(T)</span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.A <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">1</span>], [<span class="op">-</span><span class="va">self</span>.k, <span class="dv">0</span>]])</span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w <span class="op">=</span> np.sqrt(k)</span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> propagate(<span class="va">self</span>, x, v, T<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb77-14"><a href="#cb77-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-15"><a href="#cb77-15" aria-hidden="true" tabindex="-1"></a>        T <span class="op">=</span> <span class="va">self</span>.T <span class="cf">if</span> T <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> <span class="bu">float</span>(T)</span>
<span id="cb77-16"><a href="#cb77-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb77-17"><a href="#cb77-17" aria-hidden="true" tabindex="-1"></a>        U <span class="op">=</span> np.array([[np.cos(<span class="va">self</span>.w<span class="op">*</span>T), np.sin(<span class="va">self</span>.w<span class="op">*</span>T)<span class="op">/</span><span class="va">self</span>.w],</span>
<span id="cb77-18"><a href="#cb77-18" aria-hidden="true" tabindex="-1"></a>                      [<span class="op">-</span>np.sin(<span class="va">self</span>.w<span class="op">*</span>T)<span class="op">*</span><span class="va">self</span>.w, np.cos(<span class="va">self</span>.w<span class="op">*</span>T)]])</span>
<span id="cb77-19"><a href="#cb77-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-20"><a href="#cb77-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> U.dot([x, v])</span>
<span id="cb77-21"><a href="#cb77-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-22"><a href="#cb77-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> calc_hamiltonian(<span class="va">self</span>, x, v):</span>
<span id="cb77-23"><a href="#cb77-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> v<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="va">self</span>.k <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span></span>
<span id="cb77-24"><a href="#cb77-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-25"><a href="#cb77-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> gradient(<span class="va">self</span>, x):</span>
<span id="cb77-26"><a href="#cb77-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.k <span class="op">*</span> x</span>
<span id="cb77-27"><a href="#cb77-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb77-28"><a href="#cb77-28" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> (<span class="fl">10.</span>, <span class="fl">0.1</span>)[<span class="dv">1</span>]</span>
<span id="cb77-29"><a href="#cb77-29" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="fl">1000.</span></span>
<span id="cb77-30"><a href="#cb77-30" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> <span class="fl">10.</span></span>
<span id="cb77-31"><a href="#cb77-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-32"><a href="#cb77-32" aria-hidden="true" tabindex="-1"></a>osci <span class="op">=</span> Oscillator(k, T)</span>
<span id="cb77-33"><a href="#cb77-33" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> [(x0, np.random.standard_normal())]</span>
<span id="cb77-34"><a href="#cb77-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-35"><a href="#cb77-35" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="bu">len</span>(samples) <span class="op">&lt;</span> <span class="fl">1e4</span>:</span>
<span id="cb77-36"><a href="#cb77-36" aria-hidden="true" tabindex="-1"></a>    x, v <span class="op">=</span> osci.propagate(<span class="op">*</span>samples[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb77-37"><a href="#cb77-37" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> np.random.standard_normal()</span>
<span id="cb77-38"><a href="#cb77-38" aria-hidden="true" tabindex="-1"></a>    samples.append((x, v))</span>
<span id="cb77-39"><a href="#cb77-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-40"><a href="#cb77-40" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> np.array(samples)</span>
<span id="cb77-41"><a href="#cb77-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-42"><a href="#cb77-42" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> osci.k<span class="op">**</span><span class="fl">0.5</span></span>
<span id="cb77-43"><a href="#cb77-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-44"><a href="#cb77-44" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="fl">1.</span>, <span class="dv">1000</span>) <span class="op">*</span> <span class="dv">4</span> <span class="op">*</span> sigma</span>
<span id="cb77-45"><a href="#cb77-45" aria-hidden="true" tabindex="-1"></a>p_x <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> sigma<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> np.log(<span class="dv">2</span><span class="op">*</span>np.pi<span class="op">*</span>sigma<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb77-46"><a href="#cb77-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-47"><a href="#cb77-47" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="fl">1.</span>, <span class="dv">1000</span>) <span class="op">*</span> <span class="dv">4</span> </span>
<span id="cb77-48"><a href="#cb77-48" aria-hidden="true" tabindex="-1"></a>p_v <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> v<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> np.log(<span class="dv">2</span><span class="op">*</span>np.pi))</span>
<span id="cb77-49"><a href="#cb77-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-50"><a href="#cb77-50" aria-hidden="true" tabindex="-1"></a>burnin <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb77-51"><a href="#cb77-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-52"><a href="#cb77-52" aria-hidden="true" tabindex="-1"></a>kw_hist <span class="op">=</span> <span class="bu">dict</span>(bins<span class="op">=</span><span class="dv">20</span>, color<span class="op">=</span><span class="st">'k'</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb77-53"><a href="#cb77-53" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb77-54"><a href="#cb77-54" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb77-55"><a href="#cb77-55" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(<span class="op">*</span>samples[:burnin].T, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb77-56"><a href="#cb77-56" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].scatter(<span class="op">*</span>samples[:burnin].T, c<span class="op">=</span>np.linspace(<span class="fl">0.</span>, <span class="fl">1.</span>, <span class="bu">len</span>(samples[:burnin])), alpha<span class="op">=</span><span class="fl">1.</span>)</span>
<span id="cb77-57"><a href="#cb77-57" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="vs">r'$x^{(s)}$'</span>)</span>
<span id="cb77-58"><a href="#cb77-58" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="vs">r'$v^{(s)}$'</span>)</span>
<span id="cb77-59"><a href="#cb77-59" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb77-60"><a href="#cb77-60" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].hist(samples[burnin:,<span class="dv">0</span>], <span class="op">**</span>kw_hist)</span>
<span id="cb77-61"><a href="#cb77-61" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(x, p_x, color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb77-62"><a href="#cb77-62" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="vs">r'$x^{(s)}$'</span>)</span>
<span id="cb77-63"><a href="#cb77-63" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb77-64"><a href="#cb77-64" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].hist(samples[burnin:,<span class="dv">1</span>], <span class="op">**</span>kw_hist)</span>
<span id="cb77-65"><a href="#cb77-65" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].plot(v, p_v, color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb77-66"><a href="#cb77-66" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_xlabel(<span class="vs">r'$v^{(s)}$'</span>)</span>
<span id="cb77-67"><a href="#cb77-67" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb77-68"><a href="#cb77-68" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-64-output-1.png" width="1121" height="355"></p>
</div>
</div>
</section>
</section>
</section>
<section id="lecture-8-hamiltonian-monte-carlo-continued" class="level1">
<h1>Lecture 8: Hamiltonian Monte Carlo continued</h1>
<section id="outline-7" class="level2">
<h2 class="anchored" data-anchor-id="outline-7">Outline</h2>
<ul>
<li>Hamiltonian Monte Carlo continued</li>
<li>Practical Issues (convergence, diagnostic checks)</li>
</ul>
</section>
<section id="recap-4" class="level2">
<h2 class="anchored" data-anchor-id="recap-4">Recap</h2>
<p>In the last lecture we’ve looked at the <a href="https://en.wikipedia.org/wiki/Harmonic_oscillator">harmonic oscillator</a>:</p>
<div class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pylab <span class="im">as</span> plt</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Oscillator:</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, k<span class="op">=</span><span class="fl">1.</span>, T<span class="op">=</span><span class="fl">1.</span>):</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.k <span class="op">=</span> <span class="bu">float</span>(k)</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.T <span class="op">=</span> <span class="bu">float</span>(T)</span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.A <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">1</span>], [<span class="op">-</span><span class="va">self</span>.k, <span class="dv">0</span>]])</span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w <span class="op">=</span> np.sqrt(k)</span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> propagate(<span class="va">self</span>, x, v, T<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a>        T <span class="op">=</span> <span class="va">self</span>.T <span class="cf">if</span> T <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> <span class="bu">float</span>(T)</span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a>        U <span class="op">=</span> np.array([[np.cos(<span class="va">self</span>.w<span class="op">*</span>T), np.sin(<span class="va">self</span>.w<span class="op">*</span>T)<span class="op">/</span><span class="va">self</span>.w],</span>
<span id="cb78-18"><a href="#cb78-18" aria-hidden="true" tabindex="-1"></a>                      [<span class="op">-</span>np.sin(<span class="va">self</span>.w<span class="op">*</span>T)<span class="op">*</span><span class="va">self</span>.w, np.cos(<span class="va">self</span>.w<span class="op">*</span>T)]])</span>
<span id="cb78-19"><a href="#cb78-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-20"><a href="#cb78-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> U.dot([x, v])</span>
<span id="cb78-21"><a href="#cb78-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-22"><a href="#cb78-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> calc_hamiltonian(<span class="va">self</span>, x, v):</span>
<span id="cb78-23"><a href="#cb78-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> v<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="va">self</span>.k <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span></span>
<span id="cb78-24"><a href="#cb78-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-25"><a href="#cb78-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> gradient(<span class="va">self</span>, x):</span>
<span id="cb78-26"><a href="#cb78-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.k <span class="op">*</span> x</span>
<span id="cb78-27"><a href="#cb78-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb78-28"><a href="#cb78-28" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> (<span class="fl">10.</span>, <span class="fl">0.1</span>)[<span class="dv">1</span>]</span>
<span id="cb78-29"><a href="#cb78-29" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="fl">1000.</span></span>
<span id="cb78-30"><a href="#cb78-30" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> <span class="fl">10.</span></span>
<span id="cb78-31"><a href="#cb78-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-32"><a href="#cb78-32" aria-hidden="true" tabindex="-1"></a>osci <span class="op">=</span> Oscillator(k, T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="leapfrog-integrator" class="level2">
<h2 class="anchored" data-anchor-id="leapfrog-integrator">Leapfrog integrator</h2>
<p>It is only rarely possible to solve Hamilton’s equations of motion in closed form. In practice, we often have to resort to numerical integration methods to solve this system of ordinary differential equations.</p>
<p>The crucial feature for making HMC work properly is the conservation of phase space volume. If phase space volume is not conserved, symmetry of the proposal chain is no longer guaranteed, and we would have to take into account the forward and backward probabilities in the acceptance ratios. These probabilities cannot be computed for general systems. Therefore, we have to look out for <a href="https://en.wikipedia.org/wiki/Symplectic_integrator">symplectic integrators</a> that solve Hamilton’s equations of motion numerically but still conserve phase-space volume.</p>
<p><a href="https://en.wikipedia.org/wiki/Leapfrog_integration">Leapfrog integration</a> is a simple symplectic integration scheme that is often used as an integrator in HMC. The leapfrog integrator solves a finite-difference version of Hamilton’s equations of motion:</p>
<p><span class="math display">\[\begin{align}\label{eq-leapfrog}
\begin{split}
v(t+\epsilon/2) &amp;= v(t) - (\epsilon/2) \nabla_x E(x(t)) \\
x(t+\epsilon) &amp;= x(t) + \epsilon v(t+\epsilon/2) \\
v(t+\epsilon) &amp;= v(t+\epsilon/2) - (\epsilon/2) \nabla_x E(x(t+\epsilon))
\end{split}
\end{align}\]</span></p>
<p>where <span class="math inline">\(\epsilon\)</span> is the time step. The initial positions and momenta <span class="math inline">\(x(0)\)</span> and <span class="math inline">\(v(0)\)</span> are the previous state <span class="math inline">\(x^{(s)}\)</span> and a sample from the standard Gaussian <span class="math inline">\(v^{(s)} \sim \mathcal N(0, I)\)</span>.</p>
<p>The leapfrog equations can be rearranged to avoid unnecessary gradient evaluations:</p>
<ol type="1">
<li><p>From <span class="math inline">\(x(0), v(0)\)</span> compute <span class="math inline">\(v(\epsilon/2)\)</span>:</p>
<p><span class="math display">\[
v(\epsilon/2) = v(0) - (\epsilon/2) \nabla_x E(x(0))
\]</span></p></li>
<li><p>Loop over <span class="math inline">\(T-1\)</span> integration steps:</p>
<p><span class="math display">\[
\begin{aligned}
x(t+\epsilon) &amp;= x(t) + \epsilon v(t+\epsilon/2) \\
v(t+3\epsilon/2) &amp;= v(t+\epsilon/2) - \epsilon \nabla_x E(x(t+\epsilon)) \\
\end{aligned}
\]</span></p></li>
<li><p>Last full-step in <span class="math inline">\(x\)</span> and half-step in <span class="math inline">\(v\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
x(T) &amp;= x(T-\epsilon) + \epsilon v(T-\epsilon/2)\\
v(T) &amp;= v(T-\epsilon/2) - (\epsilon/2) \nabla_x E(x(T)) \\
\end{aligned}
\]</span></p>
<p>Resulting in <span class="math inline">\(T+1\)</span> gradient evaluations, whereas the original scheme requires two gradient evaluations per time step.</p></li>
</ol>
<p>The following demonstration illustrates leapfrog integration for the harmonic oscillator:</p>
<div class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> leapfrog(x0, v0, eps, gradient, n_steps):</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Leapfrog integration</span></span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>    x, v <span class="op">=</span> x0, v0</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>    v <span class="op">-=</span> <span class="fl">0.5</span> <span class="op">*</span> eps <span class="op">*</span> gradient(x)</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_steps<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a>        x <span class="op">+=</span> eps <span class="op">*</span> v</span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a>        v <span class="op">-=</span> eps <span class="op">*</span> gradient(x)</span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a>    x <span class="op">+=</span> eps <span class="op">*</span> v</span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a>    v <span class="op">-=</span> <span class="fl">0.5</span> <span class="op">*</span> eps <span class="op">*</span> gradient(x)</span>
<span id="cb79-13"><a href="#cb79-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-14"><a href="#cb79-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x, v</span>
<span id="cb79-15"><a href="#cb79-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-16"><a href="#cb79-16" aria-hidden="true" tabindex="-1"></a><span class="co"># comparison of leapfrog with analytical solution</span></span>
<span id="cb79-17"><a href="#cb79-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-18"><a href="#cb79-18" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> <span class="fl">10.</span></span>
<span id="cb79-19"><a href="#cb79-19" aria-hidden="true" tabindex="-1"></a>v0 <span class="op">=</span> np.random.standard_normal()</span>
<span id="cb79-20"><a href="#cb79-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-21"><a href="#cb79-21" aria-hidden="true" tabindex="-1"></a>n_steps <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb79-22"><a href="#cb79-22" aria-hidden="true" tabindex="-1"></a>eps <span class="op">=</span> <span class="fl">1e-1</span></span>
<span id="cb79-23"><a href="#cb79-23" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> eps <span class="op">*</span> n_steps</span>
<span id="cb79-24"><a href="#cb79-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-25"><a href="#cb79-25" aria-hidden="true" tabindex="-1"></a>traj <span class="op">=</span> [(x0, v0)]</span>
<span id="cb79-26"><a href="#cb79-26" aria-hidden="true" tabindex="-1"></a>traj2 <span class="op">=</span> [(x0, v0)]</span>
<span id="cb79-27"><a href="#cb79-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-28"><a href="#cb79-28" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="bu">len</span>(traj) <span class="op">&lt;</span> <span class="dv">30</span>:</span>
<span id="cb79-29"><a href="#cb79-29" aria-hidden="true" tabindex="-1"></a>    x0, v0 <span class="op">=</span> traj[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb79-30"><a href="#cb79-30" aria-hidden="true" tabindex="-1"></a>    x, v <span class="op">=</span> leapfrog(x0, v0, eps, osci.gradient, n_steps)</span>
<span id="cb79-31"><a href="#cb79-31" aria-hidden="true" tabindex="-1"></a>    traj.append((x, v))</span>
<span id="cb79-32"><a href="#cb79-32" aria-hidden="true" tabindex="-1"></a>    x0, v0 <span class="op">=</span> traj2[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb79-33"><a href="#cb79-33" aria-hidden="true" tabindex="-1"></a>    traj2.append(osci.propagate(x0, v0, T))</span>
<span id="cb79-34"><a href="#cb79-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-35"><a href="#cb79-35" aria-hidden="true" tabindex="-1"></a>traj <span class="op">=</span> np.array(traj)</span>
<span id="cb79-36"><a href="#cb79-36" aria-hidden="true" tabindex="-1"></a>traj2 <span class="op">=</span> np.array(traj2)</span>
<span id="cb79-37"><a href="#cb79-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-38"><a href="#cb79-38" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb79-39"><a href="#cb79-39" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(<span class="op">*</span>traj.T, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb79-40"><a href="#cb79-40" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].scatter(<span class="op">*</span>traj.T, c<span class="op">=</span>np.linspace(<span class="fl">0.</span>, <span class="fl">1.</span>, <span class="bu">len</span>(traj)))</span>
<span id="cb79-41"><a href="#cb79-41" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="vs">r'$x(t)$'</span>)</span>
<span id="cb79-42"><a href="#cb79-42" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="vs">r'$v(t)$'</span>)</span>
<span id="cb79-43"><a href="#cb79-43" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb79-44"><a href="#cb79-44" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(traj[:,<span class="dv">0</span>], color<span class="op">=</span><span class="st">'k'</span>, lw<span class="op">=</span><span class="dv">5</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'analytical'</span>)</span>
<span id="cb79-45"><a href="#cb79-45" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(traj2[:,<span class="dv">0</span>], color<span class="op">=</span><span class="st">'r'</span>, label<span class="op">=</span><span class="st">'leapfrog'</span>)</span>
<span id="cb79-46"><a href="#cb79-46" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylim(<span class="op">-</span><span class="dv">11</span>, <span class="dv">11</span>)</span>
<span id="cb79-47"><a href="#cb79-47" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="vs">r'$t$'</span>)</span>
<span id="cb79-48"><a href="#cb79-48" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="vs">r'$x(t)$'</span>)</span>
<span id="cb79-49"><a href="#cb79-49" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].legend(fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb79-50"><a href="#cb79-50" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb79-51"><a href="#cb79-51" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].plot(traj[:,<span class="dv">1</span>], color<span class="op">=</span><span class="st">'k'</span>, lw<span class="op">=</span><span class="dv">5</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb79-52"><a href="#cb79-52" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].plot(traj2[:,<span class="dv">1</span>], color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb79-53"><a href="#cb79-53" aria-hidden="true" tabindex="-1"></a><span class="co">#ax[2].set_ylim(-11, 11)</span></span>
<span id="cb79-54"><a href="#cb79-54" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_xlabel(<span class="vs">r'$t$'</span>)</span>
<span id="cb79-55"><a href="#cb79-55" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_ylabel(<span class="vs">r'$v(t)$'</span>)</span>
<span id="cb79-56"><a href="#cb79-56" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb79-57"><a href="#cb79-57" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()                             </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-66-output-1.png" width="1123" height="356"></p>
</div>
</div>
<section id="comparison-to-other-integration-methods" class="level3">
<h3 class="anchored" data-anchor-id="comparison-to-other-integration-methods">Comparison to other integration methods</h3>
<p>In his review on HMC, Radford Neal compares the leapfrog integrator with other integration methods such as Euler. The following code reproduces his figure 1</p>
<div class="cell" data-execution_count="66">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="co">Implementation of examples from Neal's HMC review</span></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> integrate(transformation, n_steps, q0, p0):</span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Integrate Hamilton's equations of motion for the one-dimensional </span></span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a><span class="co">    harmonic oscillator using a transformation matrix that implements a</span></span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a><span class="co">    numerical integration method. </span></span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> np.array([q0, p0])</span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a>    traj <span class="op">=</span> [z]</span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_steps):</span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> transformation <span class="op">@</span> z</span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a>        traj.append(z)</span>
<span id="cb80-15"><a href="#cb80-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(traj)</span>
<span id="cb80-16"><a href="#cb80-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-17"><a href="#cb80-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> euler(eps, n_steps, q0<span class="op">=</span><span class="fl">0.</span>, p0<span class="op">=</span><span class="fl">1.</span>):</span>
<span id="cb80-18"><a href="#cb80-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># q(t+eps) = q(t) + eps * p(t)</span></span>
<span id="cb80-19"><a href="#cb80-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># p(t+eps) = p(t) - eps * q(t)</span></span>
<span id="cb80-20"><a href="#cb80-20" aria-hidden="true" tabindex="-1"></a>    T <span class="op">=</span> np.array([[<span class="dv">1</span>, eps],</span>
<span id="cb80-21"><a href="#cb80-21" aria-hidden="true" tabindex="-1"></a>                  [<span class="op">-</span>eps, <span class="dv">1</span>]])</span>
<span id="cb80-22"><a href="#cb80-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> integrate(T, n_steps, q0, p0), T</span>
<span id="cb80-23"><a href="#cb80-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-24"><a href="#cb80-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> modified_euler(eps, n_steps, q0<span class="op">=</span><span class="fl">0.</span>, p0<span class="op">=</span><span class="fl">1.</span>):</span>
<span id="cb80-25"><a href="#cb80-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># p(t+eps) = p(t) - eps * q(t)</span></span>
<span id="cb80-26"><a href="#cb80-26" aria-hidden="true" tabindex="-1"></a>    T1 <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="fl">0.</span>],</span>
<span id="cb80-27"><a href="#cb80-27" aria-hidden="true" tabindex="-1"></a>                   [<span class="op">-</span>eps, <span class="dv">1</span>]])</span>
<span id="cb80-28"><a href="#cb80-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># q(t+eps) = q(t) + eps * p(t+eps)    </span></span>
<span id="cb80-29"><a href="#cb80-29" aria-hidden="true" tabindex="-1"></a>    T2 <span class="op">=</span> np.array([[<span class="dv">1</span>, eps],</span>
<span id="cb80-30"><a href="#cb80-30" aria-hidden="true" tabindex="-1"></a>                   [<span class="dv">0</span>, <span class="dv">1</span>]])</span>
<span id="cb80-31"><a href="#cb80-31" aria-hidden="true" tabindex="-1"></a>    T <span class="op">=</span> T2 <span class="op">@</span> T1</span>
<span id="cb80-32"><a href="#cb80-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb80-33"><a href="#cb80-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> integrate(T, n_steps, q0, p0), T</span>
<span id="cb80-34"><a href="#cb80-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb80-35"><a href="#cb80-35" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> leapfrog(eps, n_steps, q0<span class="op">=</span><span class="fl">0.</span>, p0<span class="op">=</span><span class="fl">1.</span>):</span>
<span id="cb80-36"><a href="#cb80-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># p(t+eps/2) = p(t) - (eps/2) * q(t)</span></span>
<span id="cb80-37"><a href="#cb80-37" aria-hidden="true" tabindex="-1"></a>    T1 <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb80-38"><a href="#cb80-38" aria-hidden="true" tabindex="-1"></a>                   [<span class="op">-</span>eps<span class="op">/</span><span class="dv">2</span>, <span class="dv">1</span>]])</span>
<span id="cb80-39"><a href="#cb80-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># q(t+eps) = q(t) + eps * p(t+eps/2)</span></span>
<span id="cb80-40"><a href="#cb80-40" aria-hidden="true" tabindex="-1"></a>    T2 <span class="op">=</span> np.array([[<span class="dv">1</span>, eps],</span>
<span id="cb80-41"><a href="#cb80-41" aria-hidden="true" tabindex="-1"></a>                   [<span class="dv">0</span>, <span class="dv">1</span>]])</span>
<span id="cb80-42"><a href="#cb80-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># p(t+eps) = p(t+eps/2) - (eps/2) * q(t+eps)</span></span>
<span id="cb80-43"><a href="#cb80-43" aria-hidden="true" tabindex="-1"></a>    T3 <span class="op">=</span> T1</span>
<span id="cb80-44"><a href="#cb80-44" aria-hidden="true" tabindex="-1"></a>    T <span class="op">=</span> T3 <span class="op">@</span> T2 <span class="op">@</span> T1</span>
<span id="cb80-45"><a href="#cb80-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb80-46"><a href="#cb80-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> integrate(T, n_steps, q0, p0), T</span>
<span id="cb80-47"><a href="#cb80-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-48"><a href="#cb80-48" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb80-49"><a href="#cb80-49" aria-hidden="true" tabindex="-1"></a><span class="co">## run all three integrators</span></span>
<span id="cb80-50"><a href="#cb80-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-51"><a href="#cb80-51" aria-hidden="true" tabindex="-1"></a>eps <span class="op">=</span> (<span class="fl">0.3</span>, <span class="fl">1.2</span>, <span class="fl">0.1</span>)[<span class="dv">0</span>]</span>
<span id="cb80-52"><a href="#cb80-52" aria-hidden="true" tabindex="-1"></a>n_steps <span class="op">=</span> <span class="bu">int</span>(np.floor(<span class="dv">2</span><span class="op">*</span>np.pi<span class="op">/</span>eps))</span>
<span id="cb80-53"><a href="#cb80-53" aria-hidden="true" tabindex="-1"></a>n_steps <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb80-54"><a href="#cb80-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-55"><a href="#cb80-55" aria-hidden="true" tabindex="-1"></a>z1, T1 <span class="op">=</span> euler(eps, n_steps)</span>
<span id="cb80-56"><a href="#cb80-56" aria-hidden="true" tabindex="-1"></a>z2, T2 <span class="op">=</span> modified_euler(eps, n_steps)</span>
<span id="cb80-57"><a href="#cb80-57" aria-hidden="true" tabindex="-1"></a>z3, T3 <span class="op">=</span> leapfrog(eps, n_steps)</span>
<span id="cb80-58"><a href="#cb80-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-59"><a href="#cb80-59" aria-hidden="true" tabindex="-1"></a>methods <span class="op">=</span> (<span class="st">'Euler'</span>, <span class="st">'modified Euler'</span>, <span class="st">'Leapfrog'</span>)</span>
<span id="cb80-60"><a href="#cb80-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Volume preservation?'</span>)</span>
<span id="cb80-61"><a href="#cb80-61" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> method, trafo <span class="kw">in</span> <span class="bu">zip</span>(methods, [T1, T2, T3]):</span>
<span id="cb80-62"><a href="#cb80-62" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="sc">{0:&gt;14}</span><span class="st">: det(trafo) = </span><span class="sc">{1:.3f}</span><span class="st">'</span>.<span class="bu">format</span>(method, np.linalg.det(trafo)))</span>
<span id="cb80-63"><a href="#cb80-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-64"><a href="#cb80-64" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb80-65"><a href="#cb80-65" aria-hidden="true" tabindex="-1"></a><span class="co">## plot trajectories</span></span>
<span id="cb80-66"><a href="#cb80-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-67"><a href="#cb80-67" aria-hidden="true" tabindex="-1"></a><span class="co"># exact dynamics</span></span>
<span id="cb80-68"><a href="#cb80-68" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.sin(eps <span class="op">*</span> np.arange(n_steps<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb80-69"><a href="#cb80-69" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> np.cos(eps <span class="op">*</span> np.arange(n_steps<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb80-70"><a href="#cb80-70" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb80-71"><a href="#cb80-71" aria-hidden="true" tabindex="-1"></a>limits <span class="op">=</span> np.fabs(z1).<span class="bu">max</span>() <span class="op">*</span> <span class="fl">1.1</span></span>
<span id="cb80-72"><a href="#cb80-72" aria-hidden="true" tabindex="-1"></a>limits <span class="op">=</span> (<span class="op">-</span>limits, limits)</span>
<span id="cb80-73"><a href="#cb80-73" aria-hidden="true" tabindex="-1"></a>plt.rc(<span class="st">'font'</span>, size<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb80-74"><a href="#cb80-74" aria-hidden="true" tabindex="-1"></a>plt.rc(<span class="st">'image'</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb80-75"><a href="#cb80-75" aria-hidden="true" tabindex="-1"></a>kw <span class="op">=</span> <span class="bu">dict</span>(aspect<span class="op">=</span><span class="fl">1.0</span>, xlim<span class="op">=</span>limits, ylim<span class="op">=</span>limits, xlabel<span class="op">=</span><span class="vs">r'$x^{(s)}$'</span>)</span>
<span id="cb80-76"><a href="#cb80-76" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>), sharey<span class="op">=</span><span class="st">'all'</span>, sharex<span class="op">=</span><span class="st">'all'</span>,</span>
<span id="cb80-77"><a href="#cb80-77" aria-hidden="true" tabindex="-1"></a>                         subplot_kw<span class="op">=</span>kw)</span>
<span id="cb80-78"><a href="#cb80-78" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="vs">r'$v^{(s)}$'</span>)</span>
<span id="cb80-79"><a href="#cb80-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-80"><a href="#cb80-80" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax, method, z <span class="kw">in</span> <span class="bu">zip</span>(axes, methods, [z1, z2, z3]):</span>
<span id="cb80-81"><a href="#cb80-81" aria-hidden="true" tabindex="-1"></a>    ax.set_title(method)</span>
<span id="cb80-82"><a href="#cb80-82" aria-hidden="true" tabindex="-1"></a>    ax.scatter(<span class="op">*</span>z.T, c<span class="op">=</span>np.linspace(<span class="fl">0.</span>, <span class="fl">1.</span>, n_steps<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb80-83"><a href="#cb80-83" aria-hidden="true" tabindex="-1"></a>    ax.plot(<span class="op">*</span>z.T, ls<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb80-84"><a href="#cb80-84" aria-hidden="true" tabindex="-1"></a>    ax.plot(x, v, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, lw<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb80-85"><a href="#cb80-85" aria-hidden="true" tabindex="-1"></a>    ax.scatter(x, v, c<span class="op">=</span>np.linspace(<span class="fl">0.</span>, <span class="fl">1.</span>, n_steps<span class="op">+</span><span class="dv">1</span>), marker<span class="op">=</span><span class="st">'*'</span>)</span>
<span id="cb80-86"><a href="#cb80-86" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Volume preservation?
         Euler: det(trafo) = 1.090
modified Euler: det(trafo) = 1.000
      Leapfrog: det(trafo) = 1.000</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-67-output-2.png" width="1107" height="399"></p>
</div>
</div>
</section>
</section>
<section id="hamiltonian-monte-carlo-1" class="level2">
<h2 class="anchored" data-anchor-id="hamiltonian-monte-carlo-1">Hamiltonian Monte Carlo</h2>
<p>Hamiltonian Monte Carlo uses the leapfrog integrator to solve Hamilton’s equations of motion. The resulting proposal state is then accepted or rejected according to the Metropolis-Hastings criterion on the augmented <span class="math inline">\((x, v)\)</span> space (phase space). That is, we accept the new state with probability</p>
<p><span class="math display">\[
\min\left\{1, \exp(-\Delta H)\right\}
\]</span></p>
<p>which holds since Hamiltonian dynamics and the leapfrog integrator preserve volume (otherwise we would have to take into account the proposal probabilities for the forward and backward dynamics).</p>
<section id="algorithm-hamiltonian-monte-carlo" class="level3">
<h3 class="anchored" data-anchor-id="algorithm-hamiltonian-monte-carlo">Algorithm: Hamiltonian Monte Carlo</h3>
<p>Generate an initial state <span class="math inline">\(x^{(0)} \sim p^{(0)}(x)\)</span> using some initial distribution. For <span class="math inline">\(s=0, 1, 2, \ldots\)</span> cycle over the following iterations:</p>
<ol type="1">
<li><p>Generate new momenta <span class="math inline">\(v^{(s)} \sim \mathcal N(0, I)\)</span></p></li>
<li><p>Integrate Hamilton’s equations of motion using the leapfrog algorithm resulting in a proposal state <span class="math inline">\((x', v')\)</span></p></li>
<li><p>Accept <span class="math inline">\((x', -v')\)</span> with probability</p>
<p><span class="math display">\[
\min\left\{1, \exp\bigl(H(x^{(s)}, v^{(s)})-H(x',-v')\bigr) \right\}
\]</span></p>
<p>as new state <span class="math inline">\((x^{(s+1)}, -v^{(s+1)})\)</span></p></li>
</ol>
<p>We have to negate the momenta at the end of the trajectory to make the proposal symmetric. Since Hamiltonian dynamics is reversible, flipping the sign of the momenta guarantees that if we reach <span class="math inline">\((x', v')\)</span> starting from <span class="math inline">\((x, v)\)</span> with Hamiltonian dynamics, then we will go back to <span class="math inline">\((x, v)\)</span> using a dynamics that starts from <span class="math inline">\((x', -v')\)</span>.</p>
<div class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> store(args, storage<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> storage <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>        storage.append([np.copy(arg) <span class="cf">for</span> arg <span class="kw">in</span> args])</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> leapfrog(x, v, gradient, eps, n_steps, traj<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Leapfrog integration</span></span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a>    store([x, v], traj)</span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_steps):</span>
<span id="cb82-11"><a href="#cb82-11" aria-hidden="true" tabindex="-1"></a>        v <span class="op">-=</span> (eps<span class="op">/</span><span class="dv">2</span>) <span class="op">*</span> gradient(x)</span>
<span id="cb82-12"><a href="#cb82-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">+=</span> eps <span class="op">*</span> v</span>
<span id="cb82-13"><a href="#cb82-13" aria-hidden="true" tabindex="-1"></a>        v <span class="op">-=</span> (eps<span class="op">/</span><span class="dv">2</span>) <span class="op">*</span> gradient(x)</span>
<span id="cb82-14"><a href="#cb82-14" aria-hidden="true" tabindex="-1"></a>        store([x, v], traj)</span>
<span id="cb82-15"><a href="#cb82-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x, v</span>
<span id="cb82-16"><a href="#cb82-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-17"><a href="#cb82-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-18"><a href="#cb82-18" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CoupledOscillator:</span>
<span id="cb82-19"><a href="#cb82-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Coupled oscillator with force constants stored in attribute 'K'"""</span></span>
<span id="cb82-20"><a href="#cb82-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, K):</span>
<span id="cb82-21"><a href="#cb82-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.K <span class="op">=</span> np.array(K)</span>
<span id="cb82-22"><a href="#cb82-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.v, <span class="va">self</span>.U <span class="op">=</span> np.linalg.eigh(<span class="va">self</span>.K)</span>
<span id="cb82-23"><a href="#cb82-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w <span class="op">=</span> np.sqrt(np.clip(<span class="va">self</span>.v, <span class="fl">0.</span>, <span class="va">None</span>))</span>
<span id="cb82-24"><a href="#cb82-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-25"><a href="#cb82-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> propagate(<span class="va">self</span>, x, v, eps):</span>
<span id="cb82-26"><a href="#cb82-26" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Move system from (x, v) to new state using Hamiltonian dynamics."""</span></span>
<span id="cb82-27"><a href="#cb82-27" aria-hidden="true" tabindex="-1"></a>        x, v <span class="op">=</span> <span class="va">self</span>.U.T <span class="op">@</span> x, <span class="va">self</span>.U.T <span class="op">@</span> v</span>
<span id="cb82-28"><a href="#cb82-28" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> np.cos(<span class="va">self</span>.w<span class="op">*</span>eps) <span class="op">*</span> x <span class="op">+</span> np.sin(<span class="va">self</span>.w<span class="op">*</span>eps) <span class="op">/</span> <span class="va">self</span>.w <span class="op">*</span> v</span>
<span id="cb82-29"><a href="#cb82-29" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> <span class="op">-</span>np.sin(<span class="va">self</span>.w<span class="op">*</span>eps) <span class="op">*</span> <span class="va">self</span>.w <span class="op">*</span> x <span class="op">+</span> np.cos(<span class="va">self</span>.w<span class="op">*</span>eps) <span class="op">*</span> v</span>
<span id="cb82-30"><a href="#cb82-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.U <span class="op">@</span> X, <span class="va">self</span>.U <span class="op">@</span> V</span>
<span id="cb82-31"><a href="#cb82-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb82-32"><a href="#cb82-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> energy(<span class="va">self</span>, x):</span>
<span id="cb82-33"><a href="#cb82-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> np.<span class="bu">sum</span>(x <span class="op">*</span> (x <span class="op">@</span> <span class="va">self</span>.K), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb82-34"><a href="#cb82-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb82-35"><a href="#cb82-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> gradient(<span class="va">self</span>, x):</span>
<span id="cb82-36"><a href="#cb82-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">@</span> <span class="va">self</span>.K</span>
<span id="cb82-37"><a href="#cb82-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-38"><a href="#cb82-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hamiltonian(<span class="va">self</span>, x, v):</span>
<span id="cb82-39"><a href="#cb82-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.energy(x) <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> np.linalg.norm(v, axis<span class="op">=-</span><span class="dv">1</span>)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb82-40"><a href="#cb82-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb82-41"><a href="#cb82-41" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> covariance_matrix(sigma1<span class="op">=</span><span class="fl">1.</span>, sigma2<span class="op">=</span><span class="fl">1.</span>, rho<span class="op">=</span><span class="fl">0.</span>):</span>
<span id="cb82-42"><a href="#cb82-42" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Build two-dimensional covariance matrix. </span></span>
<span id="cb82-43"><a href="#cb82-43" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb82-44"><a href="#cb82-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="op">-</span><span class="fl">1.</span> <span class="op">&lt;=</span> rho <span class="op">&lt;=</span> <span class="fl">1.</span></span>
<span id="cb82-45"><a href="#cb82-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-46"><a href="#cb82-46" aria-hidden="true" tabindex="-1"></a>    Sigma <span class="op">=</span> np.diag([sigma1, sigma2]) <span class="op">\</span></span>
<span id="cb82-47"><a href="#cb82-47" aria-hidden="true" tabindex="-1"></a>            <span class="op">@</span> np.array([[<span class="dv">1</span>, rho], [rho, <span class="dv">1</span>]]) <span class="op">\</span></span>
<span id="cb82-48"><a href="#cb82-48" aria-hidden="true" tabindex="-1"></a>            <span class="op">@</span> np.diag([sigma1, sigma2])</span>
<span id="cb82-49"><a href="#cb82-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-50"><a href="#cb82-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Sigma</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>sigma1, sigma2, rho <span class="op">=</span> <span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">0.95</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> np.linalg.inv(covariance_matrix(sigma1, sigma2, rho))</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>osci <span class="op">=</span> CoupledOscillator(K)</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a><span class="co"># start position and velocity</span></span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> np.array([<span class="op">-</span><span class="fl">1.5</span>, <span class="op">-</span><span class="fl">1.55</span>])</span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a>v0 <span class="op">=</span> np.array([<span class="op">-</span><span class="fl">1.</span>, <span class="dv">1</span>])</span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>n_steps, eps <span class="op">=</span> <span class="dv">250</span>, <span class="fl">0.035</span></span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a>n_steps, eps <span class="op">=</span> <span class="dv">25</span>, <span class="fl">0.25</span></span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-12"><a href="#cb83-12" aria-hidden="true" tabindex="-1"></a>xvals1 <span class="op">=</span> xvals2 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>, <span class="dv">101</span>) <span class="op">*</span> <span class="fl">2.5</span></span>
<span id="cb83-13"><a href="#cb83-13" aria-hidden="true" tabindex="-1"></a>X1, X2 <span class="op">=</span> np.meshgrid(xvals1, xvals2, indexing<span class="op">=</span><span class="st">'ij'</span>)</span>
<span id="cb83-14"><a href="#cb83-14" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> np.transpose([X1.flatten(), X2.flatten()])</span>
<span id="cb83-15"><a href="#cb83-15" aria-hidden="true" tabindex="-1"></a>logp <span class="op">=</span> <span class="op">-</span>osci.energy(grid).reshape(<span class="bu">len</span>(xvals1), <span class="bu">len</span>(xvals2))</span>
<span id="cb83-16"><a href="#cb83-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-17"><a href="#cb83-17" aria-hidden="true" tabindex="-1"></a><span class="co"># exact dynamics</span></span>
<span id="cb83-18"><a href="#cb83-18" aria-hidden="true" tabindex="-1"></a>traj1 <span class="op">=</span> [(x0, v0)]</span>
<span id="cb83-19"><a href="#cb83-19" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="bu">len</span>(traj1) <span class="op">&lt;</span> n_steps <span class="op">+</span> <span class="dv">1</span>:</span>
<span id="cb83-20"><a href="#cb83-20" aria-hidden="true" tabindex="-1"></a>    traj1.append(osci.propagate(<span class="op">*</span>traj1[<span class="op">-</span><span class="dv">1</span>], eps<span class="op">=</span>eps))</span>
<span id="cb83-21"><a href="#cb83-21" aria-hidden="true" tabindex="-1"></a>x1, v1 <span class="op">=</span> np.array(<span class="bu">list</span>(<span class="bu">zip</span>(<span class="op">*</span>traj1)))</span>
<span id="cb83-22"><a href="#cb83-22" aria-hidden="true" tabindex="-1"></a>H1 <span class="op">=</span> osci.hamiltonian(x1, v1)</span>
<span id="cb83-23"><a href="#cb83-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-24"><a href="#cb83-24" aria-hidden="true" tabindex="-1"></a><span class="co"># leapfrog integration</span></span>
<span id="cb83-25"><a href="#cb83-25" aria-hidden="true" tabindex="-1"></a>traj2 <span class="op">=</span> []</span>
<span id="cb83-26"><a href="#cb83-26" aria-hidden="true" tabindex="-1"></a>leapfrog(x0, v0, osci.gradient, eps, n_steps, traj2)</span>
<span id="cb83-27"><a href="#cb83-27" aria-hidden="true" tabindex="-1"></a>x2, v2 <span class="op">=</span> np.array(<span class="bu">list</span>(<span class="bu">zip</span>(<span class="op">*</span>traj2)))</span>
<span id="cb83-28"><a href="#cb83-28" aria-hidden="true" tabindex="-1"></a>H2 <span class="op">=</span> osci.hamiltonian(x2, v2)</span>
<span id="cb83-29"><a href="#cb83-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-30"><a href="#cb83-30" aria-hidden="true" tabindex="-1"></a><span class="co"># plot</span></span>
<span id="cb83-31"><a href="#cb83-31" aria-hidden="true" tabindex="-1"></a>plt.rc(<span class="st">'font'</span>, size<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb83-32"><a href="#cb83-32" aria-hidden="true" tabindex="-1"></a>limits <span class="op">=</span> (<span class="op">-</span><span class="fl">2.2</span>, <span class="fl">2.2</span>)</span>
<span id="cb83-33"><a href="#cb83-33" aria-hidden="true" tabindex="-1"></a>kw <span class="op">=</span> <span class="bu">dict</span>(xlim<span class="op">=</span>limits, ylim<span class="op">=</span>limits, aspect<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb83-34"><a href="#cb83-34" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">9</span>), sharex<span class="op">=</span><span class="st">'col'</span>, </span>
<span id="cb83-35"><a href="#cb83-35" aria-hidden="true" tabindex="-1"></a>                         sharey<span class="op">=</span><span class="st">'all'</span>, subplot_kw<span class="op">=</span>kw)</span>
<span id="cb83-36"><a href="#cb83-36" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> <span class="bu">list</span>(axes.flat)</span>
<span id="cb83-37"><a href="#cb83-37" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax, x <span class="kw">in</span> <span class="bu">zip</span>(axes, [x1, v1, x2, v2]):</span>
<span id="cb83-38"><a href="#cb83-38" aria-hidden="true" tabindex="-1"></a>    ax.plot(<span class="op">*</span>x.T, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb83-39"><a href="#cb83-39" aria-hidden="true" tabindex="-1"></a>    ax.scatter(<span class="op">*</span>x.T, s<span class="op">=</span><span class="dv">10</span>, c<span class="op">=</span>np.linspace(<span class="fl">0.</span>, <span class="fl">1.</span>, <span class="bu">len</span>(x)))</span>
<span id="cb83-40"><a href="#cb83-40" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_xlabel(<span class="vs">r'$x_1$'</span>)</span>
<span id="cb83-41"><a href="#cb83-41" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_ylabel(<span class="vs">r'$x_2$'</span>)</span>
<span id="cb83-42"><a href="#cb83-42" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="vs">r'$x_2$'</span>)</span>
<span id="cb83-43"><a href="#cb83-43" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">3</span>].set_xlabel(<span class="vs">r'$v_1$'</span>)</span>
<span id="cb83-44"><a href="#cb83-44" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">3</span>].set_ylabel(<span class="vs">r'$v_2$'</span>)</span>
<span id="cb83-45"><a href="#cb83-45" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="vs">r'$v_2$'</span>)</span>
<span id="cb83-46"><a href="#cb83-46" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> axes[:<span class="dv">2</span>]:</span>
<span id="cb83-47"><a href="#cb83-47" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="st">'exact dynamics'</span>)</span>
<span id="cb83-48"><a href="#cb83-48" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> axes[<span class="op">-</span><span class="dv">2</span>:]:</span>
<span id="cb83-49"><a href="#cb83-49" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="st">'leapfrog'</span>)</span>
<span id="cb83-50"><a href="#cb83-50" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].contour(xvals1, xvals2, np.exp(logp), <span class="dv">3</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb83-51"><a href="#cb83-51" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].contour(xvals1, xvals2, np.exp(logp), <span class="dv">3</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb83-52"><a href="#cb83-52" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-69-output-1.png" width="828" height="839"></p>
</div>
</div>
<p>The exact dynamics conserves the Hamiltonian (as it should), whereas the leapfrog dynamics does not exactly preserve the Hamiltonian:</p>
<div class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>ax.plot(H1, lw<span class="op">=</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'r'</span>, ls<span class="op">=</span><span class="st">'-'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="st">'exact dynamics'</span>)</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>ax.plot(H2, lw<span class="op">=</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'k'</span>, ls<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="st">'leapfrog'</span>)</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="vs">r'time step $n\epsilon$'</span>)</span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r'Hamiltonian $H(x, v)$'</span>)</span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="fl">2.0</span>, <span class="fl">2.8</span>)</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span>(<span class="fl">1.1</span>, <span class="fl">0.5</span>))<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-70-output-1.png" width="906" height="456"></p>
</div>
</div>
</section>
<section id="comparison-with-standard-metropolis-hastings" class="level3">
<h3 class="anchored" data-anchor-id="comparison-with-standard-metropolis-hastings">Comparison with standard Metropolis-Hastings</h3>
<div class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Metropolis:</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, state, n_samples<span class="op">=</span><span class="fl">1e2</span>, stepsize<span class="op">=</span><span class="fl">1e-1</span>):</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.initial_state <span class="op">=</span> np.array(state)</span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_samples <span class="op">=</span> <span class="bu">int</span>(n_samples)</span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stepsize <span class="op">=</span> <span class="bu">float</span>(stepsize)</span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._reset()</span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _reset(<span class="va">self</span>):</span>
<span id="cb85-11"><a href="#cb85-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.counter <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb85-12"><a href="#cb85-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.state <span class="op">=</span> <span class="va">self</span>.initial_state</span>
<span id="cb85-13"><a href="#cb85-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_accepted <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb85-14"><a href="#cb85-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb85-15"><a href="#cb85-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__next__</span>(<span class="va">self</span>):</span>
<span id="cb85-16"><a href="#cb85-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-17"><a href="#cb85-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.counter <span class="op">&gt;=</span> <span class="va">self</span>.n_samples:</span>
<span id="cb85-18"><a href="#cb85-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">StopIteration</span></span>
<span id="cb85-19"><a href="#cb85-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.counter <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb85-20"><a href="#cb85-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb85-21"><a href="#cb85-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># random walk </span></span>
<span id="cb85-22"><a href="#cb85-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.state.copy()</span>
<span id="cb85-23"><a href="#cb85-23" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.stepsize <span class="op">*</span> np.random.standard_normal(x.shape)</span>
<span id="cb85-24"><a href="#cb85-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-25"><a href="#cb85-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># accept/reject</span></span>
<span id="cb85-26"><a href="#cb85-26" aria-hidden="true" tabindex="-1"></a>        diff <span class="op">=</span> <span class="va">self</span>.model.energy(x) <span class="op">-</span> <span class="va">self</span>.model.energy(X)</span>
<span id="cb85-27"><a href="#cb85-27" aria-hidden="true" tabindex="-1"></a>        accept <span class="op">=</span> np.log(np.random.random()) <span class="op">&lt;</span> diff</span>
<span id="cb85-28"><a href="#cb85-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_accepted <span class="op">+=</span> <span class="bu">int</span>(accept)</span>
<span id="cb85-29"><a href="#cb85-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb85-30"><a href="#cb85-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> accept: <span class="va">self</span>.state <span class="op">=</span> X</span>
<span id="cb85-31"><a href="#cb85-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-32"><a href="#cb85-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.state</span>
<span id="cb85-33"><a href="#cb85-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-34"><a href="#cb85-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__iter__</span>(<span class="va">self</span>):</span>
<span id="cb85-35"><a href="#cb85-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._reset()</span>
<span id="cb85-36"><a href="#cb85-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb85-37"><a href="#cb85-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-38"><a href="#cb85-38" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb85-39"><a href="#cb85-39" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> acceptance_rate(<span class="va">self</span>):</span>
<span id="cb85-40"><a href="#cb85-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.n_accepted <span class="op">/</span> <span class="va">self</span>.n_samples        </span>
<span id="cb85-41"><a href="#cb85-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-42"><a href="#cb85-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb85-43"><a href="#cb85-43" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> HamiltonianMonteCarlo(Metropolis):</span>
<span id="cb85-44"><a href="#cb85-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-45"><a href="#cb85-45" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, state, n_samples, eps, n_leapfrog):</span>
<span id="cb85-46"><a href="#cb85-46" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(model, state, n_samples, eps)</span>
<span id="cb85-47"><a href="#cb85-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_leapfrog <span class="op">=</span> <span class="bu">int</span>(n_leapfrog)</span>
<span id="cb85-48"><a href="#cb85-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-49"><a href="#cb85-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__next__</span>(<span class="va">self</span>):</span>
<span id="cb85-50"><a href="#cb85-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-51"><a href="#cb85-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.counter <span class="op">&gt;=</span> <span class="va">self</span>.n_samples:</span>
<span id="cb85-52"><a href="#cb85-52" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">StopIteration</span></span>
<span id="cb85-53"><a href="#cb85-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.counter <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb85-54"><a href="#cb85-54" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb85-55"><a href="#cb85-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># leapfrog integration</span></span>
<span id="cb85-56"><a href="#cb85-56" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.state.copy()</span>
<span id="cb85-57"><a href="#cb85-57" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> np.random.standard_normal(x.shape)</span>
<span id="cb85-58"><a href="#cb85-58" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.model.hamiltonian(x, v)</span>
<span id="cb85-59"><a href="#cb85-59" aria-hidden="true" tabindex="-1"></a>        X, V <span class="op">=</span> leapfrog(</span>
<span id="cb85-60"><a href="#cb85-60" aria-hidden="true" tabindex="-1"></a>            x, v, <span class="va">self</span>.model.gradient, <span class="va">self</span>.stepsize, <span class="va">self</span>.n_leapfrog)</span>
<span id="cb85-61"><a href="#cb85-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-62"><a href="#cb85-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># accept/reject</span></span>
<span id="cb85-63"><a href="#cb85-63" aria-hidden="true" tabindex="-1"></a>        H <span class="op">=</span> <span class="va">self</span>.model.hamiltonian(X, <span class="op">-</span>V)</span>
<span id="cb85-64"><a href="#cb85-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-65"><a href="#cb85-65" aria-hidden="true" tabindex="-1"></a>        accept <span class="op">=</span> np.log(np.random.random()) <span class="op">&lt;</span> h<span class="op">-</span>H</span>
<span id="cb85-66"><a href="#cb85-66" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_accepted <span class="op">+=</span> <span class="bu">int</span>(accept)</span>
<span id="cb85-67"><a href="#cb85-67" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb85-68"><a href="#cb85-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> accept: <span class="va">self</span>.state <span class="op">=</span> X</span>
<span id="cb85-69"><a href="#cb85-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-70"><a href="#cb85-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.state</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 2d Gaussian</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>sigma1 <span class="op">=</span> sigma2 <span class="op">=</span> <span class="fl">1.</span></span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>rho <span class="op">=</span> <span class="fl">0.98</span></span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>Sigma <span class="op">=</span> covariance_matrix(sigma1, sigma2, rho)</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CoupledOscillator(np.linalg.inv(Sigma))</span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a><span class="co"># running HMC and random walk Metropolis</span></span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="fl">1e3</span></span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a>eps, n_leapfrog <span class="op">=</span> <span class="fl">0.18</span>, <span class="dv">20</span></span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a>initial <span class="op">=</span> np.array([<span class="op">-</span><span class="fl">1.55</span>, <span class="op">-</span><span class="fl">1.5</span>])</span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a>hmc <span class="op">=</span> HamiltonianMonteCarlo(model, initial, n_samples, eps, n_leapfrog)</span>
<span id="cb86-14"><a href="#cb86-14" aria-hidden="true" tabindex="-1"></a>hmc_samples <span class="op">=</span> np.array(<span class="bu">list</span>(hmc))</span>
<span id="cb86-15"><a href="#cb86-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-16"><a href="#cb86-16" aria-hidden="true" tabindex="-1"></a><span class="co"># to be fair, we allow for 'n_leapfrog' more sampling steps in Metropolis </span></span>
<span id="cb86-17"><a href="#cb86-17" aria-hidden="true" tabindex="-1"></a><span class="co"># sampling</span></span>
<span id="cb86-18"><a href="#cb86-18" aria-hidden="true" tabindex="-1"></a>metro <span class="op">=</span> Metropolis(model, initial, n_samples <span class="op">*</span> n_leapfrog, eps)</span>
<span id="cb86-19"><a href="#cb86-19" aria-hidden="true" tabindex="-1"></a>metro_samples <span class="op">=</span> np.array(<span class="bu">list</span>(metro))[::n_leapfrog]</span>
<span id="cb86-20"><a href="#cb86-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-21"><a href="#cb86-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'acceptance_rate: </span><span class="sc">{0:.2%}</span><span class="st"> (HMC), </span><span class="sc">{1:.2%}</span><span class="st"> (Metropolis)'</span>.<span class="bu">format</span>(</span>
<span id="cb86-22"><a href="#cb86-22" aria-hidden="true" tabindex="-1"></a>    hmc.acceptance_rate, metro.acceptance_rate))</span>
<span id="cb86-23"><a href="#cb86-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-24"><a href="#cb86-24" aria-hidden="true" tabindex="-1"></a><span class="co"># plotting</span></span>
<span id="cb86-25"><a href="#cb86-25" aria-hidden="true" tabindex="-1"></a>burnin <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.1</span><span class="op">*</span>n_samples)</span>
<span id="cb86-26"><a href="#cb86-26" aria-hidden="true" tabindex="-1"></a>limits <span class="op">=</span> (<span class="op">-</span><span class="fl">3.5</span>, <span class="fl">3.5</span>)</span>
<span id="cb86-27"><a href="#cb86-27" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">*</span>limits, num<span class="op">=</span><span class="dv">101</span>)</span>
<span id="cb86-28"><a href="#cb86-28" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> np.meshgrid(x, x, indexing<span class="op">=</span><span class="st">'ij'</span>)</span>
<span id="cb86-29"><a href="#cb86-29" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> np.transpose([grid[<span class="dv">0</span>].flatten(), grid[<span class="dv">1</span>].flatten()])</span>
<span id="cb86-30"><a href="#cb86-30" aria-hidden="true" tabindex="-1"></a>prob <span class="op">=</span> np.exp(<span class="op">-</span>model.energy(grid)).reshape(<span class="bu">len</span>(x), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb86-31"><a href="#cb86-31" aria-hidden="true" tabindex="-1"></a>px <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> sigma1<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> np.log(<span class="dv">2</span><span class="op">*</span>np.pi<span class="op">*</span>sigma1<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb86-32"><a href="#cb86-32" aria-hidden="true" tabindex="-1"></a>py <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> sigma2<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> np.log(<span class="dv">2</span><span class="op">*</span>np.pi<span class="op">*</span>sigma2<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb86-33"><a href="#cb86-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-34"><a href="#cb86-34" aria-hidden="true" tabindex="-1"></a>kw_panel <span class="op">=</span> <span class="bu">dict</span>(xlim<span class="op">=</span>limits)</span>
<span id="cb86-35"><a href="#cb86-35" aria-hidden="true" tabindex="-1"></a>kw_scatter <span class="op">=</span> <span class="bu">dict</span>(alpha<span class="op">=</span><span class="fl">0.1</span>, s<span class="op">=</span><span class="dv">10</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb86-36"><a href="#cb86-36" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">6</span>), subplot_kw<span class="op">=</span>kw_panel)</span>
<span id="cb86-37"><a href="#cb86-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-38"><a href="#cb86-38" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax, samples <span class="kw">in</span> <span class="bu">zip</span>(axes[:,<span class="dv">0</span>], [hmc_samples, metro_samples]):</span>
<span id="cb86-39"><a href="#cb86-39" aria-hidden="true" tabindex="-1"></a>    ax.scatter(<span class="op">*</span>samples.T, <span class="op">**</span>kw_scatter)</span>
<span id="cb86-40"><a href="#cb86-40" aria-hidden="true" tabindex="-1"></a>    ax.contour(x, x, prob, <span class="dv">5</span>)</span>
<span id="cb86-41"><a href="#cb86-41" aria-hidden="true" tabindex="-1"></a>    ax.set_ylim(<span class="op">*</span>limits)</span>
<span id="cb86-42"><a href="#cb86-42" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax, samples <span class="kw">in</span> <span class="bu">zip</span>(axes[:,<span class="dv">1</span>], [hmc_samples, metro_samples]):</span>
<span id="cb86-43"><a href="#cb86-43" aria-hidden="true" tabindex="-1"></a>    ax.hist(samples[burnin:,<span class="dv">0</span>], bins<span class="op">=</span><span class="dv">21</span>, density<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb86-44"><a href="#cb86-44" aria-hidden="true" tabindex="-1"></a>    ax.plot(x, px, color<span class="op">=</span><span class="st">'r'</span>, lw<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb86-45"><a href="#cb86-45" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax, samples <span class="kw">in</span> <span class="bu">zip</span>(axes[:,<span class="dv">2</span>], [hmc_samples, metro_samples]):</span>
<span id="cb86-46"><a href="#cb86-46" aria-hidden="true" tabindex="-1"></a>    ax.hist(samples[burnin:,<span class="dv">1</span>], bins<span class="op">=</span><span class="dv">30</span>, density<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb86-47"><a href="#cb86-47" aria-hidden="true" tabindex="-1"></a>    ax.plot(x, py, color<span class="op">=</span><span class="st">'r'</span>, lw<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb86-48"><a href="#cb86-48" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>acceptance_rate: 89.70% (HMC), 63.75% (Metropolis)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-72-output-2.png" width="837" height="550"></p>
</div>
</div>
<div class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>methods <span class="op">=</span> (<span class="st">'HMC'</span>, <span class="st">'Metropolis'</span>)</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> samples, method <span class="kw">in</span> <span class="bu">zip</span>([hmc_samples, metro_samples], methods):</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>    ax.plot(samples[burnin:,<span class="dv">0</span>], label<span class="op">=</span>method, alpha<span class="op">=</span><span class="fl">0.5</span>, </span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>           color<span class="op">=</span>{<span class="st">'HMC'</span>: <span class="st">'k'</span>, <span class="st">'Metropolis'</span>: <span class="st">'r'</span>}[method])</span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="vs">r'iteration $s$'</span>)</span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r'first coordinate $x_1^{(s)}$'</span>)</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>ax.legend()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-73-output-1.png" width="987" height="372"></p>
</div>
</div>
<p>Behavior in high-dimensional sample spaces (another example from Radford Neal):</p>
<div class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co">"""HMC in high dimensions: example from Radford Neal's HMC review</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RandomStepsize:</span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Randomized stepsize mixin. </span></span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, lower, upper):</span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lower <span class="op">=</span> <span class="bu">float</span>(lower)</span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.upper <span class="op">=</span> <span class="bu">float</span>(upper)</span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> <span class="va">self</span>.lower <span class="op">&lt;=</span> <span class="va">self</span>.upper</span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb89-12"><a href="#cb89-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb89-13"><a href="#cb89-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> stepsize(<span class="va">self</span>):</span>
<span id="cb89-14"><a href="#cb89-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._value <span class="op">=</span> np.random.uniform(<span class="va">self</span>.lower, <span class="va">self</span>.upper)</span>
<span id="cb89-15"><a href="#cb89-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._value</span>
<span id="cb89-16"><a href="#cb89-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb89-17"><a href="#cb89-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">@stepsize.setter</span></span>
<span id="cb89-18"><a href="#cb89-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> stepsize(<span class="va">self</span>, value):</span>
<span id="cb89-19"><a href="#cb89-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb89-20"><a href="#cb89-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-21"><a href="#cb89-21" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MetropolisWithRandomStepsize(Metropolis, RandomStepsize):</span>
<span id="cb89-22"><a href="#cb89-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, initial, n_samples, lower, upper):</span>
<span id="cb89-23"><a href="#cb89-23" aria-hidden="true" tabindex="-1"></a>        Metropolis.<span class="fu">__init__</span>(<span class="va">self</span>, model, initial, n_samples, lower)</span>
<span id="cb89-24"><a href="#cb89-24" aria-hidden="true" tabindex="-1"></a>        RandomStepsize.<span class="fu">__init__</span>(<span class="va">self</span>, lower, upper)</span>
<span id="cb89-25"><a href="#cb89-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb89-26"><a href="#cb89-26" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> HMCWithRandomStepsize(HamiltonianMonteCarlo, RandomStepsize):</span>
<span id="cb89-27"><a href="#cb89-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, initial, n_samples, lower, upper, n_leapfrog):</span>
<span id="cb89-28"><a href="#cb89-28" aria-hidden="true" tabindex="-1"></a>        HamiltonianMonteCarlo.<span class="fu">__init__</span>(</span>
<span id="cb89-29"><a href="#cb89-29" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>, model, initial, n_samples, lower, n_leapfrog</span>
<span id="cb89-30"><a href="#cb89-30" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb89-31"><a href="#cb89-31" aria-hidden="true" tabindex="-1"></a>        RandomStepsize.<span class="fu">__init__</span>(<span class="va">self</span>, lower, upper)</span>
<span id="cb89-32"><a href="#cb89-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb89-33"><a href="#cb89-33" aria-hidden="true" tabindex="-1"></a>ndim <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb89-34"><a href="#cb89-34" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="fl">1.</span>, ndim)</span>
<span id="cb89-35"><a href="#cb89-35" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CoupledOscillator(np.diag(<span class="dv">1</span><span class="op">/</span>sigma<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb89-36"><a href="#cb89-36" aria-hidden="true" tabindex="-1"></a>initial <span class="op">=</span> np.zeros(ndim)</span>
<span id="cb89-37"><a href="#cb89-37" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb89-38"><a href="#cb89-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-39"><a href="#cb89-39" aria-hidden="true" tabindex="-1"></a><span class="co"># hmc</span></span>
<span id="cb89-40"><a href="#cb89-40" aria-hidden="true" tabindex="-1"></a>n_leapfrog <span class="op">=</span> <span class="dv">150</span></span>
<span id="cb89-41"><a href="#cb89-41" aria-hidden="true" tabindex="-1"></a>hmc <span class="op">=</span> HMCWithRandomStepsize(</span>
<span id="cb89-42"><a href="#cb89-42" aria-hidden="true" tabindex="-1"></a>        model, initial, n_samples, <span class="fl">0.0104</span>, <span class="fl">0.0156</span>, n_leapfrog)</span>
<span id="cb89-43"><a href="#cb89-43" aria-hidden="true" tabindex="-1"></a>samples_hmc <span class="op">=</span> np.array(<span class="bu">list</span>(hmc))</span>
<span id="cb89-44"><a href="#cb89-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(hmc.acceptance_rate)</span>
<span id="cb89-45"><a href="#cb89-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-46"><a href="#cb89-46" aria-hidden="true" tabindex="-1"></a><span class="co"># metropolis</span></span>
<span id="cb89-47"><a href="#cb89-47" aria-hidden="true" tabindex="-1"></a>metro <span class="op">=</span> MetropolisWithRandomStepsize(</span>
<span id="cb89-48"><a href="#cb89-48" aria-hidden="true" tabindex="-1"></a>        model, initial, n_samples <span class="op">*</span> n_leapfrog, <span class="fl">0.0176</span>, <span class="fl">0.0264</span>)</span>
<span id="cb89-49"><a href="#cb89-49" aria-hidden="true" tabindex="-1"></a>samples_metro <span class="op">=</span> np.array(<span class="bu">list</span>(metro))[::n_leapfrog]</span>
<span id="cb89-50"><a href="#cb89-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(metro.acceptance_rate)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.866</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>0.25031</code></pre>
</div>
</div>
<div class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>burnin <span class="op">=</span> <span class="bu">int</span>(n_samples<span class="op">/</span><span class="dv">10</span>)</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a><span class="co"># plotting</span></span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">9</span>), sharey<span class="op">=</span><span class="st">'row'</span>, sharex<span class="op">=</span><span class="st">'col'</span>)</span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax, samples <span class="kw">in</span> <span class="bu">zip</span>(axes[<span class="dv">0</span>,:], [samples_metro, samples_hmc]):</span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>    ax.scatter(samples[burnin:].std(<span class="dv">0</span>), samples[burnin:].mean(<span class="dv">0</span>))</span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a>    ax.axhline(<span class="fl">0.</span>, ls<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>,<span class="dv">0</span>].set_ylim(<span class="op">-</span><span class="fl">0.7</span>, <span class="fl">0.7</span>)</span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax, samples <span class="kw">in</span> <span class="bu">zip</span>(axes[<span class="dv">1</span>,:], [samples_metro, samples_hmc]):</span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a>    ax.plot([<span class="fl">0.</span>, <span class="fl">1.</span>], [<span class="fl">0.</span>, <span class="fl">1.</span>], ls<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb92-11"><a href="#cb92-11" aria-hidden="true" tabindex="-1"></a>    ax.scatter(samples[burnin:].std(<span class="dv">0</span>), sigma)</span>
<span id="cb92-12"><a href="#cb92-12" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>,<span class="dv">0</span>].set_title(<span class="st">'Random walk Metropolis'</span>)</span>
<span id="cb92-13"><a href="#cb92-13" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>,<span class="dv">1</span>].set_title(<span class="st">'Hamiltonian Monte Carlo'</span>)</span>
<span id="cb92-14"><a href="#cb92-14" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>,<span class="dv">0</span>].set_ylabel(<span class="vs">r'sample mean $\{x_i^{(s)}\}$'</span>)</span>
<span id="cb92-15"><a href="#cb92-15" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>,<span class="dv">0</span>].set_xlabel(<span class="vs">r'sample variance $\{x_i^{(s)}\}$'</span>)</span>
<span id="cb92-16"><a href="#cb92-16" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>,<span class="dv">1</span>].set_xlabel(<span class="vs">r'sample variance $\{x_i^{(s)}\}$'</span>)</span>
<span id="cb92-17"><a href="#cb92-17" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>,<span class="dv">0</span>].set_ylabel(<span class="vs">r'$\sigma_i$'</span>)</span>
<span id="cb92-18"><a href="#cb92-18" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-75-output-1.png" width="839" height="834"></p>
</div>
</div>
<p>A practical issue in applications of HMC is that the algorithm requires the gradient of minus log <span class="math inline">\(p(x)\)</span>. For some models it is far from straightforward to implement the gradient. Moreover, the gradient evaluations are on top of the evaluations of <span class="math inline">\(\log p(x)\)</span>, which are typical for MCMC approaches based on Metropolis-Hastings. Some remedy is provided by the possibility to use <a href="https://en.wikipedia.org/wiki/Automatic_differentiation"><em>automatic differentiation</em></a> to compute the gradient without implementing it explicitly. This strategy is used, for example, in the <a href="https://en.wikipedia.org/wiki/Stan_(software)">STAN</a> software for statistical inference or in probabilistic programming packages such as <a href="https://www.tensorflow.org/probability">tensorflow probability</a>, <a href="https://docs.pymc.io/">PyMC 3</a>, or <a href="https://pyro.ai/">pyro</a>.</p>
<p>Another issue of practical importance is the question how to choose the algorithmic parameters, i.e.&nbsp;the number of integration steps <span class="math inline">\(T\)</span> and the step size <span class="math inline">\(\epsilon\)</span>. Some attempts to choose these parameters automatically has been proposed and implemented in the <a href="https://arxiv.org/abs/1111.4246">NUTS</a> algorithm.</p>
<p>A special case of HMC is <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_Langevin_dynamics">Langevin dynamics</a> which omits the acceptance/rejection step altogether and has gained some popularity in Bayesian deep learning when combined with stochastic gradient methods.</p>
</section>
</section>
</section>
<section id="lecture-9-practical-issues-and-diagnostics" class="level1">
<h1>Lecture 9: Practical issues and diagnostics</h1>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="images/Murray_Thesis_Fig2-1.png" title="Challenges" class="img-fluid figure-img">
</figure>
<p></p><figcaption class="figure-caption">Challenges in MCMC</figcaption><p></p>
</figure>
</div>
<p>Figure from <a href="http://homepages.inf.ed.ac.uk/imurray2/pub/07thesis/murray_thesis_2007.pdf">Iain Murray: Advances in Markov chain Monte Carlo methods</a></p>
<section id="challenges-in-mcmc" class="level2">
<h2 class="anchored" data-anchor-id="challenges-in-mcmc">Challenges in MCMC</h2>
<ul>
<li><p><strong>Local exploration</strong>: MCMC samplers typically employ a proposal kernel that changes the current state only locally. The magnitude of changes in the variables is controlled by the <em>step size</em> or a similar algorithmic parameter. The step size is limited by the need to maintain a reasonable acceptance rate. The time it takes for a diffusive random walk to explore a distance scales with <span class="math display">\[
(\text{distance} / \text{step size})^2  
\]</span></p></li>
<li><p><strong>Convergence</strong>: Typically, the chain starts from a highly improbable state, far away from any mode (local peak in the probability density function). To find a nearby mode, takes some time, again scaling unfavorably with dimension. But even if a mode has been found, it is not guaranteed that the Markov chain will find other modes in a reasonable amount of simulation time. These other modes could be more important in the sense that they carry more probability mass; so missing out on these modes can result in highly biased approximations.</p></li>
<li><p><strong>Mixing</strong>: To find all relevant modes is one of the greatest challenges when sampling high-dimensional probabilistic models with multiple peaks (which is the rule rather than the exception). There are many reasons for having to deal with multi-modal distributions. A common reason are symmetries such as invariance under permutation of labels resulting in the <a href="https://link.springer.com/chapter/10.1007/978-3-662-01131-7_26">label-switching problem</a> in Gaussian mixture modeling. None of the methods that we discussed so far are particularly suited to explore multi-modal probability distributions. A common approach is to use <em>tempering</em> to flatten the probability such that the Markov chain can explore sample space more freely, and simulate a chain of tempered distributions, either sequentially (e.g.&nbsp;in <a href="https://link.springer.com/article/10.1023/A:1008923215028"><em>Annealed importance sampling</em> (AIS)</a>) or in parallel (e.g.&nbsp;in <a href="https://en.wikipedia.org/wiki/Parallel_tempering"><em>Parallel tempering</em></a>). Unfortunately, there is not enough time to discuss these important methods.</p></li>
<li><p><strong>Balancing density and volume</strong>: Even if all relevant modes are found eventually, our algorithms might not visit them in due proportion. A probability peak might be very pronounced, but only carry a small amount of probability mass. If “jumps” between modes happen infrequently, the fraction of samples per mode give a bad estimate of the actual propoartion of probability mass under those modes.</p></li>
</ul>
</section>
<section id="example-bimodal-distribution" class="level2">
<h2 class="anchored" data-anchor-id="example-bimodal-distribution">Example: Bimodal distribution</h2>
<p>Let’s illustrate some of these problems on a simple probability distribution that has two modes. Multimodality offers one of the largest challenges for MCMC, which allows us to demonstrate some typical problems on a relatively simple example. We discretize the distribution so we can easily analyze the Markov chains by the eigenvalues of their transition matrices.</p>
<div class="cell" data-execution_count="75">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> logsumexp</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_target(centers, widths, weights, n_states):</span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Return a discrete probability distribution (a probability vector)</span></span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a><span class="co">    defined on a discretization with `n_states` bins of the interval [-7, 10].</span></span>
<span id="cb93-9"><a href="#cb93-9" aria-hidden="true" tabindex="-1"></a><span class="co">    The probability distribution is bimodal with gaussian peaks at `centers` and</span></span>
<span id="cb93-10"><a href="#cb93-10" aria-hidden="true" tabindex="-1"></a><span class="co">    standard deviations `widths`.</span></span>
<span id="cb93-11"><a href="#cb93-11" aria-hidden="true" tabindex="-1"></a><span class="co">    The peaks is weighed according to `weights`</span></span>
<span id="cb93-12"><a href="#cb93-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb93-13"><a href="#cb93-13" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">7.</span>, <span class="fl">10.</span>, n_states)</span>
<span id="cb93-14"><a href="#cb93-14" aria-hidden="true" tabindex="-1"></a>    dist <span class="op">=</span> np.subtract.outer(x, centers)</span>
<span id="cb93-15"><a href="#cb93-15" aria-hidden="true" tabindex="-1"></a>    log_weights <span class="op">=</span> np.log(weights)</span>
<span id="cb93-16"><a href="#cb93-16" aria-hidden="true" tabindex="-1"></a>    log_weights <span class="op">-=</span> logsumexp(log_weights)</span>
<span id="cb93-17"><a href="#cb93-17" aria-hidden="true" tabindex="-1"></a>    logp <span class="op">=</span> (</span>
<span id="cb93-18"><a href="#cb93-18" aria-hidden="true" tabindex="-1"></a>        <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> dist<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> widths<span class="op">**</span><span class="dv">2</span></span>
<span id="cb93-19"><a href="#cb93-19" aria-hidden="true" tabindex="-1"></a>        <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> np.log(<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> widths<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb93-20"><a href="#cb93-20" aria-hidden="true" tabindex="-1"></a>        <span class="op">+</span> log_weights</span>
<span id="cb93-21"><a href="#cb93-21" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb93-22"><a href="#cb93-22" aria-hidden="true" tabindex="-1"></a>    logp <span class="op">=</span> logsumexp(logp, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb93-23"><a href="#cb93-23" aria-hidden="true" tabindex="-1"></a>    logp <span class="op">-=</span> logsumexp(logp)</span>
<span id="cb93-24"><a href="#cb93-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(logp)</span>
<span id="cb93-25"><a href="#cb93-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-26"><a href="#cb93-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-27"><a href="#cb93-27" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_proposal(n_neighbors, n_states):</span>
<span id="cb93-28"><a href="#cb93-28" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb93-29"><a href="#cb93-29" aria-hidden="true" tabindex="-1"></a><span class="co">    Return a n_states x n_states transition matrix that puts equal</span></span>
<span id="cb93-30"><a href="#cb93-30" aria-hidden="true" tabindex="-1"></a><span class="co">    probability on the transition to the state's `n_neighbors' next</span></span>
<span id="cb93-31"><a href="#cb93-31" aria-hidden="true" tabindex="-1"></a><span class="co">    neighbors and zero probability to all other states.</span></span>
<span id="cb93-32"><a href="#cb93-32" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb93-33"><a href="#cb93-33" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> np.<span class="bu">sum</span>([np.eye(n_states, k<span class="op">=</span>k) <span class="cf">for</span> k </span>
<span id="cb93-34"><a href="#cb93-34" aria-hidden="true" tabindex="-1"></a>                <span class="kw">in</span> <span class="bu">range</span>(<span class="op">-</span>n_neighbors, n_neighbors<span class="op">+</span><span class="dv">1</span>)], <span class="dv">0</span>)</span>
<span id="cb93-35"><a href="#cb93-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Q <span class="op">/</span> Q.<span class="bu">sum</span>(<span class="dv">1</span>)</span>
<span id="cb93-36"><a href="#cb93-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-37"><a href="#cb93-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-38"><a href="#cb93-38" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> metropolis_hastings_discrete(</span>
<span id="cb93-39"><a href="#cb93-39" aria-hidden="true" tabindex="-1"></a>    X, p, Q, init_state, n_samples, seed<span class="op">=</span><span class="va">None</span></span>
<span id="cb93-40"><a href="#cb93-40" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb93-41"><a href="#cb93-41" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb93-42"><a href="#cb93-42" aria-hidden="true" tabindex="-1"></a><span class="co">    Metropolis-Hastings for discrete target `p` with proposal matrix `Q`</span></span>
<span id="cb93-43"><a href="#cb93-43" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb93-44"><a href="#cb93-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> seed <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb93-45"><a href="#cb93-45" aria-hidden="true" tabindex="-1"></a>        np.random.seed(seed) </span>
<span id="cb93-46"><a href="#cb93-46" aria-hidden="true" tabindex="-1"></a>    n_accepted <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb93-47"><a href="#cb93-47" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> init_state</span>
<span id="cb93-48"><a href="#cb93-48" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> [x]</span>
<span id="cb93-49"><a href="#cb93-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-50"><a href="#cb93-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># standard Metropolis-Hastings loop</span></span>
<span id="cb93-51"><a href="#cb93-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="bu">len</span>(samples) <span class="op">&lt;</span> n_samples:</span>
<span id="cb93-52"><a href="#cb93-52" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> np.random.choice(X, p<span class="op">=</span>Q[:,x])</span>
<span id="cb93-53"><a href="#cb93-53" aria-hidden="true" tabindex="-1"></a>        r <span class="op">=</span> Q[x,y] <span class="op">*</span> p[y] <span class="op">/</span> (Q[y, x] <span class="op">*</span> p[x])</span>
<span id="cb93-54"><a href="#cb93-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> r <span class="op">&gt;</span> np.random.random():</span>
<span id="cb93-55"><a href="#cb93-55" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> y</span>
<span id="cb93-56"><a href="#cb93-56" aria-hidden="true" tabindex="-1"></a>            n_accepted <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb93-57"><a href="#cb93-57" aria-hidden="true" tabindex="-1"></a>        samples.append(x)</span>
<span id="cb93-58"><a href="#cb93-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-59"><a href="#cb93-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(samples), n_accepted <span class="op">/</span> n_samples</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="mixing" class="level2">
<h2 class="anchored" data-anchor-id="mixing">Mixing</h2>
<p>For the following, we set the two peaks in our example to have equal mass (i.e.&nbsp;the integral over the density around both peaks is the same), but choose one to be slim and tall, and the other relatively broad and shallow.</p>
<p>We will see that with the particular proposal distribution and stepsize we chose, the samples are <em>not</em> a very good representation of the target distribution, since the proportion of samples under each peak is quite far from the proportion of actual probability mass under the peaks:</p>
<div class="cell" data-execution_count="76">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ipywidgets <span class="im">import</span> interact, Dropdown</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a><span class="co"># setting bimodal toy system</span></span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>centers <span class="op">=</span> np.array([<span class="op">-</span><span class="fl">4.0</span>, <span class="fl">4.0</span>])</span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>widths <span class="op">=</span> np.array([<span class="fl">0.2</span>, <span class="fl">2.0</span>])</span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> np.array([<span class="fl">0.5</span>, <span class="fl">0.5</span>])</span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span>            <span class="co"># number of states</span></span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.arange(n)   <span class="co"># sample space</span></span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> make_target(centers, widths, weights, n)</span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a><span class="co"># use local proposal chain</span></span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a>stepsize <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="fl">1e4</span></span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a><span class="co"># random walk with uniform proposal and reflective boundary</span></span>
<span id="cb94-17"><a href="#cb94-17" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> make_proposal(stepsize, n)</span>
<span id="cb94-18"><a href="#cb94-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-19"><a href="#cb94-19" aria-hidden="true" tabindex="-1"></a><span class="at">@interact</span>(</span>
<span id="cb94-20"><a href="#cb94-20" aria-hidden="true" tabindex="-1"></a>    seed<span class="op">=</span>Dropdown(</span>
<span id="cb94-21"><a href="#cb94-21" aria-hidden="true" tabindex="-1"></a>        options<span class="op">=</span>[<span class="dv">41</span>, <span class="dv">1234</span>, <span class="dv">43</span>],</span>
<span id="cb94-22"><a href="#cb94-22" aria-hidden="true" tabindex="-1"></a>        value<span class="op">=</span><span class="dv">41</span>,</span>
<span id="cb94-23"><a href="#cb94-23" aria-hidden="true" tabindex="-1"></a>        description<span class="op">=</span><span class="st">"Seed: "</span></span>
<span id="cb94-24"><a href="#cb94-24" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb94-25"><a href="#cb94-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb94-26"><a href="#cb94-26" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_samples(seed):</span>
<span id="cb94-27"><a href="#cb94-27" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb94-28"><a href="#cb94-28" aria-hidden="true" tabindex="-1"></a><span class="co">    Run the sampler with given RNG-seed and plot the samples as well as</span></span>
<span id="cb94-29"><a href="#cb94-29" aria-hidden="true" tabindex="-1"></a><span class="co">    the target distribution.</span></span>
<span id="cb94-30"><a href="#cb94-30" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb94-31"><a href="#cb94-31" aria-hidden="true" tabindex="-1"></a>    samples, acceptance_rate <span class="op">=</span> metropolis_hastings_discrete(</span>
<span id="cb94-32"><a href="#cb94-32" aria-hidden="true" tabindex="-1"></a>        X, p, Q, X[<span class="op">-</span><span class="dv">1</span>], n_samples, seed</span>
<span id="cb94-33"><a href="#cb94-33" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb94-34"><a href="#cb94-34" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"acceptance rate: </span><span class="sc">{</span>acceptance_rate<span class="sc">:.1%}</span><span class="ss">"</span>)</span>
<span id="cb94-35"><a href="#cb94-35" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>np<span class="sc">.</span>mean(samples<span class="op">&lt;</span><span class="dv">25</span>)<span class="sc">:.1%}</span><span class="ss"> of all samples are in left mode"</span>)</span>
<span id="cb94-36"><a href="#cb94-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-37"><a href="#cb94-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># plot results</span></span>
<span id="cb94-38"><a href="#cb94-38" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">'font'</span>, size<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb94-39"><a href="#cb94-39" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">5</span>))</span>
<span id="cb94-40"><a href="#cb94-40" aria-hidden="true" tabindex="-1"></a>    ax.plot(X, p, color<span class="op">=</span><span class="st">'r'</span>, lw<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb94-41"><a href="#cb94-41" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"Probability distribution with two peaks of equal mass</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb94-42"><a href="#cb94-42" aria-hidden="true" tabindex="-1"></a>             <span class="ss">f"but </span><span class="sc">{</span>np<span class="sc">.</span>mean(samples<span class="op">&lt;</span><span class="dv">25</span>)<span class="sc">:.1%}</span><span class="ss"> of all samples are in left peak"</span>)</span>
<span id="cb94-43"><a href="#cb94-43" aria-hidden="true" tabindex="-1"></a>    ax.hist(samples, bins<span class="op">=</span><span class="dv">40</span>, density<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c4ca89f02b0f4a62b47fa47eef163278","version_major":2,"version_minor":0}
</script>
</div>
</div>
<p>Assessing whether our chain has visited each peak in due proportion is in general a very hard problem, because the true proportions are not known (and estimating the proportions is equivalent to the original problem we’re trying to solve).</p>
<p>What we can say though is this: If multimodality is present, and our chain visits at least two different modes (otherwise we might not even know that more than one mode exists), then if the number of jumps between those peaks is low, the proportion of samples under each peak is very likely a bad estimate of the true proportion. This is one of the reasons why visual inspection of the samples is important. (We will elaborate on this below.)</p>
</section>
<section id="convergence" class="level2">
<h2 class="anchored" data-anchor-id="convergence">Convergence</h2>
<section id="convergence-rates-for-markov-chains" class="level3">
<h3 class="anchored" data-anchor-id="convergence-rates-for-markov-chains">Convergence rates for Markov chains</h3>
<p>The speed of convergence of a Markov chain <span class="math inline">\(P\)</span> with stationary distribution <span class="math inline">\(\pi\)</span> depends on how quickly contributions to the distance</p>
<p><span id="eq-distance"><span class="math display">\[
\left|p^{(S)} - \pi\right|
\tag{79}\]</span></span></p>
<p>die out as <span class="math inline">\(S\to\infty\)</span>. Distance (<a href="#eq-distance">Equation&nbsp;79</a>) is dominated by the second largest eigenvalue <span class="math inline">\(\lambda_2\)</span> of <span class="math inline">\(P\)</span>. Since the Markov chain is assumed to be irreducible and aperiodic, we have strictly <span class="math inline">\(|\lambda_2| &lt; 1\)</span>. If <span class="math inline">\(u_2, u_3, \ldots\)</span> are the eigenvectors of <span class="math inline">\(P\)</span> with eigenvalues <span class="math inline">\(1 &gt; |\lambda_2| \ge |\lambda_3| \ge \ldots\)</span>, then we can write the initial distribution as <span class="math display">\[
p^{(0)} = \pi + a_2 u_2 + a_3 u_3 + \ldots
\]</span> After <span class="math inline">\(S\)</span> transitions, the initial <span class="math inline">\(p^{(0)}\)</span> will be propagated to <span class="math display">\[
p^{(S)} = \pi + a_2 \lambda_2^S u_2 + a_3 \lambda_3^S u_3 + \ldots
\]</span> and therefore <span class="math display">\[
\left|p^{(S)} - \pi\right| \sim |\lambda_2|^S
\]</span></p>
<div class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> metropolis_map(Q, p):</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Construct Metropolis kernel from proposal kernel and target distribution</span></span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># M = np.clip(Q, 0., (Q * p).T / p)</span></span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span> np.<span class="bu">min</span>([Q, (Q <span class="op">*</span> p).T <span class="op">/</span> p], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>    i <span class="op">=</span> np.arange(<span class="bu">len</span>(M))</span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a>    M[i,i] <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a>    M[i,i] <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> M.<span class="bu">sum</span>(<span class="dv">0</span>)</span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-11"><a href="#cb95-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> M</span>
<span id="cb95-12"><a href="#cb95-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-13"><a href="#cb95-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> second_eigval(M):</span>
<span id="cb95-14"><a href="#cb95-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb95-15"><a href="#cb95-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Return the second largest (by absolute value) eigenvalue of square matrix `M`</span></span>
<span id="cb95-16"><a href="#cb95-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb95-17"><a href="#cb95-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.sort(np.<span class="bu">abs</span>(np.linalg.eigvals(M)))[::<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>]    </span>
<span id="cb95-18"><a href="#cb95-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-19"><a href="#cb95-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> propagate(M, p0, n):</span>
<span id="cb95-20"><a href="#cb95-20" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb95-21"><a href="#cb95-21" aria-hidden="true" tabindex="-1"></a><span class="co">    Apply Markov-transition `M` to initial distribution `p0` (vector) `n` times and return</span></span>
<span id="cb95-22"><a href="#cb95-22" aria-hidden="true" tabindex="-1"></a><span class="co">    the resulting distribution (vector)</span></span>
<span id="cb95-23"><a href="#cb95-23" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb95-24"><a href="#cb95-24" aria-hidden="true" tabindex="-1"></a>    P <span class="op">=</span> [p0.copy()]</span>
<span id="cb95-25"><a href="#cb95-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb95-26"><a href="#cb95-26" aria-hidden="true" tabindex="-1"></a>        P.append(M <span class="op">@</span> P[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb95-27"><a href="#cb95-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(P)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="78">
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>stepsizes <span class="op">=</span> (<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">20</span>, <span class="dv">50</span>)</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>distances <span class="op">=</span> []</span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a>rates <span class="op">=</span> []</span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a>p0 <span class="op">=</span> np.eye(n)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> stepsize <span class="kw">in</span> stepsizes:</span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> make_proposal(stepsize, n)</span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span> metropolis_map(Q, p)</span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a>    P <span class="op">=</span> propagate(M, p0, <span class="dv">10000</span>)</span>
<span id="cb96-10"><a href="#cb96-10" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> np.fabs(P <span class="op">-</span> p).<span class="bu">sum</span>(<span class="dv">1</span>)</span>
<span id="cb96-11"><a href="#cb96-11" aria-hidden="true" tabindex="-1"></a>    distances.append(d)</span>
<span id="cb96-12"><a href="#cb96-12" aria-hidden="true" tabindex="-1"></a>    rates.append(<span class="op">-</span>np.log(second_eigval(M)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="79">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> plt.rcParams[<span class="st">'axes.prop_cycle'</span>].by_key()[<span class="st">'color'</span>]</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">9</span>,<span class="dv">5</span>))</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(stepsizes)):</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>    ax.plot(distances[i], alpha<span class="op">=</span><span class="fl">0.3</span>, lw<span class="op">=</span><span class="dv">5</span>, </span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span><span class="st">'step size=</span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>(stepsizes[i]))</span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> np.arange(<span class="dv">10000</span>)</span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(stepsizes)):</span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a>    label <span class="op">=</span> <span class="vs">r'$\exp\{-\lambda_2 s\}$'</span> <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a>    ax.plot(np.exp(<span class="op">-</span>rates[i]<span class="op">*</span>t), ls<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.9</span>, </span>
<span id="cb97-10"><a href="#cb97-10" aria-hidden="true" tabindex="-1"></a>            color<span class="op">=</span>colors[i], label<span class="op">=</span>label)</span>
<span id="cb97-11"><a href="#cb97-11" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="vs">r'iteration $s$'</span>)</span>
<span id="cb97-12"><a href="#cb97-12" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r'$|\pi - p^{(s)}|$'</span>)</span>
<span id="cb97-13"><a href="#cb97-13" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb97-14"><a href="#cb97-14" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="fl">0.</span>, <span class="fl">1.</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-80-output-1.png" width="757" height="448"></p>
</div>
</div>
</section>
<section id="achieving-fast-convergence-rates-parameter-tuning" class="level3">
<h3 class="anchored" data-anchor-id="achieving-fast-convergence-rates-parameter-tuning">Achieving fast convergence rates: Parameter tuning</h3>
<p>In our simple discrete example we can analyze the eigenvalues of the Markov transition matrix to assess the convergence behaviour and set our algorithmic parameters in a way that minimizes the magnitude of the second largest eigenvalue. In real-world applications with high dimensional sample spaces and continous variables we are usually not so lucky. We need a heuristic that offers us some guidance.</p>
<p>In our example we’ve seen that the convergence rate depends on the step size. This is generally the case. Another thing that changes with stepsize is the acceptance rate. For a vanishingly small stepsize, the acceptance rate is close to one but the chain does practially not “move” at all and exploration of the typical set will be very slow (corresponding to a large second eigenvalue). On the other hand, if we set the step size to a very large value, the proposal distribution will mostly suggest points outside the typical set, which will get rejected most of the time, so again the chain does not “move” very much and stays at the same point most of the time with some occasional large jumps in between.</p>
<p>This suggests that one can <em>tune</em> algorithmic parameters of MCMC methods (such as stepsize) by aiming for an acceptance rate that lies somewhat in between those extremes. <a href="http://www.stat.columbia.edu/~gelman/book/">Gelman et al, Chapter 12.3</a> quote desired acceptance rates between 0.44 and 0.23, depending on dimension. These are rules of thumb, motivated using the assumtion of a (single, multivariate) normal distribution.</p>
<p>Let’s investigate the stepsize/acceptance-rate behaviour for our discrete toy example, where we have the luxury of knowing the second largest eigenvalue:</p>
<div class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>stepsizes <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">101</span>, <span class="dv">5</span>)</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>rates <span class="op">=</span> []</span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> []</span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> stepsize <span class="kw">in</span> stepsizes:</span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> make_proposal(stepsize, n)</span>
<span id="cb98-7"><a href="#cb98-7" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span> metropolis_map(Q, p)</span>
<span id="cb98-8"><a href="#cb98-8" aria-hidden="true" tabindex="-1"></a>    rates.append(<span class="op">-</span>np.log(second_eigval(M)))</span>
<span id="cb98-9"><a href="#cb98-9" aria-hidden="true" tabindex="-1"></a>    _, acceptance_rate <span class="op">=</span> metropolis_hastings_discrete(</span>
<span id="cb98-10"><a href="#cb98-10" aria-hidden="true" tabindex="-1"></a>        X, p, Q, X[<span class="op">-</span><span class="dv">1</span>], <span class="dv">50_000</span></span>
<span id="cb98-11"><a href="#cb98-11" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb98-12"><a href="#cb98-12" aria-hidden="true" tabindex="-1"></a>    acc.append(acceptance_rate)</span>
<span id="cb98-13"><a href="#cb98-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb98-14"><a href="#cb98-14" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb98-15"><a href="#cb98-15" aria-hidden="true" tabindex="-1"></a>ax.scatter(stepsizes, rates, s<span class="op">=</span><span class="dv">200</span>, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb98-16"><a href="#cb98-16" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r'-$\log|\lambda_2|$'</span>)</span>
<span id="cb98-17"><a href="#cb98-17" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'stepsize'</span>)<span class="op">;</span></span>
<span id="cb98-18"><a href="#cb98-18" aria-hidden="true" tabindex="-1"></a>ax2 <span class="op">=</span> ax.twinx()</span>
<span id="cb98-19"><a href="#cb98-19" aria-hidden="true" tabindex="-1"></a>ax2.scatter(stepsizes, acc, s<span class="op">=</span><span class="dv">200</span>, color<span class="op">=</span><span class="st">"red"</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb98-20"><a href="#cb98-20" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">"acceptance rate"</span>, color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb98-21"><a href="#cb98-21" aria-hidden="true" tabindex="-1"></a>ax2.set_ylim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb98-22"><a href="#cb98-22" aria-hidden="true" tabindex="-1"></a>ax2.tick_params(axis<span class="op">=</span><span class="st">'y'</span>, color<span class="op">=</span><span class="st">'red'</span>, labelcolor<span class="op">=</span><span class="st">'red'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-81-output-1.png" width="531" height="448"></p>
</div>
</div>
</section>
<section id="warmup-phase-burn-in" class="level3">
<h3 class="anchored" data-anchor-id="warmup-phase-burn-in">Warmup phase (“Burn-in”)</h3>
<p>As we’ve seen, the Markov chain does not start from the stationary distribution, so <span class="math display">\[
\mathbb E_{p^{(s)}}[f] \not= \mathbb E_p[f]\, ,
\]</span> and the difference can be substantial for small <span class="math inline">\(s\)</span>, thereby inducing significant bias to the Monte Carlo estimator:</p>
<p><span class="math display">\[
\frac{1}{S} \sum_{s=1}^S f\bigl(x^{(s)}\bigr).
\]</span></p>
<p>To minimize the bias from this initial phase in which the chain has not yet achieved convergence, it is common practice to discard the first samples <span class="math inline">\(x^{(0)}, \ldots, x^{(B)}\)</span>. This phase is often called <em>warm-up</em> or <em>burn-in</em>. It is assumed that the samples <span class="math inline">\(x^{(B+1)}\)</span> will approximately follow the target distribution <span class="math inline">\(p\)</span>. The Monte Carlo approximation then becomes: <span id="eq-burnin"><span class="math display">\[
\frac{1}{S-B} \sum_{s=B+1}^S f\bigl(x^{(s)}\bigr)\, .
\tag{80}\]</span></span></p>
<p>The question that arises in practice is of course how to select <span class="math inline">\(B\)</span>, or in other words how to assess after how many steps the chain has reached approximate converge.</p>
</section>
<section id="monitoring-convergence" class="level3">
<h3 class="anchored" data-anchor-id="monitoring-convergence">Monitoring convergence</h3>
<p>Determining whether or when our simulation has reached approximate convergence is unfortunately not very easy in most cases. The converse however is usually relatively straigtforward: Cases in which chains have not reached convergence are often easy to spot. Several concepts are of help:</p>
<section id="trace-plots" class="level4">
<h4 class="anchored" data-anchor-id="trace-plots">Trace plots</h4>
<p>The first is to inspect the samples of each random variable individually. One of the most useful tools is to plot the sequence of samples for each of the simulated variables <em>as an actual sequence</em>, with the iteration number <span class="math inline">\(s\)</span> on the x-axis and the sample <span class="math inline">\(x^{(s)}\)</span> on the y-axis. Such a plot is commonly called a <a href="https://python.arviz.org/en/stable/api/generated/arviz.plot_trace.html"><strong>trace plot</strong></a>. This kind of plot can reveal several problems if they’re present, e.g.&nbsp;non-stationarity (i.e.&nbsp;if the chain has not yet converged) or infrequent jumps between multiple modes. We will see an example below.</p>
</section>
<section id="simulation-of-multiple-chains" class="level4">
<h4 class="anchored" data-anchor-id="simulation-of-multiple-chains">Simulation of multiple chains</h4>
<p>Another useful strategy is to run <em>multiple</em> independent simulations either in parallel or in sequence, and to start each of the simulations at very different initial points. This can reveal problems in the simulation that would be impossible to discover with a single simulation run only. An example of this is illustrated in the left plot of figure <a href="#fig-traceplot-convergence">Figure&nbsp;1</a>, which shows two independent simulations that were started at different points in the sample space. Each sequence individually looks stationary, and if only one simulation would have been run, one might have concluded that the samples give a good representation of the desired target distribution. Observing that the second simulation also looks stationary but covers a different subset of the sample space, reveals that the target distribution has at least two modes and our simulations underexplore the sample space. We need to adapt the simulation parameters (e.g.&nbsp;increase stepsize)!</p>
<div class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>n_chains <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a><span class="co"># two peaks of small equal width</span></span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a><span class="co"># relatively large stepsize</span></span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a><span class="co"># chains both start within typical set</span></span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a>p1 <span class="op">=</span> make_target(np.array([<span class="op">-</span><span class="fl">3.0</span>, <span class="fl">5.0</span>]), np.array([<span class="fl">1.0</span>, <span class="fl">1.0</span>]), weights, n)</span>
<span id="cb99-7"><a href="#cb99-7" aria-hidden="true" tabindex="-1"></a>initial_states <span class="op">=</span> [X[<span class="dv">20</span>], X[<span class="dv">70</span>]]</span>
<span id="cb99-8"><a href="#cb99-8" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> make_proposal(<span class="dv">10</span>, n)</span>
<span id="cb99-9"><a href="#cb99-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-10"><a href="#cb99-10" aria-hidden="true" tabindex="-1"></a>samples1 <span class="op">=</span> [</span>
<span id="cb99-11"><a href="#cb99-11" aria-hidden="true" tabindex="-1"></a>    metropolis_hastings_discrete(</span>
<span id="cb99-12"><a href="#cb99-12" aria-hidden="true" tabindex="-1"></a>        X, p1, Q, initial_states[i], <span class="dv">2_000</span>, seed<span class="op">=</span><span class="dv">3</span> <span class="op">+</span> i</span>
<span id="cb99-13"><a href="#cb99-13" aria-hidden="true" tabindex="-1"></a>    )[<span class="dv">0</span>]</span>
<span id="cb99-14"><a href="#cb99-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_chains)</span>
<span id="cb99-15"><a href="#cb99-15" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb99-16"><a href="#cb99-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-17"><a href="#cb99-17" aria-hidden="true" tabindex="-1"></a><span class="co"># two peaks of very large equal width</span></span>
<span id="cb99-18"><a href="#cb99-18" aria-hidden="true" tabindex="-1"></a><span class="co"># very small stepsize</span></span>
<span id="cb99-19"><a href="#cb99-19" aria-hidden="true" tabindex="-1"></a><span class="co"># chains both start outside typical set</span></span>
<span id="cb99-20"><a href="#cb99-20" aria-hidden="true" tabindex="-1"></a>p2 <span class="op">=</span> make_target(np.array([<span class="op">-</span><span class="fl">3.0</span>, <span class="fl">5.0</span>]), np.array([<span class="fl">10.0</span>, <span class="fl">10.0</span>]), weights, n)</span>
<span id="cb99-21"><a href="#cb99-21" aria-hidden="true" tabindex="-1"></a>initial_states <span class="op">=</span> [X[<span class="dv">0</span>], X[<span class="op">-</span><span class="dv">1</span>]]</span>
<span id="cb99-22"><a href="#cb99-22" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> make_proposal(<span class="dv">1</span>, n)</span>
<span id="cb99-23"><a href="#cb99-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-24"><a href="#cb99-24" aria-hidden="true" tabindex="-1"></a>samples2 <span class="op">=</span> [</span>
<span id="cb99-25"><a href="#cb99-25" aria-hidden="true" tabindex="-1"></a>    metropolis_hastings_discrete(</span>
<span id="cb99-26"><a href="#cb99-26" aria-hidden="true" tabindex="-1"></a>        X, p2, Q, initial_states[<span class="dv">0</span>], <span class="dv">2_000</span>, seed<span class="op">=</span><span class="dv">40</span></span>
<span id="cb99-27"><a href="#cb99-27" aria-hidden="true" tabindex="-1"></a>    )[<span class="dv">0</span>],</span>
<span id="cb99-28"><a href="#cb99-28" aria-hidden="true" tabindex="-1"></a>    metropolis_hastings_discrete(</span>
<span id="cb99-29"><a href="#cb99-29" aria-hidden="true" tabindex="-1"></a>        X, p2, Q, initial_states[<span class="dv">1</span>], <span class="dv">2_000</span>, seed<span class="op">=</span><span class="dv">2</span></span>
<span id="cb99-30"><a href="#cb99-30" aria-hidden="true" tabindex="-1"></a>    )[<span class="dv">0</span>],</span>
<span id="cb99-31"><a href="#cb99-31" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb99-32"><a href="#cb99-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-33"><a href="#cb99-33" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">3</span>))</span>
<span id="cb99-34"><a href="#cb99-34" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].plot(samples1[<span class="dv">0</span>], label<span class="op">=</span><span class="st">"Chain 1"</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb99-35"><a href="#cb99-35" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].plot(samples1[<span class="dv">1</span>], label<span class="op">=</span><span class="st">"Chain 2"</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb99-36"><a href="#cb99-36" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].set_xlabel(<span class="st">"iteration $s$"</span>)</span>
<span id="cb99-37"><a href="#cb99-37" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].set_ylabel(<span class="st">"state"</span>)</span>
<span id="cb99-38"><a href="#cb99-38" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].plot(samples2[<span class="dv">0</span>], label<span class="op">=</span><span class="st">"Chain 1"</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb99-39"><a href="#cb99-39" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].plot(samples2[<span class="dv">1</span>], label<span class="op">=</span><span class="st">"Chain 2"</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb99-40"><a href="#cb99-40" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].set_xlabel(<span class="st">"iteration $s$"</span>)</span>
<span id="cb99-41"><a href="#cb99-41" aria-hidden="true" tabindex="-1"></a>plt.show()  <span class="co"># to make quarto show the caption</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-traceplot-convergence" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="lecture_files/figure-html/fig-traceplot-convergence-output-1.png" width="756" height="293" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Examples of two challenges in determining approximate convergence. The left plot show two independent simulation runs which individually look like they might have converged. Only the fact that two independent simluations were run reveals that either chain is stuck in a differen mode of the target distribution. The right plot shows again two independent simulation runs. This time both chains cover the sample space equally, but neither of the chains has reached (approximate) convergence. Figure reproduced loosely from <a href="http://www.stat.columbia.edu/~gelman/book/">Gelman et al.&nbsp;Chapter 11.4</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="convergence-metrics" class="level4">
<h4 class="anchored" data-anchor-id="convergence-metrics">Convergence metrics</h4>
<p>Third some statistics have been designed to help assessing whether the samples from multiple independent simulation runs mix well. The most widely used such statistic is the “potential scale reduction” <span class="math inline">\(\hat{R}\)</span>, sometimes also called Gelman-Rubin statistic, or simply r-hat. The basic idea behind that statistic can again be motivated from <a href="#fig-traceplot-convergence">Figure&nbsp;1</a>: Looking at the left plot we see that the variance of the samples within each individual sequence is much lower than the variance between different sequences. Before the between-sequence and in-sequence variances are compared, the sequences are split in half and thus the number of sequences is doubled. This can again be motivated using figure <a href="#fig-traceplot-convergence">Figure&nbsp;1</a>. Without splitting the sequences in half, the between-sequence variance would approximately be equal to the within-sequence variance on the right plot in figure <a href="#fig-traceplot-convergence">Figure&nbsp;1</a>.</p>
<p>Assume that after splitting we have <span class="math inline">\(K\)</span> sequences each with a number of <span class="math inline">\(S\)</span> samples. We refer to a single sample in sequence <span class="math inline">\(k\)</span> at iteration <span class="math inline">\(s\)</span> as <span class="math inline">\(x^{(s)}_k\)</span>. <span class="math inline">\(\hat{R}\)</span> is then calculated in the following way: First <span class="math inline">\(B\)</span> and <span class="math inline">\(W\)</span>, the between- and within-sequence variances are computed: <span class="math display">\[
\begin{aligned}
B &amp;= \frac{S}{K - 1} \sum_{k=1}^K \left(\bar{x}_k - \bar{x}\right)^2, \quad \text{where} \quad \bar{x}_k = \frac{1}{S} \sum_{s=1}^S x^{(s)}_k, \quad \bar{x} = \frac{1}{K} \sum_{k=1}^K \bar{x}_k \\
W &amp;= \frac{1}{K} \sum_{k=1}^K \sigma_k^2, \quad \text{where} \quad \sigma_k^2 = \frac{1}{S - 1} \sum_{s=1}^S(x^{(s)}_k - \bar{x}_k)^2
\end{aligned}
\]</span></p>
<p>From these the potential scale reduction <span class="math inline">\(\hat{R}\)</span> can be calculated: <span class="math display">\[
\hat{R} = \sqrt{\frac{\frac{(S - 1)}{S} W + \frac{1}{S} B}{W}}
\]</span></p>
<p>This quantity is larger than 1 but we want it to be close. Gelman et al quote <span class="math inline">\(1.1\)</span> as an upper threshold. Nowadays usually tighter thresholds are desired.</p>
<p>Let us re-visit the two examples of figure <a href="#fig-traceplot-convergence">Figure&nbsp;1</a>, but let us also add a third example that demonstrates what “good” simulation runs look like. We also calculate <span class="math inline">\(\hat{R}\)</span> for all three examples. The result is shown in figure <a href="#fig-traceplot-convergence-repeated">Figure&nbsp;2</a></p>
<div class="cell" data-execution_count="82">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> r_hat(chains):</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculate Gelman-Rubin potential scale reduction statistic.</span></span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># double the number of chains by splitting each chain in half</span></span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a>    chains <span class="op">=</span> np.concatenate([np.split(c, <span class="dv">2</span>) <span class="cf">for</span> c <span class="kw">in</span> chains])</span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># now have K chains, each with S samples:</span></span>
<span id="cb100-8"><a href="#cb100-8" aria-hidden="true" tabindex="-1"></a>    K, S <span class="op">=</span> chains.shape </span>
<span id="cb100-9"><a href="#cb100-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Between-chain variance:</span></span>
<span id="cb100-10"><a href="#cb100-10" aria-hidden="true" tabindex="-1"></a>    B <span class="op">=</span> S <span class="op">*</span> np.var(np.mean(chains, axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb100-11"><a href="#cb100-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Within-chain variance:</span></span>
<span id="cb100-12"><a href="#cb100-12" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> np.mean(np.var(chains, axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb100-13"><a href="#cb100-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-14"><a href="#cb100-14" aria-hidden="true" tabindex="-1"></a>    var_plus <span class="op">=</span> ((S <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> W <span class="op">+</span> B) <span class="op">/</span> S</span>
<span id="cb100-15"><a href="#cb100-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.sqrt(var_plus <span class="op">/</span> W)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>initial_states <span class="op">=</span> [X[<span class="dv">20</span>], X[<span class="dv">70</span>]]</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> make_proposal(<span class="dv">60</span>, n)</span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a>samples3 <span class="op">=</span> [</span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a>    metropolis_hastings_discrete(</span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a>        X, p1, Q, initial_states[<span class="dv">0</span>], <span class="dv">2_000</span>, seed<span class="op">=</span><span class="dv">0</span></span>
<span id="cb101-7"><a href="#cb101-7" aria-hidden="true" tabindex="-1"></a>    )[<span class="dv">0</span>],</span>
<span id="cb101-8"><a href="#cb101-8" aria-hidden="true" tabindex="-1"></a>    metropolis_hastings_discrete(</span>
<span id="cb101-9"><a href="#cb101-9" aria-hidden="true" tabindex="-1"></a>        X, p1, Q, initial_states[<span class="dv">1</span>], <span class="dv">2_000</span>, seed<span class="op">=</span><span class="dv">1</span></span>
<span id="cb101-10"><a href="#cb101-10" aria-hidden="true" tabindex="-1"></a>    )[<span class="dv">0</span>],</span>
<span id="cb101-11"><a href="#cb101-11" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb101-12"><a href="#cb101-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-13"><a href="#cb101-13" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">3</span>), sharey<span class="op">=</span><span class="va">True</span>, sharex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb101-14"><a href="#cb101-14" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].plot(samples1[<span class="dv">0</span>], label<span class="op">=</span><span class="st">"Chain 1"</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb101-15"><a href="#cb101-15" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].plot(samples1[<span class="dv">1</span>], label<span class="op">=</span><span class="st">"Chain 2"</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb101-16"><a href="#cb101-16" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].set_xlabel(<span class="st">"iteration $s$"</span>)</span>
<span id="cb101-17"><a href="#cb101-17" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].set_ylabel(<span class="st">"state"</span>)</span>
<span id="cb101-18"><a href="#cb101-18" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].set_title(<span class="vs">r"$\hat</span><span class="sc">{R}</span><span class="vs">$: "</span> <span class="op">+</span> <span class="ss">f"</span><span class="sc">{</span>r_hat(samples1)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb101-19"><a href="#cb101-19" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].plot(samples2[<span class="dv">0</span>], label<span class="op">=</span><span class="st">"Chain 1"</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb101-20"><a href="#cb101-20" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].plot(samples2[<span class="dv">1</span>], label<span class="op">=</span><span class="st">"Chain 2"</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb101-21"><a href="#cb101-21" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].set_xlabel(<span class="st">"iteration $s$"</span>)</span>
<span id="cb101-22"><a href="#cb101-22" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].set_title(<span class="vs">r"$\hat</span><span class="sc">{R}</span><span class="vs">$: "</span> <span class="op">+</span> <span class="ss">f"</span><span class="sc">{</span>r_hat(samples2)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb101-23"><a href="#cb101-23" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">2</span>].plot(samples3[<span class="dv">0</span>], label<span class="op">=</span><span class="st">"Chain 1"</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb101-24"><a href="#cb101-24" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">2</span>].plot(samples3[<span class="dv">1</span>], label<span class="op">=</span><span class="st">"Chain 2"</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb101-25"><a href="#cb101-25" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">2</span>].set_xlabel(<span class="st">"iteration $s$"</span>)</span>
<span id="cb101-26"><a href="#cb101-26" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">2</span>].set_title(<span class="vs">r"$\hat</span><span class="sc">{R}</span><span class="vs">$: "</span> <span class="op">+</span> <span class="ss">f"</span><span class="sc">{</span>r_hat(samples3)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb101-27"><a href="#cb101-27" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-traceplot-convergence-repeated" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="lecture_files/figure-html/fig-traceplot-convergence-repeated-output-1.png" width="773" height="323" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: Repetition of figure <a href="#fig-traceplot-convergence">Figure&nbsp;1</a>, with an additional figure on the right showing simulations of the same system as in the left plot, but with a much larger stepsize that leads to a much better exploration of the typical set. All three plots show the r-hat metric in the title.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="dependent-samples" class="level2">
<h2 class="anchored" data-anchor-id="dependent-samples">Dependent samples</h2>
<p>When taking the step from standard Monte Carlo sampling to Markov-Chain Monte Carlo, we deliberately gave up the property that the samples are drawn independently. This dependence between successive samples means that the variance of the MCMC estimator is higher than that of standard MC estimator based on the same number of draws.</p>
<p>It can be shown that the asymptotic variance of the MCMC estimator converges against <span id="eq-variance"><span class="math display">\[
\text{var}\left[\frac{1}{S}\sum_s f\bigl(x^{(s)}\bigr) \right] \xrightarrow[S\to\infty]{} \frac{1}{S} \text{var}_p[f] (1 + 2 \sum_{t \ge 1} \rho_t)  
\tag{81}\]</span></span> where <span class="math inline">\(\rho_t\)</span> is the <em>autocorrelation</em> of the sequence of samples at lag <span class="math inline">\(t\)</span> <span id="eq-correlation"><span class="math display">\[
\rho_t = \text{corr}[f(x^{(s)}), f(x^{(s + t)})] \, .
\tag{82}\]</span></span> The higher the autocorrelation, the larger the variance of the MCMC estimator. For uncorrelated samples <span class="math inline">\(\rho_t=0\)</span> and we are back to the standard variance of Monte Carlo estimators: <span class="math inline">\(\text{var}[f]/S\)</span>.</p>
<section id="effective-sample-size-1" class="level3">
<h3 class="anchored" data-anchor-id="effective-sample-size-1">Effective sample size</h3>
<p>Another way to look at this is that correlations decrease the <em>effective sample size</em> (ESS).</p>
<p>A way of defining an effective sample size <span class="math inline">\(S_\text{eff}\)</span> is as the number <span class="math inline">\(S_\text{eff}\)</span> of hypothetical <em>independent</em> standard-MC samples required to achieve the same variance as we achieve with the <span class="math inline">\(S\)</span> correlated samples from our MCMC estimator: <span id="eq-ess"><span class="math display">\[
S_{\text{eff}} = \frac{S}{1 + 2 \sum_{t \ge 1} \rho_t} = \frac{S}{\text{IACT}}
\tag{83}\]</span></span> where <span class="math inline">\(\text{IACT} = 1 + 2 \sum_{t \ge 1} \rho_t\)</span> is the <em>integrated auto-correlation time</em>.</p>
<div class="cell" data-execution_count="84">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="co"># autocorrelation analysis of bimodal target</span></span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> autocorrelation(x, n<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a><span class="co">    auto-correlation of a times series</span></span>
<span id="cb102-6"><a href="#cb102-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-7"><a href="#cb102-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters</span></span>
<span id="cb102-8"><a href="#cb102-8" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb102-9"><a href="#cb102-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-10"><a href="#cb102-10" aria-hidden="true" tabindex="-1"></a><span class="co">    x: array containing time series</span></span>
<span id="cb102-11"><a href="#cb102-11" aria-hidden="true" tabindex="-1"></a><span class="co">    n: Optional integer specifying maximal lag for which to compute the auto-correlation</span></span>
<span id="cb102-12"><a href="#cb102-12" aria-hidden="true" tabindex="-1"></a><span class="co">       If `n` is None, the maximal lag is defined as the first lag for which the sum of</span></span>
<span id="cb102-13"><a href="#cb102-13" aria-hidden="true" tabindex="-1"></a><span class="co">       two successive lagged autocorrelations is negative.</span></span>
<span id="cb102-14"><a href="#cb102-14" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb102-15"><a href="#cb102-15" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x <span class="op">-</span> x.mean()</span>
<span id="cb102-16"><a href="#cb102-16" aria-hidden="true" tabindex="-1"></a>    rhos <span class="op">=</span> []</span>
<span id="cb102-17"><a href="#cb102-17" aria-hidden="true" tabindex="-1"></a>    max_lag <span class="op">=</span> <span class="bu">len</span>(x) <span class="cf">if</span> n <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> n</span>
<span id="cb102-18"><a href="#cb102-18" aria-hidden="true" tabindex="-1"></a>    l1 <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb102-19"><a href="#cb102-19" aria-hidden="true" tabindex="-1"></a>    l2 <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb102-20"><a href="#cb102-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_lag):</span>
<span id="cb102-21"><a href="#cb102-21" aria-hidden="true" tabindex="-1"></a>        autocov_i <span class="op">=</span> np.mean(x[i:] <span class="op">*</span> x[:<span class="bu">len</span>(x) <span class="op">-</span> i])</span>
<span id="cb102-22"><a href="#cb102-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (n <span class="kw">is</span> <span class="va">None</span>) <span class="kw">and</span> (i <span class="op">&gt;</span> <span class="dv">1</span>) <span class="kw">and</span> (l1 <span class="op">+</span> l2 <span class="op">&lt;</span> <span class="dv">0</span>):</span>
<span id="cb102-23"><a href="#cb102-23" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb102-24"><a href="#cb102-24" aria-hidden="true" tabindex="-1"></a>        rhos.append(autocov_i)</span>
<span id="cb102-25"><a href="#cb102-25" aria-hidden="true" tabindex="-1"></a>        l2 <span class="op">=</span> l1</span>
<span id="cb102-26"><a href="#cb102-26" aria-hidden="true" tabindex="-1"></a>        l1 <span class="op">=</span> autocov_i</span>
<span id="cb102-27"><a href="#cb102-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> n <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb102-28"><a href="#cb102-28" aria-hidden="true" tabindex="-1"></a>        rhos <span class="op">=</span> rhos[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb102-29"><a href="#cb102-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(rhos) <span class="op">/</span> np.var(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="85">
<div class="sourceCode cell-code" id="cb103"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">41</span>) </span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>stepsizes <span class="op">=</span> (<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">20</span>, <span class="dv">60</span>)</span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">20_000</span></span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true" tabindex="-1"></a>ac <span class="op">=</span> []</span>
<span id="cb103-5"><a href="#cb103-5" aria-hidden="true" tabindex="-1"></a>n_effs <span class="op">=</span> []</span>
<span id="cb103-6"><a href="#cb103-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> stepsize <span class="kw">in</span> stepsizes:</span>
<span id="cb103-7"><a href="#cb103-7" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> make_proposal(stepsize, <span class="bu">len</span>(p))</span>
<span id="cb103-8"><a href="#cb103-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-9"><a href="#cb103-9" aria-hidden="true" tabindex="-1"></a>    S, acceptance_rate <span class="op">=</span> metropolis_hastings_discrete(</span>
<span id="cb103-10"><a href="#cb103-10" aria-hidden="true" tabindex="-1"></a>        X, p, Q, X[<span class="op">-</span><span class="dv">1</span>], n_samples</span>
<span id="cb103-11"><a href="#cb103-11" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb103-12"><a href="#cb103-12" aria-hidden="true" tabindex="-1"></a>    ac.append(autocorrelation(S<span class="op">*</span><span class="fl">1.</span>, <span class="dv">10000</span>))</span>
<span id="cb103-13"><a href="#cb103-13" aria-hidden="true" tabindex="-1"></a>    n_effs.append(</span>
<span id="cb103-14"><a href="#cb103-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">round</span>(</span>
<span id="cb103-15"><a href="#cb103-15" aria-hidden="true" tabindex="-1"></a>            n_samples <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.<span class="bu">sum</span>(autocorrelation(S <span class="op">*</span> <span class="fl">1.</span>)))</span>
<span id="cb103-16"><a href="#cb103-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb103-17"><a href="#cb103-17" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb103-18"><a href="#cb103-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"stepsize=</span><span class="sc">{</span>stepsize<span class="sc">}</span><span class="ss">: acceptance-rate=</span><span class="sc">{</span>acceptance_rate<span class="sc">:.1%}</span><span class="ss">, ESS=</span><span class="sc">{</span>n_effs[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb103-19"><a href="#cb103-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-20"><a href="#cb103-20" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">9</span>,<span class="dv">5</span>))</span>
<span id="cb103-21"><a href="#cb103-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(stepsizes)):</span>
<span id="cb103-22"><a href="#cb103-22" aria-hidden="true" tabindex="-1"></a>    ax.plot(ac[i], alpha<span class="op">=</span><span class="fl">0.3</span>, lw<span class="op">=</span><span class="dv">5</span>, </span>
<span id="cb103-23"><a href="#cb103-23" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span><span class="ss">f"stepsize=</span><span class="sc">{</span>stepsizes[i]<span class="sc">}</span><span class="ss">, $S_</span><span class="ch">{{</span><span class="ss">eff</span><span class="ch">}}</span><span class="ss">$=</span><span class="sc">{</span>n_effs[i]<span class="sc">:d}</span><span class="ss">"</span>)</span>
<span id="cb103-24"><a href="#cb103-24" aria-hidden="true" tabindex="-1"></a>ax.axhline(<span class="dv">0</span>, ls<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb103-25"><a href="#cb103-25" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="dv">0</span>, <span class="dv">4000</span>)</span>
<span id="cb103-26"><a href="#cb103-26" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="vs">r'lag $t$'</span>)</span>
<span id="cb103-27"><a href="#cb103-27" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r'autocorrelation $\rho_t$'</span>)</span>
<span id="cb103-28"><a href="#cb103-28" aria-hidden="true" tabindex="-1"></a>ax.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>stepsize=5: acceptance-rate=57.4%, ESS=7</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>stepsize=10: acceptance-rate=59.1%, ESS=10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>stepsize=15: acceptance-rate=47.0%, ESS=21</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>stepsize=20: acceptance-rate=36.6%, ESS=27</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>stepsize=60: acceptance-rate=30.4%, ESS=908</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="85">
<pre><code>&lt;matplotlib.legend.Legend at 0x7fb0e5dd4730&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="lecture_files/figure-html/cell-86-output-7.png" width="793" height="442"></p>
</div>
</div>
</section>
<section id="practical-summary-from-viholas-lecture-notes" class="level3">
<h3 class="anchored" data-anchor-id="practical-summary-from-viholas-lecture-notes">Practical summary (from Vihola’s lecture notes)</h3>
<p>When using MCMC, always do the following checks:</p>
<ol type="1">
<li><p>Plot MCMC traces of the variables and key functions of the variables. They should look stationary after burn-in.</p></li>
<li><p>Make multiple MCMC runs from different initial states and check that the marginal distributions (or the estimators) look similar. This test reveals if your chain is “almost reducible”.</p></li>
<li><p>Plot sample autocorrelations of the variables and functions.</p></li>
<li><p>Calculate the effective sample size and check that it is reasonably large.</p></li>
<li><p>Calculate <span class="math inline">\(\hat{R}\)</span> and check that it is close to 1 (&lt; 1.1)</p></li>
</ol>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script type="application/vnd.jupyter.widget-state+json">
{"state":{"01c83853e6a340d59b715985d3c34c27":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0440a100dac44459a1da07a21d2d59f6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"DropdownModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"DropdownModel","_options_labels":["41","1234","43"],"_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"DropdownView","description":"Seed: ","description_allow_html":false,"disabled":false,"index":0,"layout":"IPY_MODEL_e5979b14c7c84768844ca403c57b9520","style":"IPY_MODEL_36f3ee723f68426286b63346fc0c67db","tabbable":null,"tooltip":null}},"0df6dbd99de142baadd0c7cabcdf7928":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b1921f809c845939ee6cd0f60eb2fcb":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b6d5ee669ba4ebab6bcc981148d8a7c":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33e06d3337074b41abfb1059ed784088":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","description_width":""}},"36f3ee723f68426286b63346fc0c67db":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","description_width":""}},"37284889f7c8465581ac012a16c2293e":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_1b1921f809c845939ee6cd0f60eb2fcb","msg_id":"","outputs":[],"tabbable":null,"tooltip":null}},"7547c72984d941fa91923e9725c34356":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"VBoxModel","state":{"_dom_classes":["widget-interact"],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"VBoxView","box_style":"","children":[],"layout":"IPY_MODEL_e9f7231c7bf848e9aaf84f4bca618bd5","tabbable":null,"tooltip":null}},"aca9b514a4eb4eba996d5704684d07a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"DropdownModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"DropdownModel","_options_labels":["41","1234","43"],"_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"DropdownView","description":"Seed: ","description_allow_html":false,"disabled":false,"index":0,"layout":"IPY_MODEL_e5979b14c7c84768844ca403c57b9520","style":"IPY_MODEL_36f3ee723f68426286b63346fc0c67db","tabbable":null,"tooltip":null}},"c4ca89f02b0f4a62b47fa47eef163278":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"VBoxModel","state":{"_dom_classes":["widget-interact"],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_0440a100dac44459a1da07a21d2d59f6","IPY_MODEL_d397dbe0d2b44986b141d8469d05d99d"],"layout":"IPY_MODEL_e9f7231c7bf848e9aaf84f4bca618bd5","tabbable":null,"tooltip":null}},"d397dbe0d2b44986b141d8469d05d99d":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_1b1921f809c845939ee6cd0f60eb2fcb","msg_id":"","outputs":[{"name":"stdout","output_type":"stream","text":"acceptance rate: 53.6%\n65.5% of all samples are in left mode\n"},{"data":{"image/png":"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\n","text/plain":"<Figure size 864x480 with 1 Axes>"},"metadata":{"image/png":{"height":466,"width":739}},"output_type":"display_data"}],"tabbable":null,"tooltip":null}},"e5979b14c7c84768844ca403c57b9520":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9f7231c7bf848e9aaf84f4bca618bd5":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>