---
jupyter: python3
title: Exercise 2
---

# Problem 1: Law of Large Numbers and Central Limit Theorem

## a)

The Law of Large Numbers (LLN) states, that for a sequence of [independent and identically distributed (iid.)](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) random numbers $X_1, X_2, X_3, ...$ with **finite** mean $\mu = \mathbb{E}[X_i]$, the sample mean mean of $X_1$ through $X_n$

$$
\bar{X}_n = \frac{X_1 + ... + X_n}{n}
$$

converges to the true mean $\mu$ as $n \rightarrow \infty$ (note: The *strong* and *weak* LLN only differ in the way *how* the sample mean converges to $\mu$).

Verify this empirically for the following three distributions:

* A uniform distribution on the interval $[1, 4]$
* A gamma distribution with shape $k = 3$ and scale $\theta = 1$
* A (Type-1) Pareto distribution with shape $\alpha = 3/2$

Is any of these surprising?

### True mean of distributions 
```{python}
import numpy as np
from scipy.stats import uniform, gamma, pareto
```

```{python}
uniform(1, 4).mean()
```

```{python}
gamma(3, scale=1).mean()
```

```{python}
pareto(1.5).mean()
```

```{python}
import matplotlib.pyplot as plt

x = np.linspace(0, 6, 100)

pdf_uniform = uniform(1, 4).pdf(x)
pdf_gamma = gamma(3, scale=1).pdf(x)
pdf_pareto = pareto(1.5).pdf(x)
pdfs = [pdf_uniform, pdf_gamma, pdf_pareto]
titles = ["Uniform", "Gamma", "Pareto"]

fig, axs = plt.subplots(1, 3, figsize=(9, 3), sharex=True, sharey=True)

for i, (pdf, title) in enumerate(zip(pdfs, titles)):
    axs[i].vlines(3, 0, max(map(max, pdfs)), colors="k", linestyles="dashed")
    axs[i].plot(x, pdf)
    axs[i].set_title(title)
```


```{python}
N = 1_000
# draw N samples from each distribution:
x_uniform = uniform(1, 4).rvs(N)
x_gamma = gamma(3, scale=1).rvs(N)
x_pareto = pareto(1.5).rvs(N)

def cummean(x):
    return np.cumsum(x) / np.arange(1, len(x) + 1)

fig, axs = plt.subplots(1, 3, figsize=(9, 3), sharey=True)

for i, (x, title) in enumerate(zip([x_uniform, x_gamma, x_pareto], titles)):
    axs[i].hlines(3, 0, len(x), colors="k", linestyles="dashed")
    axs[i].plot(cummean(x))
    axs[i].set_title(title)
```

The Law of Large Numbers holds for the Pareto distribution with shape $\alpha = 3/2$ even though the distribution has **non-finite variance**!
However in this case, the LLN is of not much practical use for the Monte Carlo method since it only gives an asymptotic guarantee. In practice with finite sample sizes, we suffer from high sample variances.

## b)

Try to empirically verify the Central Limit Theorem (CLT) for the three distributions above

If $X_i$ iid. and $\mathbb{E}[X_i] = \mu$ and std$[X_i] = \sigma$ finite, then the CLT says something about the asymptotic ($n \rightarrow \infty$) distribution of the sample mean $X_n$ (as defined above), namely:

$$
\sqrt{n}\Bigl(\frac{\bar{X}_n - \mu}{\sigma}\Bigr) \rightarrow \mathcal{N}(0, 1)
$$
as $n \rightarrow \infty$, where $\mathcal{N}(0, 1)$ is the standard normal distribution.

```{python}
from scipy.stats import norm
from ipywidgets import interact, FloatLogSlider

def sample_means(distribution, N, n_repetitions):
    return np.array([np.mean(distribution.rvs(N)) for _ in range(n_repetitions)])


@interact(
    N=FloatLogSlider(
        value=2,
        base=2,
        min=0, # max exponent of base
        max=15, # min exponent of base
        step=1, # exponent step
        description='N'
    )
)
def plot(N):
    N = int(N)
    titles = ["Uniform", "Gamma", "Pareto"]
    distributions = [uniform(1, 4), gamma(3, scale=1), pareto(1.5)]

    fig, axs = plt.subplots(1, 3, figsize=(9, 3), sharex=True, sharey=True)

    x = np.linspace(-5, 5, 100)

    for i, (dist, title) in enumerate(zip(distributions, titles)):
        means = sample_means(dist, N, 1_000)
        axs[i].hist(np.sqrt(N) * (means - dist.mean()), bins=20, range=(-5, 5), density=True)
        axs[i].set_title(title)
        if title != "Pareto":
            axs[i].plot(x, norm(0, dist.std()).pdf(x), "r")
    fig.suptitle(f"N = {N}")
    fig.tight_layout()
```


# Problem 2: Pseudo Random Number Generators

## a)

Implement a Linear Congruential Generator with a variable period length that allows you to sample pseudorandom integers and floating point numbers.

```{python}
class LCG:
    """
    Naive implementation of a Linear Congruential Generator
    loosely mimicking the numpy.random.Generator interface
    """

    def __init__(self, m, a, c, seed):
        self.m = m
        self.a = a
        self.c = c
        self.state = seed

    def _step(self):
        self.state = (self.a * self.state + self.c) % self.m
        return self.state
    
    def integers(self, high, size):
        return np.array([self._step() % high for _ in range(size)])
    
    def uniform(self, size):
        # note that this is neither efficient (as seen in the last exercise)
        # nor good (using a potentially less than 64-bit period length for a 64-bit float)
        # don't ever do this except for demonstrating that it's bad ðŸ™ƒ
        return np.array([self._step() / self.m for _ in range(size)])
```

```{python}
plt.hist(LCG(2**32, 1664525, 1013904223, seed=42).uniform(size=1_000_000))
```

## b)

The [diehard](https://en.wikipedia.org/wiki/Diehard_tests) and [dieharder](https://sites.google.com/site/astudyofentropy/background-information/the-tests/dieharder-test-descriptions#T203) tests are a (historically influential) set of statistical tests for pseudo-random number generators.

[One of the tests](https://sites.google.com/site/astudyofentropy/background-information/the-tests/dieharder-test-descriptions#T203) uses sums of lagged random floats in the interval $[0, 1)$ as its test statistic:
From an array of $n$ consecutive random floats, every $m$-th value is taken and summed up (i.e. $n/m$ values are summed up).
This is repeated $k$ times, and the $k$ values are put into a histogram.

* What kind of distribution do you expect for those $k$ values? (**Hint:** Assume $n/m \gg 1$)
* Implement the test visually: I.e. compare the histogram to the expected distribution.

```{python}
def lagged_sums(rng, size, num_samples, lag):
    return np.array([sum(rng.uniform(size=(lag + 1) * num_samples)[::(lag + 1)]) for _ in range(size)])
```

```{python}
@interact(lag=(0, 100))
def plot_lagged_sums(lag=0):
    # RANDU:
    bad_rng = LCG(2**31, 65539, 0, seed=1)
    good_rng = np.random.default_rng()

    num_samples = 100

    range_ = (38, 62)
    fig, axs = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(7, 4))
    axs[0].hist(lagged_sums(bad_rng, 10_000, num_samples, lag), range=range_, bins=20, density=True)
    axs[0].set_title("Our RNG")
    axs[1].hist(lagged_sums(good_rng, 10_000, num_samples, lag), range=range_, bins=20, density=True)
    axs[1].set_title("Numpy RNG")

    x =  np.linspace(*range_, 100)
    expected_pdf = norm(0.5 * num_samples, np.sqrt(num_samples / 12)).pdf(x)

    axs[0].plot(x, expected_pdf)
    axs[1].plot(x, expected_pdf)
    fig.tight_layout()
```

## c)

The first of the diehard tests is called **"Birthday spacings"** (named after the [birthday problem](https://en.wikipedia.org/wiki/Birthday_problem)).
The test goes as follows:

- Choose $m$ birthdays uniformly random in a year of $n$ days.
- List the spacings (number of days) between consecutive birthdays.
- Count the number $j$ of values that appear more than once in that list.
- Repeat the procedure $k$ times and record $j$

If $n$ is large, then $j$ is approximately Poisson-distributed, with a mean value of $m^3 / (4 n)$.

Implement the procedure, and compare the histogram of $j$ to the expected Poisson distribution.
How does your implementation of a LCG compare to numpy's default RNG?
How do you interpret the result?

```{python}
from scipy.stats import poisson

def plot_birthday(N, m=2**9):
    period = 2**24
    bad_rng = LCG(period, 65539, 0, seed=1)
    good_rng = np.random.default_rng()

    fig, axs = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(7, 4))

    rate = m**3 / (4 * period)
    range_ = (0, int(10 * rate))
    x =  np.arange(range_[1])

    hist_good = np.zeros(range_[1], np.int64)
    hist_bad = np.zeros(range_[1], np.int64)

    for i in range(N):
        counts_good = m - 1 - len(np.unique(np.diff(np.sort(good_rng.integers(period, size=m)))))
        counts_bad = m - 1 - len(np.unique(np.diff(np.sort(bad_rng.integers(period, size=m)))))
        hist_good[counts_good] += 1
        hist_bad[min(range_[1] - 1, counts_bad)] += 1

    axs[0].bar(x, hist_bad / N)
    axs[1].bar(x, hist_good / N)
    axs[0].set_title("Our RNG")
    axs[1].set_title("Numpy RNG")

    expected_pdf = poisson(rate).pmf(x)

    axs[0].plot(x, expected_pdf, "o", color="orange")
    axs[1].plot(x, expected_pdf, "o", color="orange")
    fig.tight_layout()

plot_birthday(2_000)
```

# Problem 3: Variable transformation method

Consider a random variable $X \in [0, \infty)$ with the following PDF (parameterized by $\mu$ and $\sigma$):
$$
p_X(x) = \frac{1}{x \sigma \sqrt{2 \pi}} \exp\Bigl(-\frac{(ln(x) - \mu)^2}{2 \sigma^2}\Bigr)
$$

Verify numerically (using quadrature), that this is indeed a probability density function (i.e. that it is normalized).

```{python}
def p(x, mu, sigma):
    e = -np.square(np.log(x) - mu) / (2 * np.square(sigma))
    return np.exp(e) / (x * sigma * np.sqrt(2 * np.pi))

from scipy.integrate import quad

quad(lambda x: p(x, 3.0, 1.0), 0, np.inf)
```

Assume you have a RNG that delivers uniformly distributed floating point numbers in the interval $[0, 1)$.
Try to use the inversion method to implement a direct sampler for this distribution.
Is this a good way to sample from that distribution?
Can you find a different (maybe better) way using another variable transformation?

We calculate the cumulative distribution function:
$$
\begin{aligned}
    P(y) &= \int_0^y p(x) dx \\
         &= \int_0^y \frac{1}{x \sigma \sqrt{2 \pi}} \exp\Bigl(-\frac{(ln(x) - \mu)^2}{2 \sigma^2}\Bigr) \\ 
         &= \frac{1}{2}\Bigl[1 + \text{erf}\Bigl(\frac{\ln(y) - \mu}{\sigma \sqrt{2}}\Bigr)\Bigr]
\end{aligned}
$$

and invert it:

$$
P^{-1}(u) = \exp\Bigl(\sigma \sqrt{2} \text{erf}^{-1}(2 u - 1) + \mu\Bigr)
$$

```{python}
from scipy.special import erfinv

def inv_cdf(u, mu, sigma):
    return np.exp(sigma * np.sqrt(2) * erfinv(2 * u - 1) + mu)
```

```{python}
rng = np.random.default_rng()

u = rng.uniform(size=1_000_000)
sigma = 0.5
mu = 5.0
x_via_inversion = inv_cdf(u, mu, sigma)

range_ = (0, 700)
x = np.linspace(*range_, 200)

plt.hist(x_via_inversion, bins=20, range=range_, density=True, label="Samples using inversion")
plt.plot(x, p(x, mu, sigma), label="PDF")
plt.legend()
```

### Variable transformation

Can we find a different distribution than the uniform distribution as a base for our samples?

Remember the variable transformation rule.
We are looking for a random variable $Y$ with probability density function $p_Y(y)$,
 and an invertiable transformation $h(Y)$, such that $h(Y)$ with $Y \sim p_Y$ follows the same distribution as $X$ (which is defined by the PDF above).
$$
p_X(x) = \frac{1}{x \sigma \sqrt{2 \pi}} \exp\Bigl(-\frac{(ln(x) - \mu)^2}{2 \sigma^2}\Bigr) = \frac{p_Y\bigl(h^{-1}(x)\bigr)}{h'\bigl(h^{-1}(x)\bigr)}
$$

What if we try $h^{-1}(x) = (\ln(x) - \mu) / \sigma$? Then at least the exponent looks simpler.

Indeed we have 
$$
\begin{aligned}
&h^{-1}(x) = \frac{\ln(x) - \mu}{\sigma}, \\
\Rightarrow \quad &h(y) = \exp(y \sigma + \mu), \\
&h'(y) = \sigma h(y)
\end{aligned}
$$

$$
p_X(x) = \frac{p_Y\bigl(h^{-1}(x)\bigr)}{\sigma x}
$$

If we compare this with the given formula for $p_X(x)$, we see that $p_Y(y)$ must be

$$
p_Y(y) = \frac{1}{\sqrt{2 \pi}} \exp(-y^2 / 2)
$$

Does this look familiar? (It should!) This is the PDF of a standard Normal distribution.

So we conclude that if we sample $y_i \sim \mathcal{N}(0, 1)$ and transform $x_i = h(y_i) = \exp(y_i \sigma + \mu)$, we obtain samples $x_i$ that follow the given distribution $P_x$.

Indeed the distribution $P_x$ is called the [Log-normal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution), and it is actually defined as the distribution of a random variable **whose logarithm is normally distributed**. Knowing this, it's easy to remember how to sample from the Log-normal distribution.

```{python}
from scipy.stats import lognorm

x_via_exp_transform = np.exp(rng.normal(mu, sigma, size=1_000_000))

# actual distribution:
dist = lognorm(sigma, scale=np.exp(mu))

range_ = dist.ppf([0.001, 0.999])
x = np.linspace(*range_, 200)

fig, axs = plt.subplots(1, 2, sharex=False, sharey=False, figsize=(7, 4))
axs[0].hist(x_via_inversion, bins=20, range=range_, density=True)
axs[0].plot(x, lognorm(sigma, scale=np.exp(mu)).pdf(x))
axs[0].set_title("Inversion method")
axs[1].hist(x_via_exp_transform, bins=20, range=range_, density=True)
axs[1].plot(x, lognorm(sigma, scale=np.exp(mu)).pdf(x))
axs[1].set_title("Transformation method")

fig.tight_layout()
```
