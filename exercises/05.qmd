---
jupyter: python3
title: Exercise 5
---

# Problem 1: Markov Chains (from last week)

Consider the three Markov Chains in figure @fig-markov-chains

::: {#fig-markov-chains layout-ncol=3}

![](images/MC-Graph-1.png){#fig-a width=40%}

![](images/MC-Graph-2.png){#fig-b width=20%}

![](images/MC-Graph-3.png){#fig-c width=20%}

Three Markov Chains, where the edge weights represent transition probabilites between states
:::

## a)

Implement the transition probability matrix $P_i$ for all three graphs.

```{python}
import matplotlib.pyplot as plt
import numpy as np

P1 = np.array(
    [
        [0, 0, 0.5, 0, 0, 0],
        [1, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0],
        [0, 0, 0.5, 0, 0, 0.5],
        [0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 1, 0.5],
    ]
)

P2 = np.array(
    [
        [0, 0, 0.7, 0.2],
        [0, 0, 0.3, 0.8],
        [0.8, 0.4, 0, 0],
        [0.2, 0.6, 0, 0]
    ]
)

P3 = np.array(
    [
        [0, 0.5, 0.5],
        [0.5, 0, 0.5],
        [0.5, 0.5, 0]
    ]
)
```


## b)

For each of the graphs, figure out whether they are periodic or aperiodic. If a graph is periodic, find the periodicity of at least one state that has periodicity > 1.

* Graph @fig-a is periodic. The states 1, 2, and 3 all have periodicity 3, since starting from e.g. state 2 it is only possible to return to state 2 in $3 k, \, k \in \mathbb{N}$ steps.
* Graph @fig-b is periodic. All states have a periodicity of 2.
* Graph @fig-c is aperiodic. Starting from any state, one can return to that state in $k = 2, 3, 4, ...$ steps. The gcd is 1.

## c)

For each of the graphs, figure out whether they are reducible or irreducible

* Graph @fig-a is reducible. E.g. it is never possible to reach state 1 from state 5
* Graph @fig-b is irreducible. Every state is reachable from every other state in a finite number of steps.
* Graph @fig-c is irreducible for the same reason.

## d)

Find the eigenvalues and corresponding eigenvectors for the three martixes. Before doing so, think about what you expect. Does the result align with your expectation?

```{python}
def print_eigen(P):
    eigvals, eigvecs = np.linalg.eig(P)
    largest_eigvec = eigvecs[:, np.abs(eigvals).argmax()]
    largest_eigvec /= largest_eigvec.sum()

    with np.printoptions(precision=2):
        print("Eigenvalues:")
        print(eigvals, "\n")
        print("Absolute value of Eigenvalues:")
        print(np.abs(eigvals), "\n")
        print("Eigenvectors:")
        print(eigvecs, "\n")
        print("Eigenvector with largest eigenvalue:")
        print(largest_eigvec.real)
```

```{python}
print_eigen(P1)
```

```{python}
print_eigen(P2)
```

```{python}
print_eigen(P3)
```

## e)

Calculate the matrix power $P_i^{50}$ and $P_i^{51}$ for the three matrixes. Again, think about what you'd expect before doing so.

```{python}
np.linalg.matrix_power(P1, 50)
```
```{python}
np.linalg.matrix_power(P1, 51)
```

```{python}
np.linalg.matrix_power(P2, 50)
```
```{python}
np.linalg.matrix_power(P2, 51)
```

```{python}
np.linalg.matrix_power(P3, 50)
```
```{python}
np.linalg.matrix_power(P3, 51)
```

# Problem 2: Metropolis-Hastings Markov Chain

Consider again the first example from the first half of the lecture (and our first exercise):

![Probability table](images/ProbabilityTable.png)

```{python}
p = np.array([21, 8, 7, 8, 3, 24, 1, 24]) / 96

v = np.array([[1,1,1], [1,1,0], [1,0,1], [1,0,0], [0,1,1], [0,1,0], [0,0,1], [0,0,0]])

p
```

Again we want to sample from this discrete probablity distribution. This time using Metropolis-Hastings.

## a)

Find an irreducible Markov Chain (i.e. its transition matrix $P$), whose stationary distribution is the above probability table $p$. Furthermore this chain should converge to $p$ from any initial distribution.

```{python}
def mh_transition_matrix(p, Q):
    """
    given an transition matrix Q and a desired stationary distribution (vector) p, return a transition matrix P whose stationary distribution is p by applying the Metropolis-Hastings correction on top of Q.

    Parameters
    ----------
    p:
        1-d array of length M containing state probabilities
    Q:
        MxM proposal transition matrix

    Returns
    -------
    P:
        MxM transition matrix whose stationary distribution is p
    """
    A = np.clip(Q.T / Q * np.divide.outer(p, p), 0, 1)
    r = 1 - np.sum(Q * A, axis=0)
    P = Q * A + np.diag(r)
    return P

M = len(p)
Q = np.ones((M, M)) / M
P = mh_transition_matrix(p, Q)

with np.printoptions(precision=2):
    print(P)
```

## b)

Show that the Markov Chain you found has indeed the desired properties (from **a)**).

```{python}
eigvals, eigvecs = np.linalg.eig(P)
with np.printoptions(precision=2):
    print(eigvals)
    print(eigvecs)
```

We find the eigenvector corresponding to the largest eigenvalue (which should be 1):

```{python}
v_largest = eigvecs[:, np.argmax(np.abs(eigvals))]
# eigenvectors are defined up to their length
# we scale it so its elements sum to 1
v_largest /= np.sum(v_largest)
with np.printoptions(precision=2):
    print(v_largest)
    print(p)
```

We see that the principal eigenvector corresponds to the desired stationary distribution $p$.

## c)

Is the Markov Chain you found unique? In other words, is your transition matrix $P$ the only such matrix with the desired properties (from **a)**)?
Give a proof!

```{python}
# Some random stochastic matrix
Q2 = np.random.rand(M, M)
Q2 = Q2 / np.sum(Q2, axis=0)  # columns need to sum to 1

P2 = mh_transition_matrix(p, Q2)
with np.printoptions(precision=2):
    print(P2)
```

```{python}
with np.printoptions(precision=2):
    print(P)
```

```{python}
eigvals2, eigvecs2 = np.linalg.eig(P2)
with np.printoptions(precision=2):
    print(eigvals2)
    print(eigvecs2)
```

```{python}
# we find the eigenvector that corresponds to the largest eigenvalue:
v_largest2 = eigvecs2[:, np.argmax(np.abs(eigvals2))]
# eigenvectors are defined up to their length
# we scale it so its elements sum to 1
v_largest2 /= np.sum(v_largest2)
with np.printoptions(precision=2):
    print(v_largest2)
    print(p)
```

## d)

For all your Markov Chains, visualize the expected convergence behaviour *without* explicitely calculating it (i.e. don't 'run' the Markov Chain, don't calculate powers of $P$, just give a visualization of the approximate behaviour)

The convergence of the Markov Chain towards the stationary distribution should be dominated by the second largest eigenvalue of $P$
```{python}
second_largest_evalue = sorted(eigvals, key=np.abs, reverse=True)[1]
second_largest_evalue2 = sorted(eigvals2, key=np.abs, reverse=True)[1]

x = np.arange(0, 40)
plt.plot(x, np.power(np.abs(second_largest_evalue), x), label="Uniform proposal")
plt.xlabel("Iteration")
plt.plot(x, np.power(np.abs(second_largest_evalue2), x), label="Random proposal")
```

::: {.callout-note}
## Final remarks on problem 2
Of course the Metropolis-Hastings method this is not a very efficient or even sensible way of sampling from this probability distribution.
We know how to sample from finite discrete distributions directly, as demonstrated early in the lecture and the first exercise sheet.
Take this exercise as a pedagocical example.
:::

# Problem 3: MH with non-symmetric proposal

## a)

Discuss when/why a non-symmetric proposal distribution might make sense.

## b)

Implement the Metropolis-Hastings algorithm.
The method should be flexible enough to work with different target and proposal distributions.
It should return the samples, the proposal ratios $Q(x_{current} \mid x_{proposed}) /  Q(x_{proposed} \mid x_{current})$, as well as the acceptance rate.

```{python}
def mh(p, q_factory, init, n_samples, stepsize=0.5, rng=None):
    """
    Metropolis Hastings method for finite discrete probability distributions

    Parameters
    ----------

    p:
        A scipy.stats compatible (discrete) probability distribution representing
        the target probability distribution.
    q_factory:
        A function that takes the current state and the target probability vector as input
        and returns a scipy.stats compatible (discrete) probability distribution.
        Should also accept a `stepsize` kwarg.
    init:
        The initial state (should be in the domain of `p`)
    n_samples:
        The number of samples to draw
    stepsize:
        A stepsize for the proposal distribution (passed on as kwarg to `q_factory`)
    """
    rng = rng or np.random.default_rng()
    samples = [init]
    accepted = [True]
    proposal_ratios = []
    for i in range(n_samples - 1):
        current = samples[-1]
        q_forward = q_factory(current, p, stepsize=stepsize)
        proposed = q_forward.rvs()
        q_backward = q_factory(proposed, p, stepsize=stepsize)
        r = q_backward.pdf(current) / q_forward.pdf(proposed)
        proposal_ratios.append(r)
        if rng.uniform() < r * p.pdf(proposed) / p.pdf(current):
            accepted.append(True)
            samples.append(proposed)
        else:
            accepted.append(False)
            samples.append(current)
    return np.array(samples), np.array(proposal_ratios), np.mean(accepted)
```

## c)

Now consider the [Beta(4, 1)](https://en.wikipedia.org/wiki/Beta_distribution) distribution.
Sample from this distribution using your implementation of Metropolis-Hastings with **i)** a uniform proposal distribution and **i)** with a proposal distribution that linearly approximates the target distribution at the current state.
Both proposal distributions should only cover a small interval of length `stepsize` < 1.

Verify that both methods yield samples from the desired target distribution.
What is the acceptance rate for both methods (assuming equal `stepsize`)?
Also plot histograms for the logarithm of the proposal ratios $Q(x_{current} \mid x_{proposed}) /  Q(x_{proposed} \mid x_{current})$.

Let's first plot the target distribution:
```{python}
from scipy.stats import beta, triang, uniform

p = beta(4, 1)
x = np.linspace(0, 1, 100)
plt.plot(x, p.pdf(x))
plt.title("Beta(4, 1) distribution")
```

Implementation of the proposal distributions:

```{python}
def uniform_factory(x, p, stepsize):
    """
    Return a uniform distribution on an interval of length `stepsize`
    whose center is shifted by x but only so far as the  whole interval
    is still fully contained in [0, 1].
    """
    lo, hi = x - 0.5 * stepsize, x + 0.5 * stepsize
    if lo < 0:
        hi -= lo
        lo = 0
    elif hi > 1:
        lo -= hi - 1
        hi = 1
    return uniform(loc=lo, scale=hi - lo)

class Trapezoid:
    def __init__(self, a, b, c, t, rng=None):
        self.rng = rng or np.random.default_rng()
        self.a = a
        self.b = b
        self.c = c
        self.t = t
        self.Z = 1.0 / ((b - a) * (c + 0.5 * (b - a) * t))
    
    def pdf(self, x):
        in_domain = np.logical_not((x < self.a) | (x > self.b))
        return in_domain * self.Z * (self.c + self.t * (x - self.a)) 

    def rvs(self):
        # rejection sampling
        while True:
            x = self.rng.uniform(self.a, self.b)
            y = self.rng.uniform(0, self.pdf(self.b))
            if y < self.pdf(x):
                return x

def trapezoid_factory(x, p, stepsize):
    """
    Return a Trapezoid that approximates `p` at `x` around an interval of
    `stepsize`.
    """
    lo, hi = x - 0.5 * stepsize, x + 0.5 * stepsize
    if lo < 0:
        hi -= lo
        lo = 0
    elif hi > 1:
        lo -= hi - 1
        hi = 1
    c = p.pdf(lo)
    t = (p.pdf(hi) - c) / (hi - lo)
    return Trapezoid(lo, hi, c, t)
```

```{python}
p = beta(4, 1)
q_unif = uniform_factory(0.7, p, stepsize=0.2)
q_trapez = trapezoid_factory(0.7, p, stepsize=0.2)

x = np.linspace(0, 1, 500)
plt.plot(x, p.pdf(x) / (p.cdf(0.8) - p.cdf(0.6)), label="Beta(4, 1) (scaled)", linestyle="dashed")
plt.plot(x, q_unif.pdf(x), label="Uniform")
plt.plot(x, q_trapez.pdf(x), label="Trapezoidal")
plt.legend()
```

```{python}
samples1, proposal_ratios1, acceptance1 = mh(p, uniform_factory, 1.0, 10_000)
samples2, proposal_ratios2, acceptance2 = mh(p, trapezoid_factory, 1.0, 10_000)

x = np.linspace(0, 1, 100)
fig, axs = plt.subplots(1, 2, figsize=(9, 3.5))
axs[0].hist(samples1, range=(0, 1), bins=20, density=True)
axs[0].set_title(f"Uniform\nAcceptance: {acceptance1}")
axs[0].plot(x, p.pdf(x))
axs[1].hist(samples2, range=(0, 1), bins=20, density=True)
axs[1].set_title(f"Trapezoidal\nAcceptance: {acceptance2}")
axs[1].plot(x, p.pdf(x))

fig, axs = plt.subplots(1, 2, figsize=(9, 3.5), sharex=True, sharey=True)
axs[0].hist(np.log(proposal_ratios1), range=(-4, 4), bins=21)
axs[1].hist(np.log(proposal_ratios2), range=(-4, 4), bins=21)
```
