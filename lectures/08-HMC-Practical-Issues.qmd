# Lecture 8: Hamiltonian Monte Carlo, Practical Issues

Michael Habeck - University Hospital Jena - michael.habeck@uni-jena.de

## Outline

* Hamiltonian Monte Carlo continued
* Practical Issues (convergence, diagnostic checks)

## Recap

In the last lecture we've looked at the [harmonic oscillator](https://en.wikipedia.org/wiki/Harmonic_oscillator):

```{python}
import numpy as np
import matplotlib.pylab as plt

class Oscillator:

    def __init__(self, k=1., T=1.):

        self.k = float(k)
        self.T = float(T)
        self.A = np.array([[0, 1], [-self.k, 0]])
        self.w = np.sqrt(k)
        
    def propagate(self, x, v, T=None):

        T = self.T if T is None else float(T)
        
        U = np.array([[np.cos(self.w*T), np.sin(self.w*T)/self.w],
                      [-np.sin(self.w*T)*self.w, np.cos(self.w*T)]])

        return U.dot([x, v])

    def calc_hamiltonian(self, x, v):
        return 0.5 * v**2 + 0.5 * self.k * x**2

    def gradient(self, x):
        return self.k * x
    
k = (10., 0.1)[1]
T = 1000.
x0 = 10.

osci = Oscillator(k, T)
```

## Leapfrog integrator

It is only rarely possible to solve Hamilton's equations of motion in closed form. In practice, we often have to resort to numerical integration methods to solve this system of ordinary differential equations. 

The crucial feature for making HMC work properly is the conservation of phase space volume. If phase space volume is not conserved, symmetry of the proposal chain is no longer guaranteed, and we would have to take into account the forward and backward probabilities in the acceptance ratios. These probabilities cannot be computed for general systems. Therefore, we have to look out for [symplectic integrators](https://en.wikipedia.org/wiki/Symplectic_integrator) that solve Hamilton's equations of motion numerically but still conserve phase-space volume. 

[Leapfrog integration](https://en.wikipedia.org/wiki/Leapfrog_integration) is a simple symplectic integration scheme that is often used as an integrator in HMC. The leapfrog integrator solves a finite-difference version of Hamilton's equations of motion:

\begin{align}\label{eq:leapfrog}
\begin{split}
v(t+\epsilon/2) &= v(t) - (\epsilon/2) \nabla_x E(x(t)) \\
x(t+\epsilon) &= x(t) + \epsilon v(t+\epsilon/2) \\
v(t+\epsilon) &= v(t+\epsilon/2) - (\epsilon/2) \nabla_x E(x(t+\epsilon))
\end{split}
\end{align}

where $\epsilon$ is the time step. The initial positions and momenta $x(0)$ and $v(0)$ are the previous state $x^{(s)}$ and a sample from the standard Gaussian $v^{(s)} \sim \mathcal N(0, I)$. 

The leapfrog equations can be rearranged to avoid unnecessary gradient evaluations: 

1. From $x(0), v(0)$ compute $v(\epsilon/2)$:

$$
v(\epsilon/2) = v(0) - (\epsilon/2) \nabla_x E(x(0))
$$

2. Loop over $T-1$ integration steps:

\begin{align}
\begin{split}
x(t+\epsilon) &= x(t) + \epsilon v(t+\epsilon/2) \\
v(t+3\epsilon/2) &= v(t+\epsilon/2) - \epsilon \nabla_x E(x(t+\epsilon)) \\
\end{split}
\end{align}

3. Last full-step in $x$ and half-step in $v$:

\begin{align}
\begin{split}
x(T) &= x(T-\epsilon) + \epsilon v(T-\epsilon/2)\\
v(T) &= v(T-\epsilon/2) - (\epsilon/2) \nabla_x E(x(T)) \\
\end{split}
\end{align}

Resulting in $T+1$ gradient evaluations, whereas the original scheme requires two gradient evaluations per time step.

The following demonstration illustrates leapfrog integration for the harmonic oscillator:

```{python}
def leapfrog(x0, v0, eps, gradient, n_steps):
    """Leapfrog integration
    """
    x, v = x0, v0
    v -= 0.5 * eps * gradient(x)

    for _ in range(n_steps-1):
        x += eps * v
        v -= eps * gradient(x)
        
    x += eps * v
    v -= 0.5 * eps * gradient(x)

    return x, v

# comparison of leapfrog with analytical solution

x0 = 10.
v0 = np.random.standard_normal()

n_steps = 10
eps = 1e-1
T = eps * n_steps

traj = [(x0, v0)]
traj2 = [(x0, v0)]

while len(traj) < 30:
    x0, v0 = traj[-1]
    x, v = leapfrog(x0, v0, eps, osci.gradient, n_steps)
    traj.append((x, v))
    x0, v0 = traj2[-1]
    traj2.append(osci.propagate(x0, v0, T))

traj = np.array(traj)
traj2 = np.array(traj2)

fig, ax = plt.subplots(1, 3, figsize=(12, 4))
ax[0].plot(*traj.T, color='k', alpha=0.5)
ax[0].scatter(*traj.T, c=np.linspace(0., 1., len(traj)))
ax[0].set_xlabel(r'$x(t)$')
ax[0].set_ylabel(r'$v(t)$')
#
ax[1].plot(traj[:,0], color='k', lw=5, alpha=0.5, label='analytical')
ax[1].plot(traj2[:,0], color='r', label='leapfrog')
ax[1].set_ylim(-11, 11)
ax[1].set_xlabel(r'$t$')
ax[1].set_ylabel(r'$x(t)$')
ax[1].legend(fontsize=10)
#
ax[2].plot(traj[:,1], color='k', lw=5, alpha=0.5)
ax[2].plot(traj2[:,1], color='r')
#ax[2].set_ylim(-11, 11)
ax[2].set_xlabel(r'$t$')
ax[2].set_ylabel(r'$v(t)$')
#
fig.tight_layout()                             
```

### Comparison to other integration methods

In his review on HMC, Radford Neal compares the leapfrog integrator with other integration methods such as Euler. The following code reproduces his figure 1

```{python}
"""
Implementation of examples from Neal's HMC review
"""

def integrate(transformation, n_steps, q0, p0):
    """Integrate Hamilton's equations of motion for the one-dimensional 
    harmonic oscillator using a transformation matrix that implements a
    numerical integration method. 
    """
    z = np.array([q0, p0])
    traj = [z]
    for _ in range(n_steps):
        z = transformation @ z
        traj.append(z)
    return np.array(traj)

def euler(eps, n_steps, q0=0., p0=1.):
    # q(t+eps) = q(t) + eps * p(t)
    # p(t+eps) = p(t) - eps * q(t)
    T = np.array([[1, eps],
                  [-eps, 1]])
    return integrate(T, n_steps, q0, p0), T

def modified_euler(eps, n_steps, q0=0., p0=1.):
    # p(t+eps) = p(t) - eps * q(t)
    T1 = np.array([[1, 0.],
                   [-eps, 1]])
    # q(t+eps) = q(t) + eps * p(t+eps)    
    T2 = np.array([[1, eps],
                   [0, 1]])
    T = T2 @ T1
    
    return integrate(T, n_steps, q0, p0), T
    
def leapfrog(eps, n_steps, q0=0., p0=1.):
    # p(t+eps/2) = p(t) - (eps/2) * q(t)
    T1 = np.array([[1, 0],
                   [-eps/2, 1]])
    # q(t+eps) = q(t) + eps * p(t+eps/2)
    T2 = np.array([[1, eps],
                   [0, 1]])
    # p(t+eps) = p(t+eps/2) - (eps/2) * q(t+eps)
    T3 = T1
    T = T3 @ T2 @ T1
    
    return integrate(T, n_steps, q0, p0), T

###############################################################################
## run all three integrators

eps = (0.3, 1.2, 0.1)[0]
n_steps = int(np.floor(2*np.pi/eps))
n_steps = 20

z1, T1 = euler(eps, n_steps)
z2, T2 = modified_euler(eps, n_steps)
z3, T3 = leapfrog(eps, n_steps)

methods = ('Euler', 'modified Euler', 'Leapfrog')
print('Volume preservation?')
for method, trafo in zip(methods, [T1, T2, T3]):
    print('{0:>14}: det(trafo) = {1:.3f}'.format(method, np.linalg.det(trafo)))

###############################################################################
## plot trajectories

# exact dynamics
x = np.sin(eps * np.arange(n_steps+1))
v = np.cos(eps * np.arange(n_steps+1))
    
limits = np.fabs(z1).max() * 1.1
limits = (-limits, limits)
plt.rc('font', size=12)
plt.rc('image', cmap='viridis')
kw = dict(aspect=1.0, xlim=limits, ylim=limits, xlabel=r'$x^{(s)}$')
fig, axes = plt.subplots(1, 3, figsize=(12, 4), sharey='all', sharex='all',
                         subplot_kw=kw)
axes[0].set_ylabel(r'$v^{(s)}$')

for ax, method, z in zip(axes, methods, [z1, z2, z3]):
    ax.set_title(method)
    ax.scatter(*z.T, c=np.linspace(0., 1., n_steps+1))
    ax.plot(*z.T, ls='--', color='k', alpha=0.3)
    ax.plot(x, v, color='k', alpha=0.3, lw=3)
    ax.scatter(x, v, c=np.linspace(0., 1., n_steps+1), marker='*')
fig.tight_layout()
```

## Hamiltonian Monte Carlo
    
Hamiltonian Monte Carlo uses the leapfrog integrator to solve Hamilton's equations of motion. The resulting proposal state is then accepted or rejected according to the Metropolis-Hastings criterion on the augmented $(x, v)$ space (phase space). That is, we accept the new state with probability

$$
\min\left\{1, \exp(-\Delta H)\right\}
$$

which holds since Hamiltonian dynamics and the leapfrog integrator preserve volume (otherwise we would have to take into account the proposal probabilities for the forward and backward dynamics).

### Algorithm: Hamiltonian Monte Carlo

Generate an initial state $x^{(0)} \sim p^{(0)}(x)$ using some initial distribution. For $s=0, 1, 2, \ldots$ cycle over the following iterations

\begin{enumerate}
\item Generate new momenta $v^{(s)} \sim \mathcal N(0, I)$
\item Integrate Hamilton's equations of motion using the leapfrog algorithm resulting in a proposal state $(x', v')$
\item Accept $(x', -v')$ with probability 
$$
\min\left\{1, \exp\bigl(H(x^{(s)}, v^{(s)})-H(x',-v')\bigr) \right\}
$$
as new state $(x^{(s+1)}, -v^{(s+1)})$ 
\end{enumerate}

We have to negate the momenta at the end of the trajectory to make the proposal symmetric. Since Hamiltonian dynamics is reversible, flipping the sign of the momenta guarantees that if we reach $(x', v')$ starting from $(x, v)$ with Hamiltonian dynamics, then we will go back to $(x, v)$ using a dynamics that starts from $(x', -v')$. 

```{python}
def store(args, storage=None):
    if storage is not None:
        storage.append([np.copy(arg) for arg in args])

        
def leapfrog(x, v, gradient, eps, n_steps, traj=None):
    """Leapfrog integration
    """
    store([x, v], traj)
    for _ in range(n_steps):
        v -= (eps/2) * gradient(x)
        x += eps * v
        v -= (eps/2) * gradient(x)
        store([x, v], traj)
    return x, v


class CoupledOscillator:
    """Coupled oscillator with force constants stored in attribute 'K'"""
    def __init__(self, K):
        self.K = np.array(K)
        self.v, self.U = np.linalg.eigh(self.K)
        self.w = np.sqrt(np.clip(self.v, 0., None))

    def propagate(self, x, v, eps):
        """Move system from (x, v) to new state using Hamiltonian dynamics."""
        x, v = self.U.T @ x, self.U.T @ v
        X = np.cos(self.w*eps) * x + np.sin(self.w*eps) / self.w * v
        V = -np.sin(self.w*eps) * self.w * x + np.cos(self.w*eps) * v
        return self.U @ X, self.U @ V
    
    def energy(self, x):
        return 0.5 * np.sum(x * (x @ self.K), -1)
    
    def gradient(self, x):
        return x @ self.K

    def hamiltonian(self, x, v):
        return self.energy(x) + 0.5 * np.linalg.norm(v, axis=-1)**2
        
def covariance_matrix(sigma1=1., sigma2=1., rho=0.):
    """Build two-dimensional covariance matrix. 
    """
    assert -1. <= rho <= 1.

    Sigma = np.diag([sigma1, sigma2]) \
            @ np.array([[1, rho], [rho, 1]]) \
            @ np.diag([sigma1, sigma2])

    return Sigma
```

```{python}
sigma1, sigma2, rho = 1., 1., 0.95
K = np.linalg.inv(covariance_matrix(sigma1, sigma2, rho))
osci = CoupledOscillator(K)

# start position and velocity
x0 = np.array([-1.5, -1.55])
v0 = np.array([-1., 1])
    
n_steps, eps = 250, 0.035
n_steps, eps = 25, 0.25

xvals1 = xvals2 = np.linspace(-1., 1., 101) * 2.5
X1, X2 = np.meshgrid(xvals1, xvals2, indexing='ij')
grid = np.transpose([X1.flatten(), X2.flatten()])
logp = -osci.energy(grid).reshape(len(xvals1), len(xvals2))

# exact dynamics
traj1 = [(x0, v0)]
while len(traj1) < n_steps + 1:
    traj1.append(osci.propagate(*traj1[-1], eps=eps))
x1, v1 = np.array(list(zip(*traj1)))
H1 = osci.hamiltonian(x1, v1)

# leapfrog integration
traj2 = []
leapfrog(x0, v0, osci.gradient, eps, n_steps, traj2)
x2, v2 = np.array(list(zip(*traj2)))
H2 = osci.hamiltonian(x2, v2)

# plot
plt.rc('font', size=16)
limits = (-2.2, 2.2)
kw = dict(xlim=limits, ylim=limits, aspect=1.0)
fig, axes = plt.subplots(2, 2, figsize=(9, 9), sharex='col', 
                         sharey='all', subplot_kw=kw)
axes = list(axes.flat)
for ax, x in zip(axes, [x1, v1, x2, v2]):
    ax.plot(*x.T, color='k', alpha=0.3)
    ax.scatter(*x.T, s=10, c=np.linspace(0., 1., len(x)))
axes[2].set_xlabel(r'$x_1$')
axes[2].set_ylabel(r'$x_2$')
axes[0].set_ylabel(r'$x_2$')
axes[3].set_xlabel(r'$v_1$')
axes[3].set_ylabel(r'$v_2$')
axes[1].set_ylabel(r'$v_2$')
for ax in axes[:2]:
    ax.set_title('exact dynamics')
for ax in axes[-2:]:
    ax.set_title('leapfrog')
axes[0].contour(xvals1, xvals2, np.exp(logp), 3, alpha=0.5)
axes[2].contour(xvals1, xvals2, np.exp(logp), 3, alpha=0.5)
fig.tight_layout()
```

The exact dynamics conserves the Hamiltonian (as it should), whereas the leapfrog dynamics does not exactly preserve the Hamiltonian:

```{python}
fig, ax = plt.subplots()
ax.plot(H1, lw=3, color='r', ls='-', alpha=0.7, label='exact dynamics')
ax.plot(H2, lw=3, color='k', ls='--', alpha=0.7, marker='o', label='leapfrog')
ax.set_xlabel(r'time step $n\epsilon$')
ax.set_ylabel(r'Hamiltonian $H(x, v)$')
ax.set_ylim(2.0, 2.8)
ax.legend(loc=(1.1, 0.5));
```

### Comparison with standard Metropolis-Hastings


```{python}
class Metropolis:

    def __init__(self, model, state, n_samples=1e2, stepsize=1e-1):
        self.model = model
        self.initial_state = np.array(state)
        self.n_samples = int(n_samples)
        self.stepsize = float(stepsize)
        self._reset()
        
    def _reset(self):
        self.counter = 0
        self.state = self.initial_state
        self.n_accepted = 0
        
    def __next__(self):

        if self.counter >= self.n_samples:
            raise StopIteration
        self.counter += 1
        
        # random walk 
        x = self.state.copy()
        X = x + self.stepsize * np.random.standard_normal(x.shape)

        # accept/reject
        diff = self.model.energy(x) - self.model.energy(X)
        accept = np.log(np.random.random()) < diff
        self.n_accepted += int(accept)
        
        if accept: self.state = X

        return self.state

    def __iter__(self):
        self._reset()
        return self

    @property
    def acceptance_rate(self):
        return self.n_accepted / self.n_samples        

    
class HamiltonianMonteCarlo(Metropolis):

    def __init__(self, model, state, n_samples, eps, n_leapfrog):
        super().__init__(model, state, n_samples, eps)
        self.n_leapfrog = int(n_leapfrog)

    def __next__(self):

        if self.counter >= self.n_samples:
            raise StopIteration
        self.counter += 1
        
        # leapfrog integration
        x = self.state.copy()
        v = np.random.standard_normal(x.shape)
        h = self.model.hamiltonian(x, v)
        X, V = leapfrog(
            x, v, self.model.gradient, self.stepsize, self.n_leapfrog)

        # accept/reject
        H = self.model.hamiltonian(X, -V)

        accept = np.log(np.random.random()) < h-H
        self.n_accepted += int(accept)
        
        if accept: self.state = X

        return self.state
```

```{python}
# 2d Gaussian
sigma1 = sigma2 = 1.
rho = 0.98
Sigma = covariance_matrix(sigma1, sigma2, rho)
model = CoupledOscillator(np.linalg.inv(Sigma))

# running HMC and random walk Metropolis
n_samples = 1e3
eps, n_leapfrog = 0.18, 20

initial = np.array([-1.55, -1.5])

hmc = HamiltonianMonteCarlo(model, initial, n_samples, eps, n_leapfrog)
hmc_samples = np.array(list(hmc))

# to be fair, we allow for 'n_leapfrog' more sampling steps in Metropolis 
# sampling
metro = Metropolis(model, initial, n_samples * n_leapfrog, eps)
metro_samples = np.array(list(metro))[::n_leapfrog]

print('acceptance_rate: {0:.2%} (HMC), {1:.2%} (Metropolis)'.format(
    hmc.acceptance_rate, metro.acceptance_rate))

# plotting
burnin = int(0.1*n_samples)
limits = (-3.5, 3.5)
x = np.linspace(*limits, num=101)
grid = np.meshgrid(x, x, indexing='ij')
grid = np.transpose([grid[0].flatten(), grid[1].flatten()])
prob = np.exp(-model.energy(grid)).reshape(len(x), -1)
px = np.exp(-0.5 * x**2 / sigma1**2 - 0.5 * np.log(2*np.pi*sigma1**2))
py = np.exp(-0.5 * x**2 / sigma2**2 - 0.5 * np.log(2*np.pi*sigma2**2))

kw_panel = dict(xlim=limits)
kw_scatter = dict(alpha=0.1, s=10, color='k')
fig, axes = plt.subplots(2, 3, figsize=(9, 6), subplot_kw=kw_panel)

for ax, samples in zip(axes[:,0], [hmc_samples, metro_samples]):
    ax.scatter(*samples.T, **kw_scatter)
    ax.contour(x, x, prob, 5)
    ax.set_ylim(*limits)
for ax, samples in zip(axes[:,1], [hmc_samples, metro_samples]):
    ax.hist(samples[burnin:,0], bins=21, density=True, color='k', alpha=0.2)
    ax.plot(x, px, color='r', lw=3)
for ax, samples in zip(axes[:,2], [hmc_samples, metro_samples]):
    ax.hist(samples[burnin:,1], bins=30, density=True, color='k', alpha=0.2)
    ax.plot(x, py, color='r', lw=3)
fig.tight_layout()
```

```{python}
methods = ('HMC', 'Metropolis')
fig, ax = plt.subplots(figsize=(12, 4))
for samples, method in zip([hmc_samples, metro_samples], methods):
    ax.plot(samples[burnin:,0], label=method, alpha=0.5, 
           color={'HMC': 'k', 'Metropolis': 'r'}[method])
ax.set_xlabel(r'iteration $s$')
ax.set_ylabel(r'first coordinate $x_1^{(s)}$')
ax.legend();
```

Behavior in high-dimensional sample spaces (another example from Radford Neal): 

```{python}
"""HMC in high dimensions: example from Radford Neal's HMC review
"""
class RandomStepsize:
    """Randomized stepsize mixin. 
    """
    def __init__(self, lower, upper):
        self.lower = float(lower)
        self.upper = float(upper)
        self._value = None
        assert self.lower <= self.upper
        
    @property
    def stepsize(self):
        self._value = np.random.uniform(self.lower, self.upper)
        return self._value
    
    @stepsize.setter
    def stepsize(self, value):
        pass

class MetropolisWithRandomStepsize(Metropolis, RandomStepsize):
    def __init__(self, model, initial, n_samples, lower, upper):
        Metropolis.__init__(self, model, initial, n_samples, lower)
        RandomStepsize.__init__(self, lower, upper)
        
class HMCWithRandomStepsize(HamiltonianMonteCarlo, RandomStepsize):
    def __init__(self, model, initial, n_samples, lower, upper, n_leapfrog):
        HamiltonianMonteCarlo.__init__(
            self, model, initial, n_samples, lower, n_leapfrog
            )
        RandomStepsize.__init__(self, lower, upper)
        
ndim = 100
sigma = np.linspace(0.01, 1., ndim)
model = CoupledOscillator(np.diag(1/sigma**2))
initial = np.zeros(ndim)
n_samples = 2000

# hmc
n_leapfrog = 150
hmc = HMCWithRandomStepsize(
        model, initial, n_samples, 0.0104, 0.0156, n_leapfrog)
samples_hmc = np.array(list(hmc))
print(hmc.acceptance_rate)

# metropolis
metro = MetropolisWithRandomStepsize(
        model, initial, n_samples * n_leapfrog, 0.0176, 0.0264)
samples_metro = np.array(list(metro))[::n_leapfrog]
print(metro.acceptance_rate)
```

```{python}
#| scrolled: true
burnin = int(n_samples/10)

# plotting
fig, axes = plt.subplots(2, 2, figsize=(9, 9), sharey='row', sharex='col')
for ax, samples in zip(axes[0,:], [samples_metro, samples_hmc]):
    ax.scatter(samples[burnin:].std(0), samples[burnin:].mean(0))
    ax.axhline(0., ls='--', color='r')
axes[0,0].set_ylim(-0.7, 0.7)
for ax, samples in zip(axes[1,:], [samples_metro, samples_hmc]):
    ax.plot([0., 1.], [0., 1.], ls='--', color='r')
    ax.scatter(samples[burnin:].std(0), sigma)
axes[0,0].set_title('Random walk Metropolis')
axes[0,1].set_title('Hamiltonian Monte Carlo')
axes[0,0].set_ylabel(r'sample mean $\{x_i^{(s)}\}$')
axes[1,0].set_xlabel(r'sample variance $\{x_i^{(s)}\}$')
axes[1,1].set_xlabel(r'sample variance $\{x_i^{(s)}\}$')
axes[1,0].set_ylabel(r'$\sigma_i$')
fig.tight_layout()
```

A practical issue in applications of HMC is that the algorithm requires the gradient of minus log $p(x)$. For some models it is far from straightforward to implement the gradient. Moreover, the gradient evaluations are on top of the evaluations of $\log p(x)$, which are typical for MCMC approaches based on Metropolis-Hastings. Some remedy is provided by the possibility to use [*automatic differentiation*](https://en.wikipedia.org/wiki/Automatic_differentiation) to compute the gradient without implementing it explicitly. This strategy is used, for example, in the [STAN](https://en.wikipedia.org/wiki/Stan_(software)) software for statistical inference or in probabilistic programming packages such as [tensorflow probability](https://www.tensorflow.org/probability), [PyMC 3](https://docs.pymc.io/), or [pyro](https://pyro.ai/). 

Another issue of practical importance is the question how to choose the algorithmic parameters, i.e. the number of integration steps $T$ and the step size $\epsilon$. Some attempts to choose these parameters automatically has been proposed and implemented in the [NUTS](https://arxiv.org/abs/1111.4246) algorithm.  

A special case of HMC is [Langevin dynamics](https://en.wikipedia.org/wiki/Stochastic_gradient_Langevin_dynamics) which omits the acceptance/rejection step altogether and has gained some popularity in Bayesian deep learning when combined with stochastic gradient methods. 

## Practical Issues

![Challenges in MCMC](images/Murray_Thesis_Fig2-1.png "Challenges")

Figure from [Iain Murray: Advances in Markov chain Monte Carlo methods](http://homepages.inf.ed.ac.uk/imurray2/pub/07thesis/murray_thesis_2007.pdf)

### Challenges

* __Local exploration__: MCMC samplers typically employ a proposal kernel that changes the current state only locally. The magnitude of changes in the variables is controlled by the *step size* or a similar algorithmic parameter. The step size is limited by the need to maintain a reasonable acceptance rate. The time it takes for a diffusive random walk to explore a distance scales with
$$
(\text{distance} / \text{step size})^2  
$$

* __Convergence__: Typically, the chain starts from a highly improbable state, far away from any mode (local peak in the probability density function). To find a nearby mode, takes some time, again scaling unfavorably with dimension. But even if a mode has been found, it is not guaranteed that the Markov chain will find other modes in a reasonable amount of simulation time. These other modes could be more important in the sense that they carry more probability mass; so missing out on these modes can result in highly biased approximations. 

* __Mixing__: To find all relevant modes, is one of the greatest challenges when sampling high-dimensional probabilistic models with multiple peaks (which is the rule rather than the exception). There are many reasons for having to deal with multi-modal distributions. A common reason are symmetries such as invariance under permutation of labels resulting in the [label-switching problem](https://link.springer.com/chapter/10.1007/978-3-662-01131-7_26) in Gaussian mixture modeling. None of the methods that we discussed so far are particularly suited to explore multi-modal probability distributions. A common approach is to use *tempering* to flatten the probability such that the Markov chain can explore sample space more freely, and simulate a chain of tempered distributions, either sequentially (e.g. in [*Annealed importance sampling* (AIS)](https://link.springer.com/article/10.1023/A:1008923215028)) or in parallel (e.g. in [*Parallel tempering*](https://en.wikipedia.org/wiki/Parallel_tempering)). Unfortunately, there is not enough time to discuss these important methods. 

* __Balancing density and volume__: Another important issue is to not only find all relevant modes, but also visit them in due proportion. A probability peak might be very pronounced, but only carry a small amount of probability mass. If the Markov chain is stuck in this mode, samples coming from that mode will be overrepresented. 

Let's illustrate some of these problems for a discrete toy system.

```{python}
from scipy.special import logsumexp
"""
Discretized bimodal target used to study convergence. 
"""

SEED = [
        # produces bimodal distribution biased towards first mode
        41,
        # produces bimodal distribution biased towards second mode
        1234,
        # produces unimodal sample distribution
        43,   
        ][1]

class BimodalMixture:

    def __init__(self,
                 centers=np.array([0., 1.]),
                 widths=np.array([1., 1.]),
                 weight=0.5):
        self.centers = np.array(centers)
        self.widths = np.array(widths)
        self.weight = float(weight)

    def log_prob(self, x):
        dist = np.subtract.outer(x, self.centers)
        logp = -0.5 * dist**2 / self.widths**2 \
          - 0.5 * np.log(2*np.pi*self.widths**2) \
          + np.log(self.weight) + np.log(1-self.weight)
        return logsumexp(logp, axis=1)

def make_proposal(n_neighbors, n_states):
    Q = np.sum([np.eye(n_states, k=k) for k 
                in range(-n_neighbors, n_neighbors+1)], 0)
    return Q / Q.sum(1)

# setting bimodal toy system
centers = np.array([-1., 1.]) * 4
widths = np.array([0.2, 2.])
weight = 0.5

prob = BimodalMixture(centers, widths, weight)

n = 100            # number of states
X = np.arange(n)   # sample space
p = np.exp(prob.log_prob(np.linspace(-7., 10., n)))
p /= p.sum()       # discretized probability density

# use local proposal chain
stepsize = 5
n_samples = 1e4

# random walk with uniform proposal and reflective boundary
Q = make_proposal(stepsize, n) 

# run Metropolis-Hastings
np.random.seed(SEED) 

n_accepted = 0
x = X[-1]
samples = [x]
while len(samples) < n_samples:
    y = np.random.choice(X, p=Q[:,x])
    r = Q[x,y] * p[y] / (Q[y, x] * p[x])
    if r > np.random.random():
        x = y
        n_accepted += 1
    samples.append(x)
samples = np.array(samples)
logp = np.log(p[samples])

print('acceptance rate: {0:.1%}'.format(n_accepted/n_samples))
print('{0:.1%} of all samples are in left mode'.format(
    np.mean(samples<25)))

# plot results
plt.rc('font', size=14)
fig, ax = plt.subplots(figsize=(10, 5))
ax.plot(X, p, color='r', lw=2, alpha=0.7)
ax.hist(samples, bins=40, density=True, color='k', alpha=0.2);
```

```{python}
#| scrolled: true
fig, ax = plt.subplots(figsize=(10, 5))
ax.axhline(np.log(p).max(), color='r', lw=3, ls='--', alpha=0.7,
              label=r'$\max\{\log{p}\}$')
ax.plot(logp, color='k', lw=3, alpha=0.7, label=r'$\log p(x^{(s)}))$')
ax.legend()
fig.tight_layout()
```

### Convergence

#### Convergence rates for Markov chains

The speed of convergence of a Markov chain $P$ with stationary distribution $\pi$ depends on how quickly contributions to the distance

\begin{equation}\label{eq:distance}
\left|p^{(S)} - \pi\right|
\end{equation}

die out as $S\to\infty$. Distance (\ref{eq:distance}) is dominated by the second largest eigenvalue $\lambda_2$ of $P$. Since the Markov chain is assumed to be irreducible and aperiodic, we have strictly $|\lambda_2| < 1$. If $u_2, u_3, \ldots$ are the eigenvectors of $P$ with eigenvalues $1 > |\lambda_2| \ge |\lambda_3| >\ldots$, then we can write the initial distribution as
$$
p^{(0)} = \pi + a_2 u_2 + a_3 u_3 + \ldots
$$
After $S$ transitions, the initial $p^{(0)}$ will be propagated to 
$$
p^{(S)} = \pi + a_2 \lambda_2^S u_2 + a_3 \lambda_3^S u_3 + \ldots
$$
and therefore
$$
\left|p^{(S)} - \pi\right| \sim |\lambda_2|^S
$$

The convergence rate depends on the step size, which calls for ways to *tune* algorithmic parameters of MCMC methods:

```{python}
def metropolis_map(Q, p):
    """
    Construct Metropolis kernel from proposal kernel and target distribution
    """
    # M = np.clip(Q, 0., (Q * p).T / p)
    M = np.min([Q, (Q * p).T / p], axis=0)
    i = np.arange(len(M))
    M[i,i] = 0.
    M[i,i] = 1 - M.sum(0)

    return M

def second_eigval(M):
    return np.sort(np.abs(np.linalg.eigvals(M)))[::-1][1]    

def propagate(M, p0, n):
    P = [p0.copy()]
    for _ in range(n):
        P.append(M @ P[-1])
    return np.array(P)
```

```{python}
stepsizes = (5, 10, 15, 20)
distances = []
rates = []

p0 = np.eye(n)[-1]
for stepsize in stepsizes:
    Q = make_proposal(stepsize, n)
    M = metropolis_map(Q, p)
    P = propagate(M, p0, 10000)
    d = np.fabs(P - p).sum(1)
    distances.append(d)
    rates.append(-np.log(second_eigval(M)))
```

```{python}
colors = plt.rcParams['axes.prop_cycle'].by_key()['color']
fig, ax = plt.subplots(figsize=(10,5))
for i in range(len(stepsizes)):
    ax.plot(distances[i], alpha=0.3, lw=5, 
            label='step size={0}'.format(stepsizes[i]))
t = np.arange(10000)
for i in range(len(stepsizes)):
    ax.plot(np.exp(-rates[i]*t), ls='--', alpha=0.9, 
            color=colors[i], label=r'$\exp\{-\lambda_2 s\}$')
ax.set_xlabel(r'iteration $s$')
ax.set_ylabel(r'$|\pi - p^{(s)}|$')
ax.legend()
ax.set_ylim(0., 1.);
    
```

```{python}
stepsizes = range(1, 31)
rates = []

for stepsize in stepsizes:
    Q = make_proposal(stepsize, n)
    M = metropolis_map(Q, p)
    rates.append(-np.log(second_eigval(M)))
    
fig, ax = plt.subplots(figsize=(5, 5))
ax.scatter(stepsizes, rates, s=200, color='k', alpha=0.7)
ax.set_ylabel(r'-$\log|\lambda_2|$')
ax.set_xlabel('stepsize');
```

### Burn-in bias

The MCMC chain does not start from the stationary distribution, so 
$$
\mathbb E_{p^{(s)}}[f] \not= \mathbb E_p[f]\, , 
$$
and the difference can be substantial for small $s$, thereby inducing significant bias to the Monte Carlo estimator:

\begin{equation}
\frac{1}{S} \sum_{s=1}^S f\bigl(x^{(s)}\bigr).
\end{equation}

It is difficult to assess the reliability of MCMC approximations, because of the dependence of the samples $x^{(s)}$. The dependence usually adds variance to the estimator, when compared against simple Monte Carlo averages.

To minimize biases stemming from a poor choice of the initial distribution $p^{(0)}$, it is common practice to discard the first samples $x^{(0)}, \ldots, x^{(B)}$ called *burn-in*. It is assumed that $x^{(B+1)}$ will approximately follow the target distribution $p$. The Monte Carlo approximation then becomes:
\begin{equation}\label{eq:burnin}
\frac{1}{S-B} \sum_{s=B+1}^S f\bigl(x^{(s)}\bigr)\, .
\end{equation}
Several statistics can be used to detect bias in MCMC simulations. However, they usually rely on rather strong assumptions, such as the asymptotic normality, or at least uni-modality of the target.

### Auto-correlation diagnostic

The asymptotic variance of the MCMC estimator can be shown to converge against
\begin{equation}\label{eq:variance}
\text{var}\left[\frac{1}{S}\sum_s f\bigl(x^{(s)}\bigr) \right] \xrightarrow[S\to\infty]{} \frac{1}{S} \text{var}_p[f] (1 + 2 \sum_{s\ge 1} \rho_s)  
\end{equation}
where $\rho_s$ are the *correlations* between the initial samples and the $s$-th samples
\begin{equation}\label{eq:correlation}
\rho_s = \text{corr}[f(x^{(0)}), f(x^{(s)})] \, .
\end{equation}
For uncorrelated samples $\rho_s=0$ and we are back to the standard variance of Monte Carlo estimators: $\text{var}[f]/S$. However, due to correlations the variance can be increased significantly (in principle, the variance can approach infinity for perfectly correlated samples). 

Another way to look at this is that correlations decrease the *effective sample size* (ESS), which becomes
\begin{equation}\label{eq:ess}
S_{\text{eff}} = \frac{S}{1 + 2 \sum_{s\ge 1} \rho_s} = \frac{S}{\text{IACT}}
\end{equation}
where $\text{IACT} = 1 + 2 \sum_{s\ge 1} \rho_s$ is the *integrated auto-correlation time*. 

```{python}
# autocorrelation analysis of bimodal target

def autocorrelation(x, n):
    """
    auto-correlation of a times series

    Parameters
    ----------

    x: array containing time series
    n: Integer specifying maximal lag for which to compute the auto-correlation
    """
    x = x - x.mean()
    return np.array([np.mean(x[i:] * x[:len(x) - i]) for i in range(n)]) / np.std(x)**2

def run_metropolis(Q, p, X, n_samples=1e4):
    samples = [X[-1]]
    x = samples[0]
    n_acc = 0
    while len(samples) < n_samples:
        y = np.random.choice(X, p=Q[:,x])
        r = Q[x,y] * p[y] / (Q[y, x] * p[x])
        if r > np.random.random():
            x = y
            n_acc += 1
        samples.append(x)
    return np.array(samples), n_acc

np.random.seed(41) 
stepsizes = (5, 10, 15, 20)
ac = []
for stepsize in stepsizes:
    Q = make_proposal(stepsize, len(p))
    S, n_acc = run_metropolis(Q, p, X, 2e4)
    ac.append(autocorrelation(S*1., 10000))
    print('stepsize={0}: acceptance-rate={1:.1%}'.format(
        stepsize, n_acc/len(S)))
```

```{python}
fig, ax = plt.subplots(figsize=(10,5))
for i in range(len(stepsizes)):
    ax.plot(ac[i], alpha=0.3, lw=5, 
            label='stepsize={0}'.format(stepsizes[i]))
ax.axhline(0, ls='--', color='k', alpha=0.7)
ax.set_xlim(0, 4000)
ax.set_xlabel(r'iteration $s$')
ax.set_ylabel(r'autocorrelation')
ax.legend();
```

### Practical summary (from Vihola's lecture notes)

When using MCMC, always do the following checks:

1. Plot MCMC traces of the variables and key functions of the variables. They should look stationary after burn-in.

2. Make multiple MCMC runs from different initial states and check that the marginal distributions (or the estimators) look similar. This test reveals if your chain is "almost reducible".

3. Plot sample autocorrelations of the variables and functions.

4. Calculate the effective sample size and check that it is reasonably large. 
