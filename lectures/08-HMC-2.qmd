# Lecture 8: Hamiltonian Monte Carlo continued

## Outline

* Hamiltonian Monte Carlo continued
* Practical Issues (convergence, diagnostic checks)

## Recap

In the last lecture we've looked at the [harmonic oscillator](https://en.wikipedia.org/wiki/Harmonic_oscillator):

```{python}
import numpy as np
import matplotlib.pylab as plt

class Oscillator:

    def __init__(self, k=1., T=1.):

        self.k = float(k)
        self.T = float(T)
        self.A = np.array([[0, 1], [-self.k, 0]])
        self.w = np.sqrt(k)
        
    def propagate(self, x, v, T=None):

        T = self.T if T is None else float(T)
        
        U = np.array([[np.cos(self.w*T), np.sin(self.w*T)/self.w],
                      [-np.sin(self.w*T)*self.w, np.cos(self.w*T)]])

        return U.dot([x, v])

    def calc_hamiltonian(self, x, v):
        return 0.5 * v**2 + 0.5 * self.k * x**2

    def gradient(self, x):
        return self.k * x
    
k = (10., 0.1)[1]
T = 1000.
x0 = 10.

osci = Oscillator(k, T)
```

## Leapfrog integrator

It is only rarely possible to solve Hamilton's equations of motion in closed form. In practice, we often have to resort to numerical integration methods to solve this system of ordinary differential equations. 

The crucial feature for making HMC work properly is the conservation of phase space volume. If phase space volume is not conserved, symmetry of the proposal chain is no longer guaranteed, and we would have to take into account the forward and backward probabilities in the acceptance ratios. These probabilities cannot be computed for general systems. Therefore, we have to look out for [symplectic integrators](https://en.wikipedia.org/wiki/Symplectic_integrator) that solve Hamilton's equations of motion numerically but still conserve phase-space volume. 

[Leapfrog integration](https://en.wikipedia.org/wiki/Leapfrog_integration) is a simple symplectic integration scheme that is often used as an integrator in HMC. The leapfrog integrator solves a finite-difference version of Hamilton's equations of motion:

\begin{align}\label{eq-leapfrog}
\begin{split}
v(t+\epsilon/2) &= v(t) - (\epsilon/2) \nabla_x E(x(t)) \\
x(t+\epsilon) &= x(t) + \epsilon v(t+\epsilon/2) \\
v(t+\epsilon) &= v(t+\epsilon/2) - (\epsilon/2) \nabla_x E(x(t+\epsilon))
\end{split}
\end{align}

where $\epsilon$ is the time step. The initial positions and momenta $x(0)$ and $v(0)$ are the previous state $x^{(s)}$ and a sample from the standard Gaussian $v^{(s)} \sim \mathcal N(0, I)$. 

The leapfrog equations can be rearranged to avoid unnecessary gradient evaluations: 

1. From $x(0), v(0)$ compute $v(\epsilon/2)$:

	$$
	v(\epsilon/2) = v(0) - (\epsilon/2) \nabla_x E(x(0))
	$$

2. Loop over $T-1$ integration steps:

	$$
	\begin{aligned}
	x(t+\epsilon) &= x(t) + \epsilon v(t+\epsilon/2) \\
	v(t+3\epsilon/2) &= v(t+\epsilon/2) - \epsilon \nabla_x E(x(t+\epsilon)) \\
	\end{aligned}
	$$
	
3. Last full-step in $x$ and half-step in $v$:

    $$
	\begin{aligned}
	x(T) &= x(T-\epsilon) + \epsilon v(T-\epsilon/2)\\
	v(T) &= v(T-\epsilon/2) - (\epsilon/2) \nabla_x E(x(T)) \\
	\end{aligned}
	$$
	
	Resulting in $T+1$ gradient evaluations, whereas the original scheme requires two gradient evaluations per time step.

The following demonstration illustrates leapfrog integration for the harmonic oscillator:

```{python}
def leapfrog(x0, v0, eps, gradient, n_steps):
    """Leapfrog integration
    """
    x, v = x0, v0
    v -= 0.5 * eps * gradient(x)

    for _ in range(n_steps-1):
        x += eps * v
        v -= eps * gradient(x)
        
    x += eps * v
    v -= 0.5 * eps * gradient(x)

    return x, v

# comparison of leapfrog with analytical solution

x0 = 10.
v0 = np.random.standard_normal()

n_steps = 10
eps = 1e-1
T = eps * n_steps

traj = [(x0, v0)]
traj2 = [(x0, v0)]

while len(traj) < 30:
    x0, v0 = traj[-1]
    x, v = leapfrog(x0, v0, eps, osci.gradient, n_steps)
    traj.append((x, v))
    x0, v0 = traj2[-1]
    traj2.append(osci.propagate(x0, v0, T))

traj = np.array(traj)
traj2 = np.array(traj2)

fig, ax = plt.subplots(1, 3, figsize=(12, 4))
ax[0].plot(*traj.T, color='k', alpha=0.5)
ax[0].scatter(*traj.T, c=np.linspace(0., 1., len(traj)))
ax[0].set_xlabel(r'$x(t)$')
ax[0].set_ylabel(r'$v(t)$')
#
ax[1].plot(traj[:,0], color='k', lw=5, alpha=0.5, label='analytical')
ax[1].plot(traj2[:,0], color='r', label='leapfrog')
ax[1].set_ylim(-11, 11)
ax[1].set_xlabel(r'$t$')
ax[1].set_ylabel(r'$x(t)$')
ax[1].legend(fontsize=10)
#
ax[2].plot(traj[:,1], color='k', lw=5, alpha=0.5)
ax[2].plot(traj2[:,1], color='r')
#ax[2].set_ylim(-11, 11)
ax[2].set_xlabel(r'$t$')
ax[2].set_ylabel(r'$v(t)$')
#
fig.tight_layout()                             
```

### Comparison to other integration methods

In his review on HMC, Radford Neal compares the leapfrog integrator with other integration methods such as Euler. The following code reproduces his figure 1

```{python}
"""
Implementation of examples from Neal's HMC review
"""

def integrate(transformation, n_steps, q0, p0):
    """Integrate Hamilton's equations of motion for the one-dimensional 
    harmonic oscillator using a transformation matrix that implements a
    numerical integration method. 
    """
    z = np.array([q0, p0])
    traj = [z]
    for _ in range(n_steps):
        z = transformation @ z
        traj.append(z)
    return np.array(traj)

def euler(eps, n_steps, q0=0., p0=1.):
    # q(t+eps) = q(t) + eps * p(t)
    # p(t+eps) = p(t) - eps * q(t)
    T = np.array([[1, eps],
                  [-eps, 1]])
    return integrate(T, n_steps, q0, p0), T

def modified_euler(eps, n_steps, q0=0., p0=1.):
    # p(t+eps) = p(t) - eps * q(t)
    T1 = np.array([[1, 0.],
                   [-eps, 1]])
    # q(t+eps) = q(t) + eps * p(t+eps)    
    T2 = np.array([[1, eps],
                   [0, 1]])
    T = T2 @ T1
    
    return integrate(T, n_steps, q0, p0), T
    
def leapfrog(eps, n_steps, q0=0., p0=1.):
    # p(t+eps/2) = p(t) - (eps/2) * q(t)
    T1 = np.array([[1, 0],
                   [-eps/2, 1]])
    # q(t+eps) = q(t) + eps * p(t+eps/2)
    T2 = np.array([[1, eps],
                   [0, 1]])
    # p(t+eps) = p(t+eps/2) - (eps/2) * q(t+eps)
    T3 = T1
    T = T3 @ T2 @ T1
    
    return integrate(T, n_steps, q0, p0), T

###############################################################################
## run all three integrators

eps = (0.3, 1.2, 0.1)[0]
n_steps = int(np.floor(2*np.pi/eps))
n_steps = 20

z1, T1 = euler(eps, n_steps)
z2, T2 = modified_euler(eps, n_steps)
z3, T3 = leapfrog(eps, n_steps)

methods = ('Euler', 'modified Euler', 'Leapfrog')
print('Volume preservation?')
for method, trafo in zip(methods, [T1, T2, T3]):
    print('{0:>14}: det(trafo) = {1:.3f}'.format(method, np.linalg.det(trafo)))

###############################################################################
## plot trajectories

# exact dynamics
x = np.sin(eps * np.arange(n_steps+1))
v = np.cos(eps * np.arange(n_steps+1))
    
limits = np.fabs(z1).max() * 1.1
limits = (-limits, limits)
plt.rc('font', size=12)
plt.rc('image', cmap='viridis')
kw = dict(aspect=1.0, xlim=limits, ylim=limits, xlabel=r'$x^{(s)}$')
fig, axes = plt.subplots(1, 3, figsize=(12, 4), sharey='all', sharex='all',
                         subplot_kw=kw)
axes[0].set_ylabel(r'$v^{(s)}$')

for ax, method, z in zip(axes, methods, [z1, z2, z3]):
    ax.set_title(method)
    ax.scatter(*z.T, c=np.linspace(0., 1., n_steps+1))
    ax.plot(*z.T, ls='--', color='k', alpha=0.3)
    ax.plot(x, v, color='k', alpha=0.3, lw=3)
    ax.scatter(x, v, c=np.linspace(0., 1., n_steps+1), marker='*')
fig.tight_layout()
```

## Hamiltonian Monte Carlo
    
Hamiltonian Monte Carlo uses the leapfrog integrator to solve Hamilton's equations of motion. The resulting proposal state is then accepted or rejected according to the Metropolis-Hastings criterion on the augmented $(x, v)$ space (phase space). That is, we accept the new state with probability

$$
\min\left\{1, \exp(-\Delta H)\right\}
$$

which holds since Hamiltonian dynamics and the leapfrog integrator preserve volume (otherwise we would have to take into account the proposal probabilities for the forward and backward dynamics).

### Algorithm: Hamiltonian Monte Carlo

Generate an initial state $x^{(0)} \sim p^{(0)}(x)$ using some initial distribution. For $s=0, 1, 2, \ldots$ cycle over the following iterations:

1. Generate new momenta $v^{(s)} \sim \mathcal N(0, I)$

2. Integrate Hamilton's equations of motion using the leapfrog algorithm resulting in a proposal state $(x', v')$

3. Accept $(x', -v')$ with probability 

   $$
   \min\left\{1, \exp\bigl(H(x^{(s)}, v^{(s)})-H(x',-v')\bigr) \right\}
   $$
   
   as new state $(x^{(s+1)}, -v^{(s+1)})$ 

We have to negate the momenta at the end of the trajectory to make the proposal symmetric. Since Hamiltonian dynamics is reversible, flipping the sign of the momenta guarantees that if we reach $(x', v')$ starting from $(x, v)$ with Hamiltonian dynamics, then we will go back to $(x, v)$ using a dynamics that starts from $(x', -v')$. 

```{python}
def store(args, storage=None):
    if storage is not None:
        storage.append([np.copy(arg) for arg in args])

        
def leapfrog(x, v, gradient, eps, n_steps, traj=None):
    """Leapfrog integration
    """
    store([x, v], traj)
    for _ in range(n_steps):
        v -= (eps/2) * gradient(x)
        x += eps * v
        v -= (eps/2) * gradient(x)
        store([x, v], traj)
    return x, v


class CoupledOscillator:
    """Coupled oscillator with force constants stored in attribute 'K'"""
    def __init__(self, K):
        self.K = np.array(K)
        self.v, self.U = np.linalg.eigh(self.K)
        self.w = np.sqrt(np.clip(self.v, 0., None))

    def propagate(self, x, v, eps):
        """Move system from (x, v) to new state using Hamiltonian dynamics."""
        x, v = self.U.T @ x, self.U.T @ v
        X = np.cos(self.w*eps) * x + np.sin(self.w*eps) / self.w * v
        V = -np.sin(self.w*eps) * self.w * x + np.cos(self.w*eps) * v
        return self.U @ X, self.U @ V
    
    def energy(self, x):
        return 0.5 * np.sum(x * (x @ self.K), -1)
    
    def gradient(self, x):
        return x @ self.K

    def hamiltonian(self, x, v):
        return self.energy(x) + 0.5 * np.linalg.norm(v, axis=-1)**2
        
def covariance_matrix(sigma1=1., sigma2=1., rho=0.):
    """Build two-dimensional covariance matrix. 
    """
    assert -1. <= rho <= 1.

    Sigma = np.diag([sigma1, sigma2]) \
            @ np.array([[1, rho], [rho, 1]]) \
            @ np.diag([sigma1, sigma2])

    return Sigma
```

```{python}
sigma1, sigma2, rho = 1., 1., 0.95
K = np.linalg.inv(covariance_matrix(sigma1, sigma2, rho))
osci = CoupledOscillator(K)

# start position and velocity
x0 = np.array([-1.5, -1.55])
v0 = np.array([-1., 1])
    
n_steps, eps = 250, 0.035
n_steps, eps = 25, 0.25

xvals1 = xvals2 = np.linspace(-1., 1., 101) * 2.5
X1, X2 = np.meshgrid(xvals1, xvals2, indexing='ij')
grid = np.transpose([X1.flatten(), X2.flatten()])
logp = -osci.energy(grid).reshape(len(xvals1), len(xvals2))

# exact dynamics
traj1 = [(x0, v0)]
while len(traj1) < n_steps + 1:
    traj1.append(osci.propagate(*traj1[-1], eps=eps))
x1, v1 = np.array(list(zip(*traj1)))
H1 = osci.hamiltonian(x1, v1)

# leapfrog integration
traj2 = []
leapfrog(x0, v0, osci.gradient, eps, n_steps, traj2)
x2, v2 = np.array(list(zip(*traj2)))
H2 = osci.hamiltonian(x2, v2)

# plot
plt.rc('font', size=16)
limits = (-2.2, 2.2)
kw = dict(xlim=limits, ylim=limits, aspect=1.0)
fig, axes = plt.subplots(2, 2, figsize=(9, 9), sharex='col', 
                         sharey='all', subplot_kw=kw)
axes = list(axes.flat)
for ax, x in zip(axes, [x1, v1, x2, v2]):
    ax.plot(*x.T, color='k', alpha=0.3)
    ax.scatter(*x.T, s=10, c=np.linspace(0., 1., len(x)))
axes[2].set_xlabel(r'$x_1$')
axes[2].set_ylabel(r'$x_2$')
axes[0].set_ylabel(r'$x_2$')
axes[3].set_xlabel(r'$v_1$')
axes[3].set_ylabel(r'$v_2$')
axes[1].set_ylabel(r'$v_2$')
for ax in axes[:2]:
    ax.set_title('exact dynamics')
for ax in axes[-2:]:
    ax.set_title('leapfrog')
axes[0].contour(xvals1, xvals2, np.exp(logp), 3, alpha=0.5)
axes[2].contour(xvals1, xvals2, np.exp(logp), 3, alpha=0.5)
fig.tight_layout()
```

The exact dynamics conserves the Hamiltonian (as it should), whereas the leapfrog dynamics does not exactly preserve the Hamiltonian:

```{python}
fig, ax = plt.subplots()
ax.plot(H1, lw=3, color='r', ls='-', alpha=0.7, label='exact dynamics')
ax.plot(H2, lw=3, color='k', ls='--', alpha=0.7, marker='o', label='leapfrog')
ax.set_xlabel(r'time step $n\epsilon$')
ax.set_ylabel(r'Hamiltonian $H(x, v)$')
ax.set_ylim(2.0, 2.8)
ax.legend(loc=(1.1, 0.5));
```

### Comparison with standard Metropolis-Hastings


```{python}
class Metropolis:

    def __init__(self, model, state, n_samples=1e2, stepsize=1e-1):
        self.model = model
        self.initial_state = np.array(state)
        self.n_samples = int(n_samples)
        self.stepsize = float(stepsize)
        self._reset()
        
    def _reset(self):
        self.counter = 0
        self.state = self.initial_state
        self.n_accepted = 0
        
    def __next__(self):

        if self.counter >= self.n_samples:
            raise StopIteration
        self.counter += 1
        
        # random walk 
        x = self.state.copy()
        X = x + self.stepsize * np.random.standard_normal(x.shape)

        # accept/reject
        diff = self.model.energy(x) - self.model.energy(X)
        accept = np.log(np.random.random()) < diff
        self.n_accepted += int(accept)
        
        if accept: self.state = X

        return self.state

    def __iter__(self):
        self._reset()
        return self

    @property
    def acceptance_rate(self):
        return self.n_accepted / self.n_samples        

    
class HamiltonianMonteCarlo(Metropolis):

    def __init__(self, model, state, n_samples, eps, n_leapfrog):
        super().__init__(model, state, n_samples, eps)
        self.n_leapfrog = int(n_leapfrog)

    def __next__(self):

        if self.counter >= self.n_samples:
            raise StopIteration
        self.counter += 1
        
        # leapfrog integration
        x = self.state.copy()
        v = np.random.standard_normal(x.shape)
        h = self.model.hamiltonian(x, v)
        X, V = leapfrog(
            x, v, self.model.gradient, self.stepsize, self.n_leapfrog)

        # accept/reject
        H = self.model.hamiltonian(X, -V)

        accept = np.log(np.random.random()) < h-H
        self.n_accepted += int(accept)
        
        if accept: self.state = X

        return self.state
```

```{python}
# 2d Gaussian
sigma1 = sigma2 = 1.
rho = 0.98
Sigma = covariance_matrix(sigma1, sigma2, rho)
model = CoupledOscillator(np.linalg.inv(Sigma))

# running HMC and random walk Metropolis
n_samples = 1e3
eps, n_leapfrog = 0.18, 20

initial = np.array([-1.55, -1.5])

hmc = HamiltonianMonteCarlo(model, initial, n_samples, eps, n_leapfrog)
hmc_samples = np.array(list(hmc))

# to be fair, we allow for 'n_leapfrog' more sampling steps in Metropolis 
# sampling
metro = Metropolis(model, initial, n_samples * n_leapfrog, eps)
metro_samples = np.array(list(metro))[::n_leapfrog]

print('acceptance_rate: {0:.2%} (HMC), {1:.2%} (Metropolis)'.format(
    hmc.acceptance_rate, metro.acceptance_rate))

# plotting
burnin = int(0.1*n_samples)
limits = (-3.5, 3.5)
x = np.linspace(*limits, num=101)
grid = np.meshgrid(x, x, indexing='ij')
grid = np.transpose([grid[0].flatten(), grid[1].flatten()])
prob = np.exp(-model.energy(grid)).reshape(len(x), -1)
px = np.exp(-0.5 * x**2 / sigma1**2 - 0.5 * np.log(2*np.pi*sigma1**2))
py = np.exp(-0.5 * x**2 / sigma2**2 - 0.5 * np.log(2*np.pi*sigma2**2))

kw_panel = dict(xlim=limits)
kw_scatter = dict(alpha=0.1, s=10, color='k')
fig, axes = plt.subplots(2, 3, figsize=(9, 6), subplot_kw=kw_panel)

for ax, samples in zip(axes[:,0], [hmc_samples, metro_samples]):
    ax.scatter(*samples.T, **kw_scatter)
    ax.contour(x, x, prob, 5)
    ax.set_ylim(*limits)
for ax, samples in zip(axes[:,1], [hmc_samples, metro_samples]):
    ax.hist(samples[burnin:,0], bins=21, density=True, color='k', alpha=0.2)
    ax.plot(x, px, color='r', lw=3)
for ax, samples in zip(axes[:,2], [hmc_samples, metro_samples]):
    ax.hist(samples[burnin:,1], bins=30, density=True, color='k', alpha=0.2)
    ax.plot(x, py, color='r', lw=3)
fig.tight_layout()
```

```{python}
methods = ('HMC', 'Metropolis')
fig, ax = plt.subplots(figsize=(12, 4))
for samples, method in zip([hmc_samples, metro_samples], methods):
    ax.plot(samples[burnin:,0], label=method, alpha=0.5, 
           color={'HMC': 'k', 'Metropolis': 'r'}[method])
ax.set_xlabel(r'iteration $s$')
ax.set_ylabel(r'first coordinate $x_1^{(s)}$')
ax.legend();
```

Behavior in high-dimensional sample spaces (another example from Radford Neal): 

```{python}
"""HMC in high dimensions: example from Radford Neal's HMC review
"""
class RandomStepsize:
    """Randomized stepsize mixin. 
    """
    def __init__(self, lower, upper):
        self.lower = float(lower)
        self.upper = float(upper)
        self._value = None
        assert self.lower <= self.upper
        
    @property
    def stepsize(self):
        self._value = np.random.uniform(self.lower, self.upper)
        return self._value
    
    @stepsize.setter
    def stepsize(self, value):
        pass

class MetropolisWithRandomStepsize(Metropolis, RandomStepsize):
    def __init__(self, model, initial, n_samples, lower, upper):
        Metropolis.__init__(self, model, initial, n_samples, lower)
        RandomStepsize.__init__(self, lower, upper)
        
class HMCWithRandomStepsize(HamiltonianMonteCarlo, RandomStepsize):
    def __init__(self, model, initial, n_samples, lower, upper, n_leapfrog):
        HamiltonianMonteCarlo.__init__(
            self, model, initial, n_samples, lower, n_leapfrog
            )
        RandomStepsize.__init__(self, lower, upper)
        
ndim = 100
sigma = np.linspace(0.01, 1., ndim)
model = CoupledOscillator(np.diag(1/sigma**2))
initial = np.zeros(ndim)
n_samples = 2000

# hmc
n_leapfrog = 150
hmc = HMCWithRandomStepsize(
        model, initial, n_samples, 0.0104, 0.0156, n_leapfrog)
samples_hmc = np.array(list(hmc))
print(hmc.acceptance_rate)

# metropolis
metro = MetropolisWithRandomStepsize(
        model, initial, n_samples * n_leapfrog, 0.0176, 0.0264)
samples_metro = np.array(list(metro))[::n_leapfrog]
print(metro.acceptance_rate)
```

```{python}
#| scrolled: true
burnin = int(n_samples/10)

# plotting
fig, axes = plt.subplots(2, 2, figsize=(9, 9), sharey='row', sharex='col')
for ax, samples in zip(axes[0,:], [samples_metro, samples_hmc]):
    ax.scatter(samples[burnin:].std(0), samples[burnin:].mean(0))
    ax.axhline(0., ls='--', color='r')
axes[0,0].set_ylim(-0.7, 0.7)
for ax, samples in zip(axes[1,:], [samples_metro, samples_hmc]):
    ax.plot([0., 1.], [0., 1.], ls='--', color='r')
    ax.scatter(samples[burnin:].std(0), sigma)
axes[0,0].set_title('Random walk Metropolis')
axes[0,1].set_title('Hamiltonian Monte Carlo')
axes[0,0].set_ylabel(r'sample mean $\{x_i^{(s)}\}$')
axes[1,0].set_xlabel(r'sample variance $\{x_i^{(s)}\}$')
axes[1,1].set_xlabel(r'sample variance $\{x_i^{(s)}\}$')
axes[1,0].set_ylabel(r'$\sigma_i$')
fig.tight_layout()
```

A practical issue in applications of HMC is that the algorithm requires the gradient of minus log $p(x)$. For some models it is far from straightforward to implement the gradient. Moreover, the gradient evaluations are on top of the evaluations of $\log p(x)$, which are typical for MCMC approaches based on Metropolis-Hastings. Some remedy is provided by the possibility to use [*automatic differentiation*](https://en.wikipedia.org/wiki/Automatic_differentiation) to compute the gradient without implementing it explicitly. This strategy is used, for example, in the [STAN](https://en.wikipedia.org/wiki/Stan_(software)) software for statistical inference or in probabilistic programming packages such as [tensorflow probability](https://www.tensorflow.org/probability), [PyMC 3](https://docs.pymc.io/), or [pyro](https://pyro.ai/). 

Another issue of practical importance is the question how to choose the algorithmic parameters, i.e. the number of integration steps $T$ and the step size $\epsilon$. Some attempts to choose these parameters automatically has been proposed and implemented in the [NUTS](https://arxiv.org/abs/1111.4246) algorithm.  

A special case of HMC is [Langevin dynamics](https://en.wikipedia.org/wiki/Stochastic_gradient_Langevin_dynamics) which omits the acceptance/rejection step altogether and has gained some popularity in Bayesian deep learning when combined with stochastic gradient methods. 
