---
editor:
    render-on-save: true
---

# Lecture 9: Practical issues and diagnostics

* Trace plots (examples, mixing, (non-)stationarity / convergence)
* ESS
* (split-)$\hat{R}$
* (t)rank plots
* Rank-normalized $\hat{R}$ https://arxiv.org/pdf/1903.08008.pdf
* https://arxiv.org/pdf/2003.07900.pdf as an example for other approach
* Divergent transitions in HMC?

## Practical Issues

![Challenges in MCMC](images/Murray_Thesis_Fig2-1.png "Challenges")

Figure from [Iain Murray: Advances in Markov chain Monte Carlo methods](http://homepages.inf.ed.ac.uk/imurray2/pub/07thesis/murray_thesis_2007.pdf)

### Challenges

* __Local exploration__: MCMC samplers typically employ a proposal kernel that changes the current state only locally. The magnitude of changes in the variables is controlled by the *step size* or a similar algorithmic parameter. The step size is limited by the need to maintain a reasonable acceptance rate. The time it takes for a diffusive random walk to explore a distance scales with
$$
(\text{distance} / \text{step size})^2  
$$

* __Convergence__: Typically, the chain starts from a highly improbable state, far away from any mode (local peak in the probability density function). To find a nearby mode, takes some time, again scaling unfavorably with dimension. But even if a mode has been found, it is not guaranteed that the Markov chain will find other modes in a reasonable amount of simulation time. These other modes could be more important in the sense that they carry more probability mass; so missing out on these modes can result in highly biased approximations. 

* __Mixing__: To find all relevant modes, is one of the greatest challenges when sampling high-dimensional probabilistic models with multiple peaks (which is the rule rather than the exception). There are many reasons for having to deal with multi-modal distributions. A common reason are symmetries such as invariance under permutation of labels resulting in the [label-switching problem](https://link.springer.com/chapter/10.1007/978-3-662-01131-7_26) in Gaussian mixture modeling. None of the methods that we discussed so far are particularly suited to explore multi-modal probability distributions. A common approach is to use *tempering* to flatten the probability such that the Markov chain can explore sample space more freely, and simulate a chain of tempered distributions, either sequentially (e.g. in [*Annealed importance sampling* (AIS)](https://link.springer.com/article/10.1023/A:1008923215028)) or in parallel (e.g. in [*Parallel tempering*](https://en.wikipedia.org/wiki/Parallel_tempering)). Unfortunately, there is not enough time to discuss these important methods. 

* __Balancing density and volume__: Another important issue is to not only find all relevant modes, but also visit them in due proportion. A probability peak might be very pronounced, but only carry a small amount of probability mass. If the Markov chain is stuck in this mode, samples coming from that mode will be overrepresented. 

Let's illustrate some of these problems for a discrete toy system.

```{python}
from scipy.special import logsumexp
"""
Discretized bimodal target used to study convergence. 
"""

SEED = [
        # produces bimodal distribution biased towards first mode
        41,
        # produces bimodal distribution biased towards second mode
        1234,
        # produces unimodal sample distribution
        43,   
        ][1]

class BimodalMixture:

    def __init__(self,
                 centers=np.array([0., 1.]),
                 widths=np.array([1., 1.]),
                 weight=0.5):
        self.centers = np.array(centers)
        self.widths = np.array(widths)
        self.weight = float(weight)

    def log_prob(self, x):
        dist = np.subtract.outer(x, self.centers)
        logp = -0.5 * dist**2 / self.widths**2 \
          - 0.5 * np.log(2*np.pi*self.widths**2) \
          + np.log(self.weight) + np.log(1-self.weight)
        return logsumexp(logp, axis=1)

def make_proposal(n_neighbors, n_states):
    Q = np.sum([np.eye(n_states, k=k) for k 
                in range(-n_neighbors, n_neighbors+1)], 0)
    return Q / Q.sum(1)

# setting bimodal toy system
centers = np.array([-1., 1.]) * 4
widths = np.array([0.2, 2.])
weight = 0.5

prob = BimodalMixture(centers, widths, weight)

n = 100            # number of states
X = np.arange(n)   # sample space
p = np.exp(prob.log_prob(np.linspace(-7., 10., n)))
p /= p.sum()       # discretized probability density

# use local proposal chain
stepsize = 5
n_samples = 1e4

# random walk with uniform proposal and reflective boundary
Q = make_proposal(stepsize, n) 

# run Metropolis-Hastings
np.random.seed(SEED) 

n_accepted = 0
x = X[-1]
samples = [x]
while len(samples) < n_samples:
    y = np.random.choice(X, p=Q[:,x])
    r = Q[x,y] * p[y] / (Q[y, x] * p[x])
    if r > np.random.random():
        x = y
        n_accepted += 1
    samples.append(x)
samples = np.array(samples)
logp = np.log(p[samples])

print('acceptance rate: {0:.1%}'.format(n_accepted/n_samples))
print('{0:.1%} of all samples are in left mode'.format(
    np.mean(samples<25)))

# plot results
plt.rc('font', size=14)
fig, ax = plt.subplots(figsize=(10, 5))
ax.plot(X, p, color='r', lw=2, alpha=0.7)
ax.hist(samples, bins=40, density=True, color='k', alpha=0.2);
```

```{python}
#| scrolled: true
fig, ax = plt.subplots(figsize=(10, 5))
ax.axhline(np.log(p).max(), color='r', lw=3, ls='--', alpha=0.7,
              label=r'$\max\{\log{p}\}$')
ax.plot(logp, color='k', lw=3, alpha=0.7, label=r'$\log p(x^{(s)}))$')
ax.legend()
fig.tight_layout()
```

### Convergence

#### Convergence rates for Markov chains

The speed of convergence of a Markov chain $P$ with stationary distribution $\pi$ depends on how quickly contributions to the distance

$$
\left|p^{(S)} - \pi\right|
$$ {#eq-distance}

die out as $S\to\infty$. Distance (@eq-distance) is dominated by the second largest eigenvalue $\lambda_2$ of $P$. Since the Markov chain is assumed to be irreducible and aperiodic, we have strictly $|\lambda_2| < 1$. If $u_2, u_3, \ldots$ are the eigenvectors of $P$ with eigenvalues $1 > |\lambda_2| \ge |\lambda_3| \ge \ldots$, then we can write the initial distribution as
$$
p^{(0)} = \pi + a_2 u_2 + a_3 u_3 + \ldots
$$
After $S$ transitions, the initial $p^{(0)}$ will be propagated to 
$$
p^{(S)} = \pi + a_2 \lambda_2^S u_2 + a_3 \lambda_3^S u_3 + \ldots
$$
and therefore
$$
\left|p^{(S)} - \pi\right| \sim |\lambda_2|^S
$$

The convergence rate depends on the step size, which calls for ways to *tune* algorithmic parameters of MCMC methods:

```{python}
def metropolis_map(Q, p):
    """
    Construct Metropolis kernel from proposal kernel and target distribution
    """
    # M = np.clip(Q, 0., (Q * p).T / p)
    M = np.min([Q, (Q * p).T / p], axis=0)
    i = np.arange(len(M))
    M[i,i] = 0.
    M[i,i] = 1 - M.sum(0)

    return M

def second_eigval(M):
    return np.sort(np.abs(np.linalg.eigvals(M)))[::-1][1]    

def propagate(M, p0, n):
    P = [p0.copy()]
    for _ in range(n):
        P.append(M @ P[-1])
    return np.array(P)
```

```{python}
stepsizes = (5, 10, 15, 20)
distances = []
rates = []

p0 = np.eye(n)[-1]
for stepsize in stepsizes:
    Q = make_proposal(stepsize, n)
    M = metropolis_map(Q, p)
    P = propagate(M, p0, 10000)
    d = np.fabs(P - p).sum(1)
    distances.append(d)
    rates.append(-np.log(second_eigval(M)))
```

```{python}
colors = plt.rcParams['axes.prop_cycle'].by_key()['color']
fig, ax = plt.subplots(figsize=(10,5))
for i in range(len(stepsizes)):
    ax.plot(distances[i], alpha=0.3, lw=5, 
            label='step size={0}'.format(stepsizes[i]))
t = np.arange(10000)
for i in range(len(stepsizes)):
    ax.plot(np.exp(-rates[i]*t), ls='--', alpha=0.9, 
            color=colors[i], label=r'$\exp\{-\lambda_2 s\}$')
ax.set_xlabel(r'iteration $s$')
ax.set_ylabel(r'$|\pi - p^{(s)}|$')
ax.legend()
ax.set_ylim(0., 1.);
    
```

```{python}
stepsizes = range(1, 31)
rates = []

for stepsize in stepsizes:
    Q = make_proposal(stepsize, n)
    M = metropolis_map(Q, p)
    rates.append(-np.log(second_eigval(M)))
    
fig, ax = plt.subplots(figsize=(5, 5))
ax.scatter(stepsizes, rates, s=200, color='k', alpha=0.7)
ax.set_ylabel(r'-$\log|\lambda_2|$')
ax.set_xlabel('stepsize');
```

### Burn-in bias

The MCMC chain does not start from the stationary distribution, so 
$$
\mathbb E_{p^{(s)}}[f] \not= \mathbb E_p[f]\, , 
$$
and the difference can be substantial for small $s$, thereby inducing significant bias to the Monte Carlo estimator:

$$
\frac{1}{S} \sum_{s=1}^S f\bigl(x^{(s)}\bigr).
$$

It is difficult to assess the reliability of MCMC approximations, because of the dependence of the samples $x^{(s)}$. The dependence usually adds variance to the estimator, when compared against simple Monte Carlo averages.

To minimize biases stemming from a poor choice of the initial distribution $p^{(0)}$, it is common practice to discard the first samples $x^{(0)}, \ldots, x^{(B)}$ called *burn-in*. It is assumed that $x^{(B+1)}$ will approximately follow the target distribution $p$. The Monte Carlo approximation then becomes:
$$
\frac{1}{S-B} \sum_{s=B+1}^S f\bigl(x^{(s)}\bigr)\, .
$$ {#eq-burnin}
Several statistics can be used to detect bias in MCMC simulations. However, they usually rely on rather strong assumptions, such as the asymptotic normality, or at least uni-modality of the target.

### Auto-correlation diagnostic

The asymptotic variance of the MCMC estimator can be shown to converge against
$$
\text{var}\left[\frac{1}{S}\sum_s f\bigl(x^{(s)}\bigr) \right] \xrightarrow[S\to\infty]{} \frac{1}{S} \text{var}_p[f] (1 + 2 \sum_{s\ge 1} \rho_s)  
$$ {#eq-variance}
where $\rho_s$ are the *correlations* between the initial samples and the $s$-th samples
$$
\rho_s = \text{corr}[f(x^{(0)}), f(x^{(s)})] \, .
$$ {#eq-correlation}
For uncorrelated samples $\rho_s=0$ and we are back to the standard variance of Monte Carlo estimators: $\text{var}[f]/S$. However, due to correlations the variance can be increased significantly (in principle, the variance can approach infinity for perfectly correlated samples). 

Another way to look at this is that correlations decrease the *effective sample size* (ESS), which becomes
$$
S_{\text{eff}} = \frac{S}{1 + 2 \sum_{s\ge 1} \rho_s} = \frac{S}{\text{IACT}}
$$ {#eq-ess}
where $\text{IACT} = 1 + 2 \sum_{s\ge 1} \rho_s$ is the *integrated auto-correlation time*. 

```{python}
# autocorrelation analysis of bimodal target

def autocorrelation(x, n):
    """
    auto-correlation of a times series

    Parameters
    ----------

    x: array containing time series
    n: Integer specifying maximal lag for which to compute the auto-correlation
    """
    x = x - x.mean()
    return np.array([np.mean(x[i:] * x[:len(x) - i]) for i in range(n)]) / np.std(x)**2

def run_metropolis(Q, p, X, n_samples=1e4):
    samples = [X[-1]]
    x = samples[0]
    n_acc = 0
    while len(samples) < n_samples:
        y = np.random.choice(X, p=Q[:,x])
        r = Q[x,y] * p[y] / (Q[y, x] * p[x])
        if r > np.random.random():
            x = y
            n_acc += 1
        samples.append(x)
    return np.array(samples), n_acc

np.random.seed(41) 
stepsizes = (5, 10, 15, 20)
ac = []
for stepsize in stepsizes:
    Q = make_proposal(stepsize, len(p))
    S, n_acc = run_metropolis(Q, p, X, 2e4)
    ac.append(autocorrelation(S*1., 10000))
    print('stepsize={0}: acceptance-rate={1:.1%}'.format(
        stepsize, n_acc/len(S)))
```

```{python}
fig, ax = plt.subplots(figsize=(10,5))
for i in range(len(stepsizes)):
    ax.plot(ac[i], alpha=0.3, lw=5, 
            label='stepsize={0}'.format(stepsizes[i]))
ax.axhline(0, ls='--', color='k', alpha=0.7)
ax.set_xlim(0, 4000)
ax.set_xlabel(r'iteration $s$')
ax.set_ylabel(r'autocorrelation')
ax.legend();
```

### Practical summary (from Vihola's lecture notes)

When using MCMC, always do the following checks:

1. Plot MCMC traces of the variables and key functions of the variables. They should look stationary after burn-in.

2. Make multiple MCMC runs from different initial states and check that the marginal distributions (or the estimators) look similar. This test reveals if your chain is "almost reducible".

3. Plot sample autocorrelations of the variables and functions.

4. Calculate the effective sample size and check that it is reasonably large. 

```{python}
import numpy as np
import matplotlib.pyplot as plt
from scipy.special import logsumexp


class BimodalMixture:
    def __init__(self,
                 centers=np.array([0., 1.]),
                 widths=np.array([1., 1.]),
                 weight=0.5):
        self.centers = np.array(centers)
        self.widths = np.array(widths)
        self.weight = float(weight)

    def log_prob(self, x):
        dist = np.subtract.outer(x, self.centers)
        logp = -0.5 * dist**2 / self.widths**2 \
          - 0.5 * np.log(2*np.pi*self.widths**2) \
          + np.log(self.weight) + np.log(1-self.weight)
        return logsumexp(logp, axis=1)


class Uniform:
    def __init__(self, limits):
        self.limits = limits
    
    def log_prob(self, x):
        return np.log((x > self.limits[0]) & (x < self.limits[1]))


def random_walk_mh(state, model, stepsize, rng=None, n_steps=1_000):
    rng = rng or np.random.default_rng()
    dim = state.shape[0]
    n_chains = state.shape[1]
    chains = np.empty_like(state, shape=(dim, n_steps, n_chains))
    chains[:, 0, :] = state
    for c in range(n_chains):
        logp = model.log_prob(chains[:, 0, c])
        for s in range(1, n_steps):
            state = chains[:, s - 1, c]
            proposal = state + stepsize * rng.normal(size=dim)
            logp_proposal = model.log_prob(proposal)
            if rng.uniform() < np.exp(logp_proposal - logp):
                # accept
                logp = logp_proposal
                state = proposal
            chains[:, s, c] = state
    return chains
```

```{python}
initial_state = np.array([[1.0, -2.0]])  # 2 chains with 1-dimensional state
centers = np.array([1., -2.])
widths = np.array([0.5, 0.5])
weight = 0.5
model1 = BimodalMixture(centers, widths, weight)
model2 = Uniform(np.array([-3.0, 2.0]))

rng = np.random.default_rng(25)
chains1 = random_walk_mh(np.array([[1.0, -2.0]]), model1, stepsize=0.3, rng=rng) 

rng = np.random.default_rng(1)
rng = np.random.default_rng(2)
chains2 = random_walk_mh(np.array([[1.9, -2.9]]), model2, stepsize=0.1, rng=rng) 

fig, axs = plt.subplots(1, 2, figsize=(8, 3))
axs[0].plot(chains1[0, :, 0], label="chain 1")
axs[0].plot(chains1[0, :, 1], label="chain 2")
axs[1].plot(chains2[0, :, 0], label="chain 1")
axs[1].plot(chains2[0, :, 1], label="chain 2")
```

